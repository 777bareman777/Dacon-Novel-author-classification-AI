{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data/dacon-novel-author-classification')\n",
    "feature_dir = Path('../build/feature')\n",
    "val_dir = Path('../build/val')\n",
    "tst_dir = Path('../build/tst')\n",
    "sub_dir = Path('../build/sub')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_name = 'mlp'\n",
    "\n",
    "feature_names= ['stacking-layer1-stopwords-yes-nn',\n",
    "               'stacking-layer1-stopwords-no-nn',\n",
    "               'stacking-layer1-stopwords-yes-ml',\n",
    "               'stacking-layer1-stopwords-no-ml']\n",
    "\n",
    "feature_target_file = feature_dir / f'feature_target.csv'\n",
    "\n",
    "model_names = []\n",
    "for feature_name in feature_names:\n",
    "    model_names.append(f'{algorithm_name}_{feature_name}')\n",
    "    \n",
    "stacking_oof_pred_files=[]\n",
    "for model_name in model_names:\n",
    "    stacking_oof_pred_files.append( val_dir / f'{model_name}_oof_pred.csv')\n",
    "    \n",
    "stacking_test_pred_files=[]\n",
    "for model_name in model_names:\n",
    "    stacking_test_pred_files.append( tst_dir / f'{model_name}_test_pred.csv')\n",
    "    \n",
    "stacking_submission_files=[]\n",
    "for model_name in model_names:\n",
    "    stacking_submission_files.append( sub_dir / f'{model_name}_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking feature 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(model_names, number_of_ver=None, kind=None):\n",
    "    oof_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    if number_of_ver==None or kind==None:\n",
    "        print('error')\n",
    "        return None\n",
    "    \n",
    "    # 딥러닝 시리즈 4가지 버전\n",
    "    if kind == 0:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            for i in range(1,number_of_ver+1):\n",
    "                oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv', delimiter=','))\n",
    "                test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "    \n",
    "    # 로지스틱 회귀 6가지 버전\n",
    "    elif kind == 1:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            for i in range(1, number_of_ver+1):\n",
    "                oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv', delimiter=','))\n",
    "                test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "\n",
    "    # 신경망 기반 불용어 처리 21가지 버전 또는 머신러닝 기반 불용어 처리 18가지 버전\n",
    "    elif kind == 2:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            if model.find('feature') != -1:\n",
    "                for i in range(2,5):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('tfidf') != -1:\n",
    "                for i in range(1,4):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('hashing') != -1:\n",
    "                for i in range(1,4):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('bow') != -1:\n",
    "                for i in range(1,4):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            else:\n",
    "                print('not found')\n",
    "    \n",
    "    # 신경만 기반 불용어 처리 X 13가지 버전 또는 머신러닝 기반 불용어 처리 X 18가지 버전\n",
    "    elif kind == 3:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            if model.find('feature') != -1:\n",
    "                for i in range(1,2):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('tfidf') != -1:\n",
    "                for i in range(4,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('hashing') != -1:\n",
    "                for i in range(4,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('bow') != -1:\n",
    "                for i in range(4,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            else:\n",
    "                print('not found')\n",
    "                \n",
    "    # 모든 버전 가져오기\n",
    "    elif kind == 4:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            if model.find('feature') != -1:\n",
    "                for i in range(1,5):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('tfidf') != -1:\n",
    "                for i in range(1,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('hashing') != -1:\n",
    "                for i in range(1,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('bow') != -1:\n",
    "                for i in range(1,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            else:\n",
    "                print('not found')\n",
    "    \n",
    "    return oof_list, test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cnn_feature_cv\n",
      "load lstm_feature_cv\n",
      "load mlp_feature_cv\n",
      "load transformer_feature_cv\n",
      "load transformer_v2_feature_cv\n",
      "load cnn_tfidf_cv\n",
      "load cnn_hashing_cv\n",
      "load cnn_bow_cv\n",
      "nn_yes shape : (54879, 120), (19617, 120)\n",
      "load cnn_feature_cv\n",
      "load lstm_feature_cv\n",
      "load mlp_feature_cv\n",
      "load transformer_feature_cv\n",
      "load transformer_v2_feature_cv\n",
      "load cnn_tfidf_cv\n",
      "load cnn_hashing_cv\n",
      "load cnn_bow_cv\n",
      "nn_yes shape : (54879, 70), (19617, 70)\n",
      "load mlp_tfidf_cv\n",
      "load mlp_hashing_cv\n",
      "load mlp_bow_cv\n",
      "load lr_tfidf_cv\n",
      "load lr_hashing_cv\n",
      "load lr_bow_cv\n",
      "nn_yes shape : (54879, 90), (19617, 90)\n",
      "load mlp_tfidf_cv\n",
      "load mlp_hashing_cv\n",
      "load mlp_bow_cv\n",
      "load lr_tfidf_cv\n",
      "load lr_hashing_cv\n",
      "load lr_bow_cv\n",
      "nn_yes shape : (54879, 90), (19617, 90)\n"
     ]
    }
   ],
   "source": [
    "nn_model_names= ['cnn_feature', 'lstm_feature' , 'mlp_feature', 'transformer_feature','transformer_v2_feature',\n",
    "               'cnn_tfidf', 'cnn_hashing', 'cnn_bow']\n",
    "\n",
    "ml_model_names= ['mlp_tfidf', 'mlp_hashing', 'mlp_bow','lr_tfidf','lr_hashing','lr_bow']\n",
    "\n",
    "\n",
    "trash = -1 # 의미없는 값\n",
    "stopwords_yes_kind = 2 # 의미있는 값\n",
    "stopwords_no_kind = 3 # 의미있는 값\n",
    "\n",
    "\n",
    "# stopwords-yes-nn 버전\n",
    "nn_yes_oof, nn_yes_test = load_feature(nn_model_names, trash, stopwords_yes_kind)\n",
    "nn_yes_oof = np.concatenate(nn_yes_oof, axis=1)\n",
    "nn_yes_test = np.concatenate(nn_yes_test, axis=1)\n",
    "print(f'nn_yes shape : {nn_yes_oof.shape}, {nn_yes_test.shape}')\n",
    "\n",
    "# stopwords-no-nn 버전\n",
    "nn_no_oof, nn_no_test = load_feature(nn_model_names, trash, stopwords_no_kind)\n",
    "nn_no_oof = np.concatenate(nn_no_oof, axis=1)\n",
    "nn_no_test = np.concatenate(nn_no_test, axis=1)\n",
    "print(f'nn_yes shape : {nn_no_oof.shape}, {nn_no_test.shape}')\n",
    "\n",
    "\n",
    "# stopwords-yes-ml 버전\n",
    "ml_yes_oof, ml_yes_test = load_feature(ml_model_names, trash, stopwords_yes_kind)\n",
    "ml_yes_oof = np.concatenate(ml_yes_oof, axis=1)\n",
    "ml_yes_test = np.concatenate(ml_yes_test, axis=1)\n",
    "print(f'nn_yes shape : {ml_yes_oof.shape}, {ml_yes_test.shape}')\n",
    "\n",
    "\n",
    "# stopwords-no-ml 버전\n",
    "ml_no_oof, ml_no_test = load_feature(ml_model_names, trash, stopwords_no_kind)\n",
    "ml_no_oof = np.concatenate(ml_no_oof, axis=1)\n",
    "ml_no_test = np.concatenate(ml_no_test, axis=1)\n",
    "print(f'nn_yes shape : {ml_no_oof.shape}, {ml_no_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(feature_target_file, index_col=0, usecols=['index',target_col]).values.flatten()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스태킹\n",
    "\n",
    "- 각 oof 마다 fold별로 logloos 변동이 있으므로 최대한 정보를 뽑아내고자 스태킹을 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(number):\n",
    "    inputs = Input(shape=(number,))\n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    outputs = Dense(n_class, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start : 1\n",
      "training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6376 - val_loss: 0.5378\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5364 - val_loss: 0.5339\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5229 - val_loss: 0.5227\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5156 - val_loss: 0.5190\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5120 - val_loss: 0.5266\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5062 - val_loss: 0.5262\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5013 - val_loss: 0.5177\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4973 - val_loss: 0.5204\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4927 - val_loss: 0.5184\n",
      "Epoch 10/100\n",
      "75/86 [=========================>....] - ETA: 0s - loss: 0.4887Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4904 - val_loss: 0.5213\n",
      "Epoch 00010: early stopping\n",
      "training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6245 - val_loss: 0.5453\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5321 - val_loss: 0.5362\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5217 - val_loss: 0.5347\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5135 - val_loss: 0.5286\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5065 - val_loss: 0.5286\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5047 - val_loss: 0.5272\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4996 - val_loss: 0.5319\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4957 - val_loss: 0.5273\n",
      "Epoch 9/100\n",
      "73/86 [========================>.....] - ETA: 0s - loss: 0.4900Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4910 - val_loss: 0.5320\n",
      "Epoch 00009: early stopping\n",
      "training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6312 - val_loss: 0.5358\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5350 - val_loss: 0.5235\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5241 - val_loss: 0.5311\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5166 - val_loss: 0.5246\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5117 - val_loss: 0.5192\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5063 - val_loss: 0.5216\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5018 - val_loss: 0.5125\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4988 - val_loss: 0.5147\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4941 - val_loss: 0.5157\n",
      "Epoch 10/100\n",
      "72/86 [========================>.....] - ETA: 0s - loss: 0.4899Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4921 - val_loss: 0.5199\n",
      "Epoch 00010: early stopping\n",
      "training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6285 - val_loss: 0.5552\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5328 - val_loss: 0.5379\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5190 - val_loss: 0.5282\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5144 - val_loss: 0.5259\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5063 - val_loss: 0.5284\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5022 - val_loss: 0.5242\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4986 - val_loss: 0.5235\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4943 - val_loss: 0.5343\n",
      "Epoch 9/100\n",
      "76/86 [=========================>....] - ETA: 0s - loss: 0.4930Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4923 - val_loss: 0.5260\n",
      "Epoch 00009: early stopping\n",
      "training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6382 - val_loss: 0.5506\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5335 - val_loss: 0.5375\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5232 - val_loss: 0.5306\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5154 - val_loss: 0.5180\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5064 - val_loss: 0.5166\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5033 - val_loss: 0.5227\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4991 - val_loss: 0.5172\n",
      "Epoch 8/100\n",
      "73/86 [========================>.....] - ETA: 0s - loss: 0.4965Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4947 - val_loss: 0.5164\n",
      "Epoch 00008: early stopping\n",
      "end : 1\n",
      "start : 2\n",
      "training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6775 - val_loss: 0.5706\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5603 - val_loss: 0.5568\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5507 - val_loss: 0.5543\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5457 - val_loss: 0.5480\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5377 - val_loss: 0.5494\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5328 - val_loss: 0.5405\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5290 - val_loss: 0.5437\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5253 - val_loss: 0.5536\n",
      "Epoch 9/100\n",
      "76/86 [=========================>....] - ETA: 0s - loss: 0.5234Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5231 - val_loss: 0.5407\n",
      "Epoch 00009: early stopping\n",
      "training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6682 - val_loss: 0.5737\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5561 - val_loss: 0.5578\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5469 - val_loss: 0.5568\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5395 - val_loss: 0.5535\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5342 - val_loss: 0.5499\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5298 - val_loss: 0.5456\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5250 - val_loss: 0.5426\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5223 - val_loss: 0.5471\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5172 - val_loss: 0.5458\n",
      "Epoch 10/100\n",
      "73/86 [========================>.....] - ETA: 0s - loss: 0.5153Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5155 - val_loss: 0.5463\n",
      "Epoch 00010: early stopping\n",
      "training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6738 - val_loss: 0.5653\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5598 - val_loss: 0.5581\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5500 - val_loss: 0.5520\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5426 - val_loss: 0.5378\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5374 - val_loss: 0.5464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5322 - val_loss: 0.5372\n",
      "Epoch 7/100\n",
      "75/86 [=========================>....] - ETA: 0s - loss: 0.5240Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5267 - val_loss: 0.5383\n",
      "Epoch 00007: early stopping\n",
      "training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6702 - val_loss: 0.5575\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5568 - val_loss: 0.5609\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5479 - val_loss: 0.5572\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5413 - val_loss: 0.5500\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5358 - val_loss: 0.5459\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5313 - val_loss: 0.5456\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5275 - val_loss: 0.5453\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5239 - val_loss: 0.5398\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5203 - val_loss: 0.5576\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5152 - val_loss: 0.5396\n",
      "Epoch 11/100\n",
      "75/86 [=========================>....] - ETA: 0s - loss: 0.5128Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5148 - val_loss: 0.5402\n",
      "Epoch 00011: early stopping\n",
      "training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6703 - val_loss: 0.5595\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5563 - val_loss: 0.5533\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5498 - val_loss: 0.5580\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5415 - val_loss: 0.5462\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5363 - val_loss: 0.5454\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5315 - val_loss: 0.5362\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5276 - val_loss: 0.5399\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5220 - val_loss: 0.5370\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5192 - val_loss: 0.5349\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5167 - val_loss: 0.5331\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5141 - val_loss: 0.5327\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5106 - val_loss: 0.5341\n",
      "Epoch 13/100\n",
      "75/86 [=========================>....] - ETA: 0s - loss: 0.5095Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5094 - val_loss: 0.5343\n",
      "Epoch 00013: early stopping\n",
      "end : 2\n",
      "start : 3\n",
      "training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.7124 - val_loss: 0.6331\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6203 - val_loss: 0.6216\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6080 - val_loss: 0.6126\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6020 - val_loss: 0.6074\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5979 - val_loss: 0.6051\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5954 - val_loss: 0.6066\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5918 - val_loss: 0.6034\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5897 - val_loss: 0.6016\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5870 - val_loss: 0.5990\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5866 - val_loss: 0.5986\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5834 - val_loss: 0.6035\n",
      "Epoch 12/100\n",
      "78/86 [==========================>...] - ETA: 0s - loss: 0.5836Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5833 - val_loss: 0.6012\n",
      "Epoch 00012: early stopping\n",
      "training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.7006 - val_loss: 0.6383\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6169 - val_loss: 0.6245\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6082 - val_loss: 0.6148\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6000 - val_loss: 0.6138\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5972 - val_loss: 0.6123\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5927 - val_loss: 0.6110\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5891 - val_loss: 0.6135\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5882 - val_loss: 0.6076\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5860 - val_loss: 0.6076\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5829 - val_loss: 0.6060\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5795 - val_loss: 0.6087\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5793 - val_loss: 0.6084\n",
      "Epoch 13/100\n",
      "74/86 [========================>.....] - ETA: 0s - loss: 0.5777Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5771 - val_loss: 0.6094\n",
      "Epoch 00013: early stopping\n",
      "training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.7144 - val_loss: 0.6008\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6243 - val_loss: 0.5901\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6133 - val_loss: 0.5892\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6082 - val_loss: 0.5846\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6049 - val_loss: 0.5877\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5979 - val_loss: 0.5830\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5974 - val_loss: 0.5811\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5940 - val_loss: 0.5791\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5935 - val_loss: 0.5809\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5901 - val_loss: 0.5828\n",
      "Epoch 11/100\n",
      "72/86 [========================>.....] - ETA: 0s - loss: 0.5859Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5879 - val_loss: 0.5799\n",
      "Epoch 00011: early stopping\n",
      "training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.7115 - val_loss: 0.6200\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6219 - val_loss: 0.6075\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6100 - val_loss: 0.6072\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6056 - val_loss: 0.6001\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6013 - val_loss: 0.6025\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5985 - val_loss: 0.5999\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5940 - val_loss: 0.5946\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5917 - val_loss: 0.5992\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5888 - val_loss: 0.5965\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5888 - val_loss: 0.5931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5831 - val_loss: 0.6020\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5829 - val_loss: 0.5960\n",
      "Epoch 13/100\n",
      "73/86 [========================>.....] - ETA: 0s - loss: 0.5786Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5810 - val_loss: 0.5959\n",
      "Epoch 00013: early stopping\n",
      "training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.7110 - val_loss: 0.6335\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6177 - val_loss: 0.6192\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6077 - val_loss: 0.6174\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6013 - val_loss: 0.6108\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5974 - val_loss: 0.6121\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5937 - val_loss: 0.6104\n",
      "Epoch 7/100\n",
      "74/86 [========================>.....] - ETA: 0s - loss: 0.5865Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5896 - val_loss: 0.6121\n",
      "Epoch 00007: early stopping\n",
      "end : 3\n",
      "start : 4\n",
      "training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6291 - val_loss: 0.5426\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5332 - val_loss: 0.5300\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5217 - val_loss: 0.5238\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5158 - val_loss: 0.5180\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5107 - val_loss: 0.5154\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5079 - val_loss: 0.5182\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5056 - val_loss: 0.5152\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5025 - val_loss: 0.5140\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5009 - val_loss: 0.5140\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4974 - val_loss: 0.5161\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4974 - val_loss: 0.5130\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4945 - val_loss: 0.5184\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4929 - val_loss: 0.5116\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4907 - val_loss: 0.5136\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4891 - val_loss: 0.5147\n",
      "Epoch 16/100\n",
      "73/86 [========================>.....] - ETA: 0s - loss: 0.4919Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4890 - val_loss: 0.5167\n",
      "Epoch 00016: early stopping\n",
      "training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6311 - val_loss: 0.5548\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5319 - val_loss: 0.5396\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5193 - val_loss: 0.5271\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5142 - val_loss: 0.5227\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5099 - val_loss: 0.5272\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5069 - val_loss: 0.5204\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5026 - val_loss: 0.5194\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5004 - val_loss: 0.5206\n",
      "Epoch 9/100\n",
      "75/86 [=========================>....] - ETA: 0s - loss: 0.4932Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.4991 - val_loss: 0.5207\n",
      "Epoch 00009: early stopping\n",
      "training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6280 - val_loss: 0.5248\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5397 - val_loss: 0.5130\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5261 - val_loss: 0.5093\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5224 - val_loss: 0.5083\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5160 - val_loss: 0.5015\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5130 - val_loss: 0.4975\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5097 - val_loss: 0.4998\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5077 - val_loss: 0.4974\n",
      "Epoch 9/100\n",
      "75/86 [=========================>....] - ETA: 0s - loss: 0.5076Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5065 - val_loss: 0.5030\n",
      "Epoch 00009: early stopping\n",
      "training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6346 - val_loss: 0.5442\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5340 - val_loss: 0.5300\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5210 - val_loss: 0.5185\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5143 - val_loss: 0.5187\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5105 - val_loss: 0.5195\n",
      "Epoch 6/100\n",
      "73/86 [========================>.....] - ETA: 0s - loss: 0.5078Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5082 - val_loss: 0.5182\n",
      "Epoch 00006: early stopping\n",
      "training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6288 - val_loss: 0.5394\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5356 - val_loss: 0.5289\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5219 - val_loss: 0.5184\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5153 - val_loss: 0.5148\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5118 - val_loss: 0.5148\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5076 - val_loss: 0.5148\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5050 - val_loss: 0.5123\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5020 - val_loss: 0.5137\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5000 - val_loss: 0.5162\n",
      "Epoch 10/100\n",
      "75/86 [=========================>....] - ETA: 0s - loss: 0.4964Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.4981 - val_loss: 0.5176\n",
      "Epoch 00010: early stopping\n",
      "end : 4\n"
     ]
    }
   ],
   "source": [
    "datasets = [(nn_yes_oof, nn_yes_test, y),\n",
    "            (nn_no_oof, nn_no_test, y),\n",
    "            (ml_yes_oof, ml_yes_test, y),\n",
    "            (ml_no_oof, ml_no_test, y)]\n",
    "\n",
    "mlogloss = []\n",
    "\n",
    "mlp_oof_preds = []\n",
    "mlp_test_preds = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "\n",
    "for number, (X, test , y) in enumerate(datasets, 1):\n",
    "    print(f'start : {number}')\n",
    "    \n",
    "    mlp_oof_pred = np.zeros((X.shape[0], n_class))\n",
    "    mlp_test_pred = np.zeros((test.shape[0], n_class))\n",
    "    \n",
    "    for i, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n",
    "        print(f'training model for CV #{i}')\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
    "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
    "        \n",
    "        clf = get_model(X.shape[1])\n",
    "        clf.fit(X[i_trn], \n",
    "            to_categorical(y[i_trn]),\n",
    "            validation_data=(X[i_val], to_categorical(y[i_val])),\n",
    "            epochs=100,\n",
    "            batch_size=512,\n",
    "            callbacks=[es])\n",
    "                \n",
    "        mlp_oof_pred[i_val, :] = clf.predict(X[i_val])\n",
    "        mlp_test_pred += clf.predict(test) / n_fold\n",
    "        mlogloss.append(log_loss(y[i_val], mlp_oof_pred[i_val]))\n",
    "    mlp_oof_preds.append(mlp_oof_pred)\n",
    "    mlp_test_preds.append(mlp_test_pred)\n",
    "    \n",
    "    print(f'end : {number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss =   0.5196\n",
      "accuracy =  81.2278\n",
      "logloss =   0.5388\n",
      "accuracy =  80.5481\n",
      "logloss =   0.5976\n",
      "accuracy =  78.0408\n",
      "logloss =   0.5120\n",
      "accuracy =  81.5631\n",
      "mean logloss =  0.5420083747910155\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(mlp_oof_preds,1):\n",
    "    print(f'logloss = {log_loss(pd.get_dummies(y),j):8.4f}')\n",
    "    print(f'accuracy = {accuracy_score(y, np.argmax(j,axis=1))*100:8.4f}')\n",
    "print('mean logloss = ',np.mean(mlogloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제출 파일 및 기타 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일 생성\n",
    "sub = pd.read_csv(sample_file,index_col=0)\n",
    "\n",
    "for filename, test_pred in zip(stacking_submission_files, mlp_test_preds):\n",
    "    sub[sub.columns] = test_pred\n",
    "    sub.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_oof_pred 파일 생성\n",
    "\n",
    "for filename, oof_pred in zip(stacking_oof_pred_files, mlp_oof_preds):\n",
    "    np.savetxt(filename, oof_pred, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_test_pred 파일 생성\n",
    "\n",
    "for filename, test_pred in zip(stacking_test_pred_files, mlp_test_preds):\n",
    "    np.savetxt(filename, test_pred, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
