{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 import 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:22.350713Z",
     "start_time": "2020-11-09T04:32:22.049823Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.188282Z",
     "start_time": "2020-11-09T04:32:22.352714Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.216883Z",
     "start_time": "2020-11-09T04:32:23.190555Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.250120Z",
     "start_time": "2020-11-09T04:32:23.219024Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path('../data/dacon-novel-author-classification')\n",
    "feature_dir = Path('../build/feature')\n",
    "val_dir = Path('../build/val')\n",
    "tst_dir = Path('../build/tst')\n",
    "sub_dir = Path('../build/sub')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.282083Z",
     "start_time": "2020-11-09T04:32:23.252439Z"
    }
   },
   "outputs": [],
   "source": [
    "algo_name = 'mlp'\n",
    "feature_name = 'tfidf'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "feature_file = feature_dir / f'{feature_name}.csv'\n",
    "\n",
    "p_val_ver1_file = val_dir / f'{model_name}_oof_pred_ver1.csv'\n",
    "p_tst_ver1_file = tst_dir / f'{model_name}_test_pred_ver1.csv'\n",
    "\n",
    "p_val_ver2_file = val_dir / f'{model_name}_oof_pred_ver2.csv'\n",
    "p_tst_ver2_file = tst_dir / f'{model_name}_test_pred_ver2.csv'\n",
    "\n",
    "p_val_ver3_file = val_dir / f'{model_name}_oof_pred_ver3.csv'\n",
    "p_tst_ver3_file = tst_dir / f'{model_name}_test_pred_ver3.csv'\n",
    "\n",
    "p_val_ver4_file = val_dir / f'{model_name}_oof_pred_ver4.csv'\n",
    "p_tst_ver4_file = tst_dir / f'{model_name}_test_pred_ver4.csv'\n",
    "\n",
    "p_val_ver5_file = val_dir / f'{model_name}_oof_pred_ver5.csv'\n",
    "p_tst_ver5_file = tst_dir / f'{model_name}_test_pred_ver5.csv'\n",
    "\n",
    "p_val_ver6_file = val_dir / f'{model_name}_oof_pred_ver6.csv'\n",
    "p_tst_ver6_file = tst_dir / f'{model_name}_test_pred_ver6.csv'\n",
    "\n",
    "sub_ver1_file = sub_dir / f'{model_name}_ver1.csv'\n",
    "sub_ver2_file = sub_dir / f'{model_name}_ver2.csv'\n",
    "sub_ver3_file = sub_dir / f'{model_name}_ver3.csv'\n",
    "sub_ver4_file = sub_dir / f'{model_name}_ver4.csv'\n",
    "sub_ver5_file = sub_dir / f'{model_name}_ver5.csv'\n",
    "sub_ver6_file = sub_dir / f'{model_name}_ver6.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.456017Z",
     "start_time": "2020-11-09T04:32:23.283900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      He was almost choking. There was so much, so m...       3\n",
       "1                 “Your sister asked for it, I suppose?”       2\n",
       "2       She was engaged one day as she walked, in per...       1\n",
       "3      The captain was in the porch, keeping himself ...       4\n",
       "4      “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn = pd.read_csv(trn_file, index_col=0)\n",
    "print(trn.shape)\n",
    "trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.580127Z",
     "start_time": "2020-11-09T04:32:23.458379Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19617, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Not at all. I think she is one of the most ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As the lady had stated her intention of scream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“And then suddenly in the silence I heard a so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>His conviction remained unchanged. So far as I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "index                                                   \n",
       "0      “Not at all. I think she is one of the most ch...\n",
       "1      \"No,\" replied he, with sudden consciousness, \"...\n",
       "2      As the lady had stated her intention of scream...\n",
       "3      “And then suddenly in the silence I heard a so...\n",
       "4      His conviction remained unchanged. So far as I..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = pd.read_csv(tst_file, index_col=0)\n",
    "print(tst.shape)\n",
    "tst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 토큰화 비교, 어간 추출과 표제어 추출 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.609289Z",
     "start_time": "2020-11-09T04:32:23.583484Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLTK에 있는 단어 토큰화\n",
    "# -> Don't를 Do 와 n't로 분리, Jone's를 Jone 과 '로 분리.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK에 있는 단어 토큰화 \n",
    "# -> Don't를 Don 과 ' 와 t 로 분리, Jone's를 Jone 과 ' 와 s로 분리.\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# 케라스에 있는 단어 토큰화\n",
    "# -> 모든 알파벳을 소문자로 바꾸고, 온점이나, 컴마, 느낌표 등의 구두점을 제거.\n",
    "# -> 하지만 don't 나 jone's와 같은 경우 아포스트로피를 보존함.\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "# NLTK에 있는 어간 추출(Stemming) 표제어 추출(Lemmatization)\n",
    "\n",
    "# -> WordNetLemmatizer는 기본형을 추출, 속도가 오래 걸리고 복잡함.\n",
    "# -> Pos(Part of Speech)에 대한 설정이 없으면, 제대로된 어간을 추출하지 못할 수 있음.\n",
    "# -> 제대로된 어간을 추출하고 싶다면, 단어의 쓰임새를 알아야 됨.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# -> Poter Stemmer은 대표적인 문법 기준을 뽑아서, 추출하는 방식, 즉 어간 추출함. 어간은 단어의 의미를 담고 있는 핵심 부분임.\n",
    "# -> 영어의 접미사(suffix)를 제거해서, 단어의 의미를 담고있는 어간만 추출함\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# -> LancasterStemmer은 Poter Stemmer와 비슷하지만, 알고리즘이 다름.\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# -> Porter Stemmer의 개선판, Porter Stemmer2라고 보면 됨.\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk의 word_tokenize를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.645042Z",
     "start_time": "2020-11-09T04:32:23.612849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_1 = trn.text[4]\n",
    "print(s_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'his', 'hands', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', '....', 'Oh', ',', 'my', 'God', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_1 = word_tokenize(s_1)\n",
    "print(tokenized_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentleman', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', '....', 'Oh', ',', 'my', 'God', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_1 = [lemmatizer.lemmatize(t) for t in tokenized_word_1]\n",
    "print(tokenized_lemmatizer_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'hi', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'I', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'Oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_1 = [porterStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_porter_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'hav', 'mercy', ',', 'gentlem', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'writ', 'that', ',', 'anyway', ';', 'hav', 'som', 'sham', '.', 'her', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'bef', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_1 = [lancasterStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_lancaster_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_1 = [snowballStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_snowball_word_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk의 WordPunctTokenizer를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_2 = trn.text[4]\n",
    "print(s_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'his', 'hands', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', '....', 'Oh', ',', 'my', 'God', '!”']\n"
     ]
    }
   ],
   "source": [
    "wordPunctTokenizer = WordPunctTokenizer()\n",
    "tokenized_word_2 = wordPunctTokenizer.tokenize(s_2)\n",
    "print(tokenized_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentleman', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', '....', 'Oh', ',', 'my', 'God', '!”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_2 = [lemmatizer.lemmatize(t) for t in tokenized_word_2]\n",
    "print(tokenized_lemmatizer_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'hi', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'I', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'Oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_2 = [porterStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_porter_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'hav', 'mercy', ',', 'gentlem', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'writ', 'that', ',', 'anyway', ';', 'hav', 'som', 'sham', '.', 'her', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'bef', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_2 = [lancasterStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_lancaster_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_2 = [snowballStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_snowball_word_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras의 text_to_word_sequence를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_3 = trn.text[4]\n",
    "print(s_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentlemen', '”', 'odin', 'flung', 'up', 'his', 'hands', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’ve', 'torn', 'my', 'heart', 'asunder', 'before', 'you', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_3 = text_to_word_sequence(s_3)\n",
    "print(tokenized_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentleman', '”', 'odin', 'flung', 'up', 'his', 'hand', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’ve', 'torn', 'my', 'heart', 'asunder', 'before', 'you', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_3 = [lemmatizer.lemmatize(t) for t in tokenized_word_3]\n",
    "print(tokenized_lemmatizer_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'merci', 'gentlemen', '”', 'odin', 'flung', 'up', 'hi', 'hand', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’v', 'torn', 'my', 'heart', 'asund', 'befor', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_3 = [porterStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_porter_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentlem', '”', 'odin', 'flung', 'up', 'his', 'hand', '“don’t', 'writ', 'that', 'anyway', 'hav', 'som', 'sham', 'her', 'i’ve', 'torn', 'my', 'heart', 'asund', 'bef', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_3 = [lancasterStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_lancaster_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'merci', 'gentlemen', '”', 'odin', 'flung', 'up', 'his', 'hand', \"“don't\", 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', \"i'v\", 'torn', 'my', 'heart', 'asund', 'befor', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_3 = [snowballStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_snowball_word_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T14:54:47.082620Z",
     "start_time": "2020-11-04T14:54:47.055487Z"
    }
   },
   "source": [
    "## TF-IDF 피쳐 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 word_tokenize 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.310245Z",
     "start_time": "2020-11-09T04:44:31.758835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 5897) (19617, 5897)\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "X_1 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_1 = vec.transform(tst['text']).toarray()\n",
    "print(X_1.shape, X_tst_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.342347Z",
     "start_time": "2020-11-09T04:45:10.312078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 WordPunctTokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 5772) (19617, 5772)\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(tokenizer=wordPunctTokenizer.tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "X_2 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_2 = vec.transform(tst['text']).toarray()\n",
    "print(X_2.shape, X_tst_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras의 text_to_word_sequence 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 3745) (19617, 3745)\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(tokenizer=text_to_word_sequence, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "X_3 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_3 = vec.transform(tst['text']).toarray()\n",
    "print(X_3.shape, X_tst_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 word_tokenize 사용, stopword 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.310245Z",
     "start_time": "2020-11-09T04:44:31.758835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 12254) (19617, 12254)\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3), min_df=50)\n",
    "X_4 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_4 = vec.transform(tst['text']).toarray()\n",
    "print(X_4.shape, X_tst_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.342347Z",
     "start_time": "2020-11-09T04:45:10.312078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_4[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 WordPunctTokenizer 사용, stopword 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 12267) (19617, 12267)\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(tokenizer=wordPunctTokenizer.tokenize, ngram_range=(1, 3), min_df=50)\n",
    "X_5 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_5 = vec.transform(tst['text']).toarray()\n",
    "print(X_5.shape, X_tst_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_5[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras의 text_to_word_sequence 사용, stopword 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 9555) (19617, 9555)\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(tokenizer=text_to_word_sequence, ngram_range=(1, 3), min_df=50)\n",
    "X_6 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_6 = vec.transform(tst['text']).toarray()\n",
    "print(X_6.shape, X_tst_6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_6[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T15:15:47.430701Z",
     "start_time": "2020-11-04T15:15:47.404265Z"
    }
   },
   "source": [
    "## mlp 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.370865Z",
     "start_time": "2020-11-09T04:45:10.344734Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(number):\n",
    "    inputs = Input(shape=(number,))\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    outputs = Dense(n_class, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.399912Z",
     "start_time": "2020-11-09T04:45:10.373016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = trn.author.values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:30.682036Z",
     "start_time": "2020-11-09T04:45:10.401772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for CV #1\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 1.3143 - val_loss: 0.9899\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7924 - val_loss: 0.7260\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6066 - val_loss: 0.6562\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5257 - val_loss: 0.6337\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4771 - val_loss: 0.6309\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4423 - val_loss: 0.6324\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4155 - val_loss: 0.6426\n",
      "Epoch 8/10\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.3942Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3939 - val_loss: 0.6538\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 1.3105 - val_loss: 0.9958\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7917 - val_loss: 0.7368\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6038 - val_loss: 0.6696\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5226 - val_loss: 0.6470\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4733 - val_loss: 0.6449\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4390 - val_loss: 0.6474\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4121 - val_loss: 0.6576\n",
      "Epoch 8/10\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3910Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3910 - val_loss: 0.6697\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 1.3076 - val_loss: 0.9678\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7896 - val_loss: 0.7065\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6080 - val_loss: 0.6406\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5279 - val_loss: 0.6173\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4790 - val_loss: 0.6138\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4446 - val_loss: 0.6184\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4187 - val_loss: 0.6267\n",
      "Epoch 8/10\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.3975Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3972 - val_loss: 0.6370\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 1.2967 - val_loss: 0.9896\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7906 - val_loss: 0.7306\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6064 - val_loss: 0.6595\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5253 - val_loss: 0.6368\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4765 - val_loss: 0.6315\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4416 - val_loss: 0.6344\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4148 - val_loss: 0.6434\n",
      "Epoch 8/10\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3931Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3931 - val_loss: 0.6540\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.2991 - val_loss: 0.9734\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7837 - val_loss: 0.7241\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6022 - val_loss: 0.6610\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5220 - val_loss: 0.6422\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4734 - val_loss: 0.6410\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4386 - val_loss: 0.6466\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4121 - val_loss: 0.6572\n",
      "Epoch 8/10\n",
      "77/86 [=========================>....] - ETA: 0s - loss: 0.3894Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3913 - val_loss: 0.6693\n",
      "Epoch 00008: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 1.3194 - val_loss: 0.9935\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7948 - val_loss: 0.7230\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6059 - val_loss: 0.6501\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5243 - val_loss: 0.6269\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4765 - val_loss: 0.6217\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4418 - val_loss: 0.6257\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4153 - val_loss: 0.6353\n",
      "Epoch 8/10\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3943Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3943 - val_loss: 0.6459\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.3078 - val_loss: 0.9845\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7783 - val_loss: 0.7292\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5949 - val_loss: 0.6634\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5153 - val_loss: 0.6443\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4670 - val_loss: 0.6418\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4336 - val_loss: 0.6477\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4077 - val_loss: 0.6564\n",
      "Epoch 8/10\n",
      "77/86 [=========================>....] - ETA: 0s - loss: 0.3830Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3864 - val_loss: 0.6695\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.3047 - val_loss: 0.9670\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7873 - val_loss: 0.7011\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6031 - val_loss: 0.6330\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5231 - val_loss: 0.6124\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4750 - val_loss: 0.6093\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4415 - val_loss: 0.6149\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4158 - val_loss: 0.6229\n",
      "Epoch 8/10\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3947Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3947 - val_loss: 0.6338\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 1.3147 - val_loss: 0.9966\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7886 - val_loss: 0.7248\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6008 - val_loss: 0.6540\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5202 - val_loss: 0.6315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4723 - val_loss: 0.6284\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4378 - val_loss: 0.6325\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4120 - val_loss: 0.6404\n",
      "Epoch 8/10\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3903Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3903 - val_loss: 0.6530\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 1.2999 - val_loss: 0.9718\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7790 - val_loss: 0.7183\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5970 - val_loss: 0.6547\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5179 - val_loss: 0.6365\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4697 - val_loss: 0.6343\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4359 - val_loss: 0.6406\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.4096 - val_loss: 0.6505\n",
      "Epoch 8/10\n",
      "80/86 [==========================>...] - ETA: 0s - loss: 0.3862Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.3887 - val_loss: 0.6634\n",
      "Epoch 00008: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.3654 - val_loss: 1.0772\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.8844 - val_loss: 0.8157\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7034 - val_loss: 0.7529\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.6328 - val_loss: 0.7372\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.5945 - val_loss: 0.7372\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5697 - val_loss: 0.7438\n",
      "Epoch 7/10\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5513Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.5517 - val_loss: 0.7511\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.3644 - val_loss: 1.0787\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.8830 - val_loss: 0.8212\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7043 - val_loss: 0.7577\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6334 - val_loss: 0.7403\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.5948 - val_loss: 0.7402\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5702 - val_loss: 0.7469\n",
      "Epoch 7/10\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5522Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5518 - val_loss: 0.7533\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.3562 - val_loss: 1.0586\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.8876 - val_loss: 0.7964\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7103 - val_loss: 0.7301\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.6393 - val_loss: 0.7131\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.6010 - val_loss: 0.7109\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5760 - val_loss: 0.7163\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.5578 - val_loss: 0.7240\n",
      "Epoch 8/10\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.5420Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5439 - val_loss: 0.7331\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.3585 - val_loss: 1.0770\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.8809 - val_loss: 0.8179\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7040 - val_loss: 0.7524\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.6339 - val_loss: 0.7356\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.5962 - val_loss: 0.7325\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.5714 - val_loss: 0.7385\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5536 - val_loss: 0.7466\n",
      "Epoch 8/10\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5389Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5394 - val_loss: 0.7552\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.3637 - val_loss: 1.0733\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.8823 - val_loss: 0.8120\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7034 - val_loss: 0.7492\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.6339 - val_loss: 0.7336\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5960 - val_loss: 0.7329\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.5711 - val_loss: 0.7376\n",
      "Epoch 7/10\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.5524Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.5531 - val_loss: 0.7440\n",
      "Epoch 00007: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2478 - val_loss: 0.8937\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6841 - val_loss: 0.6386\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4876 - val_loss: 0.5686\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3913 - val_loss: 0.5429\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3280 - val_loss: 0.5387\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2807 - val_loss: 0.5426\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2425 - val_loss: 0.5548\n",
      "Epoch 8/10\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.2114Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2115 - val_loss: 0.5709\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2450 - val_loss: 0.8926\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6798 - val_loss: 0.6385\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 0.4837 - val_loss: 0.5691\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3879 - val_loss: 0.5463\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3240 - val_loss: 0.5431\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2770 - val_loss: 0.5501\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2392 - val_loss: 0.5624\n",
      "Epoch 8/10\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.2078Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2076 - val_loss: 0.5792\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2453 - val_loss: 0.8762\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6834 - val_loss: 0.6207\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4889 - val_loss: 0.5502\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3932 - val_loss: 0.5256\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3300 - val_loss: 0.5200\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2827 - val_loss: 0.5238\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2452 - val_loss: 0.5365\n",
      "Epoch 8/10\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.2143Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2144 - val_loss: 0.5526\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2368 - val_loss: 0.8899\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6772 - val_loss: 0.6400\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4824 - val_loss: 0.5704\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3866 - val_loss: 0.5469\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3231 - val_loss: 0.5436\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2761 - val_loss: 0.5510\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2383 - val_loss: 0.5635\n",
      "Epoch 8/10\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.2061Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2069 - val_loss: 0.5804\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2400 - val_loss: 0.8813\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6785 - val_loss: 0.6294\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4853 - val_loss: 0.5607\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3904 - val_loss: 0.5394\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3265 - val_loss: 0.5369\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2793 - val_loss: 0.5418\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2414 - val_loss: 0.5539\n",
      "Epoch 8/10\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.2101Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2103 - val_loss: 0.5712\n",
      "Epoch 00008: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 17ms/step - loss: 1.2519 - val_loss: 0.8965\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6856 - val_loss: 0.6370\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4851 - val_loss: 0.5649\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3885 - val_loss: 0.5401\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3250 - val_loss: 0.5343\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2774 - val_loss: 0.5397\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2394 - val_loss: 0.5518\n",
      "Epoch 8/10\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.2083Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2083 - val_loss: 0.5672\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2547 - val_loss: 0.9042\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6860 - val_loss: 0.6419\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4846 - val_loss: 0.5698\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3867 - val_loss: 0.5464\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3229 - val_loss: 0.5418\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2754 - val_loss: 0.5484\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2377 - val_loss: 0.5611\n",
      "Epoch 8/10\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.2059Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2064 - val_loss: 0.5804\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2346 - val_loss: 0.8725\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6790 - val_loss: 0.6202\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4842 - val_loss: 0.5493\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3882 - val_loss: 0.5271\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3252 - val_loss: 0.5219\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2779 - val_loss: 0.5291\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2398 - val_loss: 0.5394\n",
      "Epoch 8/10\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.2086Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2086 - val_loss: 0.5589\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2602 - val_loss: 0.9082\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6872 - val_loss: 0.6428\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4853 - val_loss: 0.5704\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3873 - val_loss: 0.5466\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3235 - val_loss: 0.5416\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2762 - val_loss: 0.5503\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2382 - val_loss: 0.5608\n",
      "Epoch 8/10\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.2073Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2072 - val_loss: 0.5781\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2610 - val_loss: 0.8941\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.6847 - val_loss: 0.6310\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.4851 - val_loss: 0.5611\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3881 - val_loss: 0.5378\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.3244 - val_loss: 0.5357\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2769 - val_loss: 0.5425\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2388 - val_loss: 0.5544\n",
      "Epoch 8/10\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.2077Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 11ms/step - loss: 0.2075 - val_loss: 0.5704\n",
      "Epoch 00008: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 1.3126 - val_loss: 0.9960\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.7849 - val_loss: 0.7305\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5777 - val_loss: 0.6591\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4794 - val_loss: 0.6393\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4166 - val_loss: 0.6385\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3707 - val_loss: 0.6519\n",
      "Epoch 7/10\n",
      "80/86 [==========================>...] - ETA: 0s - loss: 0.3349Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3345 - val_loss: 0.6672\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 1.3139 - val_loss: 1.0075\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.7898 - val_loss: 0.7400\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5810 - val_loss: 0.6641\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4816 - val_loss: 0.6420\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4188 - val_loss: 0.6404\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3721 - val_loss: 0.6494\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3356 - val_loss: 0.6651\n",
      "Epoch 8/10\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.3030Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.3050 - val_loss: 0.6850\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 1.3132 - val_loss: 0.9894\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.7895 - val_loss: 0.7194\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5836 - val_loss: 0.6452\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.4853 - val_loss: 0.6218\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4221 - val_loss: 0.6195\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3763 - val_loss: 0.6296\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3396 - val_loss: 0.6443\n",
      "Epoch 8/10\n",
      "79/86 [==========================>...] - ETA: 0s - loss: 0.3074Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3093 - val_loss: 0.6646\n",
      "Epoch 00008: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 16ms/step - loss: 1.3201 - val_loss: 1.0081\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.7860 - val_loss: 0.7378\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.5769 - val_loss: 0.6657\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4778 - val_loss: 0.6441\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.4155 - val_loss: 0.6455\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3701 - val_loss: 0.6540\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.3338Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3338 - val_loss: 0.6702\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/10\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 1.3132 - val_loss: 0.9946\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.7851 - val_loss: 0.7283\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5797 - val_loss: 0.6549\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4821 - val_loss: 0.6331\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4199 - val_loss: 0.6317\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3741 - val_loss: 0.6405\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3378 - val_loss: 0.6562\n",
      "Epoch 8/10\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.3072Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3076 - val_loss: 0.6760\n",
      "Epoch 00008: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "lr ver1 Accuracy (CV):  76.5940%\n",
      "lr ver1 Log Loss (CV):   0.6324\n",
      "lr ver2 Accuracy (CV):  76.9274%\n",
      "lr ver2 Log Loss (CV):   0.6271\n",
      "lr ver3 Accuracy (CV):  72.5523%\n",
      "lr ver3 Log Loss (CV):   0.7309\n",
      "lr ver4 Accuracy (CV):  80.6283%\n",
      "lr ver4 Log Loss (CV):   0.5365\n",
      "lr ver5 Accuracy (CV):  80.7230%\n",
      "lr ver5 Log Loss (CV):   0.5351\n",
      "lr ver6 Accuracy (CV):  76.8491%\n",
      "lr ver6 Log Loss (CV):   0.6350\n"
     ]
    }
   ],
   "source": [
    "p_val_ver1 = np.zeros((X_1.shape[0], n_class))\n",
    "p_tst_ver1 = np.zeros((X_tst_1.shape[0], n_class))\n",
    "p_val_ver2 = np.zeros((X_2.shape[0], n_class))\n",
    "p_tst_ver2 = np.zeros((X_tst_2.shape[0], n_class))\n",
    "p_val_ver3 = np.zeros((X_3.shape[0], n_class))\n",
    "p_tst_ver3 = np.zeros((X_tst_3.shape[0], n_class))\n",
    "p_val_ver4 = np.zeros((X_4.shape[0], n_class))\n",
    "p_tst_ver4 = np.zeros((X_tst_4.shape[0], n_class))\n",
    "p_val_ver5 = np.zeros((X_5.shape[0], n_class))\n",
    "p_tst_ver5 = np.zeros((X_tst_5.shape[0], n_class))\n",
    "p_val_ver6 = np.zeros((X_6.shape[0], n_class))\n",
    "p_tst_ver6 = np.zeros((X_tst_6.shape[0], n_class))\n",
    "\n",
    "for X, test in [(X_1, X_tst_1), (X_2, X_tst_2), (X_3, X_tst_3),\n",
    "               (X_4, X_tst_4), (X_5, X_tst_5), (X_6, X_tst_6)]:  \n",
    "    for i_cv, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n",
    "        print(f'Training model for CV #{i_cv}')\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
    "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
    "        \n",
    "        clf = get_model(X.shape[1])\n",
    "        clf.fit(X[i_trn], \n",
    "            to_categorical(y[i_trn]),\n",
    "            validation_data=(X[i_val], to_categorical(y[i_val])),\n",
    "            epochs=10,\n",
    "            batch_size=512,\n",
    "            callbacks=[es])\n",
    "       \n",
    "        # Predict\n",
    "        if X.shape[1]==5897:\n",
    "            p_val_ver1[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver1 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==5772:\n",
    "            p_val_ver2[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver2 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==3745:\n",
    "            p_val_ver3[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver3 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==12254:\n",
    "            p_val_ver4[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver4 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==12267:\n",
    "            p_val_ver5[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver5 += clf.predict(test) / n_class\n",
    "        else:\n",
    "            p_val_ver6[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver6 += clf.predict(test) / n_class\n",
    "            \n",
    "        del clf\n",
    "        clear_session()\n",
    "        gc.collect()\n",
    "            \n",
    "    print(\"Training has finished\")\n",
    "    print(\"*\"*100)\n",
    "\n",
    "            \n",
    "print(f'lr ver1 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver1, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver1 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver1):8.4f}')\n",
    "print(f'lr ver2 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver2, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver2 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver2):8.4f}')\n",
    "print(f'lr ver3 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver3, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver3 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver3):8.4f}')\n",
    "print(f'lr ver4 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver4, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver4 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver4):8.4f}')\n",
    "print(f'lr ver5 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver5, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver5 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver5):8.4f}')\n",
    "print(f'lr ver6 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver6, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver6 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver6):8.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제출 파일 생성 및 기타 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일 생성\n",
    "\n",
    "sub = pd.read_csv(sample_file, index_col=0)\n",
    "\n",
    "# Ver1\n",
    "sub[sub.columns] = p_tst_ver1\n",
    "sub.to_csv(sub_ver1_file)\n",
    "\n",
    "# Ver2\n",
    "sub[sub.columns] = p_tst_ver2\n",
    "sub.to_csv(sub_ver2_file)\n",
    "\n",
    "# Ver3\n",
    "sub[sub.columns] = p_tst_ver3\n",
    "sub.to_csv(sub_ver3_file)\n",
    "\n",
    "# Ver4\n",
    "sub[sub.columns] = p_tst_ver4\n",
    "sub.to_csv(sub_ver4_file)\n",
    "\n",
    "# Ver5\n",
    "sub[sub.columns] = p_tst_ver5\n",
    "sub.to_csv(sub_ver5_file)\n",
    "\n",
    "# Ver6\n",
    "sub[sub.columns] = p_tst_ver6\n",
    "sub.to_csv(sub_ver6_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:31.272596Z",
     "start_time": "2020-11-09T04:45:31.074976Z"
    }
   },
   "outputs": [],
   "source": [
    "# p_val 파일 생성 -> oof\n",
    "\n",
    "# Ver1\n",
    "np.savetxt(p_val_ver1_file, p_val_ver1, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(p_val_ver2_file, p_val_ver2, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(p_val_ver3_file, p_val_ver3, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(p_val_ver4_file, p_val_ver4, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver5\n",
    "np.savetxt(p_val_ver5_file, p_val_ver5, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver6\n",
    "np.savetxt(p_val_ver6_file, p_val_ver6, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_tst 파일 생성 -> test \n",
    "\n",
    "# Ver1\n",
    "np.savetxt(p_tst_ver1_file, p_tst_ver1, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(p_tst_ver2_file, p_tst_ver2, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(p_tst_ver3_file, p_tst_ver3, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(p_tst_ver4_file, p_tst_ver4, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver5\n",
    "np.savetxt(p_tst_ver5_file, p_tst_ver5, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver6\n",
    "np.savetxt(p_tst_ver6_file, p_tst_ver6, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
