{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "from matplotlib import rcParams, pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Sequential, layers\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (16, 8)\n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2020\n",
    "# np.random.seed(seed)\n",
    "# rn.seed(seed)\n",
    "# tf.random.set_seed(seed)\n",
    "# session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
    "#                              inter_op_parallelism_threads=1)\n",
    "# sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "# tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data/dacon-novel-author-classification')\n",
    "feature_dir = Path('../build/feature')\n",
    "val_dir = Path('../build/val')\n",
    "tst_dir = Path('../build/tst')\n",
    "sub_dir = Path('../build/sub')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_name = 'transformer'\n",
    "feature_name = 'feature'\n",
    "model_name = f'{algorithm_name}_{feature_name}'\n",
    "\n",
    "feature_Ver1_file = feature_dir / f'{feature_name}_Ver1.csv'\n",
    "feature_Ver2_file = feature_dir / f'{feature_name}_Ver2.csv'\n",
    "feature_Ver3_file = feature_dir / f'{feature_name}_Ver3.csv'\n",
    "feature_Ver4_file = feature_dir / f'{feature_name}_Ver4.csv'\n",
    "\n",
    "\n",
    "transformer_oof_pred_ver1_file = val_dir / f'{model_name}_oof_pred_ver1.csv'\n",
    "transformer_oof_pred_ver2_file = val_dir / f'{model_name}_oof_pred_ver2.csv'\n",
    "transformer_oof_pred_ver3_file = val_dir / f'{model_name}_oof_pred_ver3.csv'\n",
    "transformer_oof_pred_ver4_file = val_dir / f'{model_name}_oof_pred_ver4.csv'\n",
    "\n",
    "\n",
    "transformer_test_pred_ver1_file = tst_dir / f'{model_name}_test_pred_ver1.csv'\n",
    "transformer_test_pred_ver2_file = tst_dir / f'{model_name}_test_pred_ver2.csv'\n",
    "transformer_test_pred_ver3_file = tst_dir / f'{model_name}_test_pred_ver3.csv'\n",
    "transformer_test_pred_ver4_file = tst_dir / f'{model_name}_test_pred_ver4.csv'\n",
    "\n",
    "\n",
    "transformer_submission_ver1_file = sub_dir / f'{model_name}_submission_Ver1.csv'\n",
    "transformer_submission_ver2_file = sub_dir / f'{model_name}_submission_Ver2.csv'\n",
    "transformer_submission_ver3_file = sub_dir / f'{model_name}_submission_Ver3.csv'\n",
    "transformer_submission_ver4_file = sub_dir / f'{model_name}_submission_Ver4.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ver 1 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74496, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he was almost choking there was so much so muc...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>your sister asked for it i suppose</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she was engaged one day as she walked in peru...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the captain was in the porch keeping himself c...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>have mercy gentlemen odin flung up his hands d...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      he was almost choking there was so much so muc...     3.0\n",
       "1                     your sister asked for it i suppose     2.0\n",
       "2       she was engaged one day as she walked in peru...     1.0\n",
       "3      the captain was in the porch keeping himself c...     4.0\n",
       "4      have mercy gentlemen odin flung up his hands d...     3.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(feature_Ver1_file, index_col=0)\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1) (54879,) (19617, 1)\n"
     ]
    }
   ],
   "source": [
    "# train set\n",
    "Ver1_X = dataset.loc[dataset[target_col] != -1 , :]\n",
    "Ver1_X.drop(columns=target_col,inplace=True,axis=1)\n",
    "Ver1_y = dataset.loc[dataset[target_col] != -1, target_col]\n",
    "Ver1_y.astype(int)\n",
    "\n",
    "# test set\n",
    "Ver1_test = dataset.loc[dataset[target_col] == -1, :]\n",
    "Ver1_test.drop(columns=target_col, inplace=True,axis=1)\n",
    "\n",
    "print(Ver1_X.shape, Ver1_y.shape, Ver1_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ver 2 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74496, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>almost choking much much wanted say strange ex...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sister asked suppose</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>engaged one day walked perusing janes last let...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>captain porch keeping carefully way treacherou...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mercy gentlemen odin flung hands dont write an...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      almost choking much much wanted say strange ex...     3.0\n",
       "1                                   sister asked suppose     2.0\n",
       "2      engaged one day walked perusing janes last let...     1.0\n",
       "3      captain porch keeping carefully way treacherou...     4.0\n",
       "4      mercy gentlemen odin flung hands dont write an...     3.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(feature_Ver2_file, index_col=0)\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1) (54879,) (19617, 1)\n"
     ]
    }
   ],
   "source": [
    "# train set\n",
    "Ver2_X = dataset.loc[dataset[target_col] != -1 , :]\n",
    "Ver2_X.drop(columns=target_col,inplace=True,axis=1)\n",
    "Ver2_y = dataset.loc[dataset[target_col] != -1, target_col]\n",
    "Ver2_y.astype(int)\n",
    "\n",
    "# test set\n",
    "Ver2_test = dataset.loc[dataset[target_col] == -1, :]\n",
    "Ver2_test.drop(columns=target_col, inplace=True,axis=1)\n",
    "\n",
    "print(Ver2_X.shape, Ver2_y.shape, Ver2_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ver3 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74496, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>almost choking much much wanted say strange ex...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sister asked suppose</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>engaged day walked perusing janes last letter ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>captain porch keeping himself carefully out wa...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mercy gentlemen flung up hands dont write anyw...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      almost choking much much wanted say strange ex...     3.0\n",
       "1                                   sister asked suppose     2.0\n",
       "2      engaged day walked perusing janes last letter ...     1.0\n",
       "3      captain porch keeping himself carefully out wa...     4.0\n",
       "4      mercy gentlemen flung up hands dont write anyw...     3.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(feature_Ver3_file, index_col=0)\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1) (54879,) (19617, 1)\n"
     ]
    }
   ],
   "source": [
    "# train set\n",
    "Ver3_X = dataset.loc[dataset[target_col] != -1 , :]\n",
    "Ver3_X.drop(columns=target_col,inplace=True,axis=1)\n",
    "Ver3_y = dataset.loc[dataset[target_col] != -1, target_col]\n",
    "Ver3_y.astype(int)\n",
    "\n",
    "# test set\n",
    "Ver3_test = dataset.loc[dataset[target_col] == -1, :]\n",
    "Ver3_test.drop(columns=target_col, inplace=True,axis=1)\n",
    "\n",
    "print(Ver3_X.shape, Ver3_y.shape, Ver3_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ver4 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74496, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he almost choking there so much so much he wan...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sister asked for it suppose</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>she engaged one day she walked perusing janes ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>captain porch keeping himself carefully out wa...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>have mercy gentlemen odin flung up hands dont ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      he almost choking there so much so much he wan...     3.0\n",
       "1                            sister asked for it suppose     2.0\n",
       "2      she engaged one day she walked perusing janes ...     1.0\n",
       "3      captain porch keeping himself carefully out wa...     4.0\n",
       "4      have mercy gentlemen odin flung up hands dont ...     3.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(feature_Ver4_file, index_col=0)\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1) (54879,) (19617, 1)\n"
     ]
    }
   ],
   "source": [
    "# train set\n",
    "Ver4_X = dataset.loc[dataset[target_col] != -1 , :]\n",
    "Ver4_X.drop(columns=target_col,inplace=True,axis=1)\n",
    "Ver4_y = dataset.loc[dataset[target_col] != -1, target_col]\n",
    "Ver4_y.astype(int)\n",
    "\n",
    "# test set\n",
    "Ver4_test = dataset.loc[dataset[target_col] == -1, :]\n",
    "Ver4_test.drop(columns=target_col, inplace=True,axis=1)\n",
    "\n",
    "print(Ver4_X.shape, Ver4_y.shape, Ver4_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000\n",
    "maxlen = 500\n",
    "embed_dim = 128\n",
    "num_heads = 8  # Number of attention heads\n",
    "padding_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    ff_dim = 64  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(n_class, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start : 1\n",
      "(54879, 500) (19617, 500)\n",
      "traing model for CV #1\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.3079WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0198s). Check your callbacks.\n",
      "1372/1372 [==============================] - 124s 91ms/step - loss: 1.3079 - val_loss: 0.9779\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 123s 90ms/step - loss: 0.8065 - val_loss: 0.7319\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 124s 90ms/step - loss: 0.5109 - val_loss: 0.7323\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.3523 - val_loss: 0.8557\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2729Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.2729 - val_loss: 1.0067\n",
      "Epoch 00005: early stopping\n",
      "traing model for CV #2\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.3197WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0038s vs `on_test_batch_end` time: 0.0198s). Check your callbacks.\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 1.3197 - val_loss: 1.0526\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.8819 - val_loss: 0.9184\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.5883 - val_loss: 0.7622\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.4003 - val_loss: 0.7892\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.3031 - val_loss: 1.0271\n",
      "Epoch 6/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2415Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.2415 - val_loss: 1.0281\n",
      "Epoch 00006: early stopping\n",
      "traing model for CV #3\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.2841WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0198s). Check your callbacks.\n",
      "1372/1372 [==============================] - 127s 93ms/step - loss: 1.2841 - val_loss: 0.9414\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.7597 - val_loss: 0.7066\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.4675 - val_loss: 0.7296\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.3258 - val_loss: 0.8846\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2596Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.2596 - val_loss: 0.9249\n",
      "Epoch 00005: early stopping\n",
      "traing model for CV #4\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.3104WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0197s). Check your callbacks.\n",
      "1372/1372 [==============================] - 127s 93ms/step - loss: 1.3104 - val_loss: 1.0676\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 127s 93ms/step - loss: 0.8252 - val_loss: 0.8525\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 127s 93ms/step - loss: 0.5098 - val_loss: 0.7764\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.3428 - val_loss: 0.7870\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.2595 - val_loss: 1.0492\n",
      "Epoch 6/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2122Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.2122 - val_loss: 1.2462\n",
      "Epoch 00006: early stopping\n",
      "traing model for CV #5\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - 126s 91ms/step - loss: 1.3304 - val_loss: 1.0575\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.8438 - val_loss: 0.7505\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 124s 91ms/step - loss: 0.5271 - val_loss: 0.7640\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.3595 - val_loss: 0.8693\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2721Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 124s 91ms/step - loss: 0.2721 - val_loss: 0.9326\n",
      "Epoch 00005: early stopping\n",
      "end : 1\n",
      "start : 2\n",
      "(54879, 500) (19617, 500)\n",
      "traing model for CV #1\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.2910WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0198s). Check your callbacks.\n",
      "1372/1372 [==============================] - 127s 93ms/step - loss: 1.2910 - val_loss: 1.0085\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 127s 93ms/step - loss: 0.8392 - val_loss: 0.8033\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 127s 93ms/step - loss: 0.5255 - val_loss: 0.7884\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.3626 - val_loss: 0.9425\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - 127s 93ms/step - loss: 0.2847 - val_loss: 0.9678\n",
      "Epoch 6/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2391Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.2391 - val_loss: 1.3037\n",
      "Epoch 00006: early stopping\n",
      "traing model for CV #2\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.3344WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0198s). Check your callbacks.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 1.3344 - val_loss: 1.0337\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.7994 - val_loss: 0.7460\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.4800 - val_loss: 0.8153\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.3348 - val_loss: 0.8870\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2590Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.2590 - val_loss: 1.0567\n",
      "Epoch 00005: early stopping\n",
      "traing model for CV #3\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.3762WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_test_batch_end` time: 0.0198s). Check your callbacks.\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 1.3762 - val_loss: 1.1162\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.9812 - val_loss: 0.9074\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.6582 - val_loss: 0.7513\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.4347 - val_loss: 0.8879\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.3271 - val_loss: 0.9356\n",
      "Epoch 6/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2667Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.2667 - val_loss: 0.9377\n",
      "Epoch 00006: early stopping\n",
      "traing model for CV #4\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.3058WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_test_batch_end` time: 0.0204s). Check your callbacks.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 1.3058 - val_loss: 1.0507\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.8458 - val_loss: 0.7780\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.5156 - val_loss: 0.7713\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.3418 - val_loss: 0.9017\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.2630 - val_loss: 1.0048\n",
      "Epoch 6/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2112Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.2112 - val_loss: 1.2188\n",
      "Epoch 00006: early stopping\n",
      "traing model for CV #5\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - 124s 91ms/step - loss: 1.2909 - val_loss: 1.0352\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 123s 90ms/step - loss: 0.8145 - val_loss: 0.7957\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.5062 - val_loss: 0.7875\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 124s 90ms/step - loss: 0.3520 - val_loss: 0.9214\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - 124s 90ms/step - loss: 0.2742 - val_loss: 1.0357\n",
      "Epoch 6/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2235Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 124s 90ms/step - loss: 0.2235 - val_loss: 1.0344\n",
      "Epoch 00006: early stopping\n",
      "end : 2\n",
      "start : 3\n",
      "(54879, 500) (19617, 500)\n",
      "traing model for CV #1\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.3450WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0197s). Check your callbacks.\n",
      "1372/1372 [==============================] - 127s 93ms/step - loss: 1.3450 - val_loss: 1.1131\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.9107 - val_loss: 0.7918\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.5559 - val_loss: 0.7704\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.3747 - val_loss: 0.8631\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.2857 - val_loss: 1.0477\n",
      "Epoch 6/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2353Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.2353 - val_loss: 1.1711\n",
      "Epoch 00006: early stopping\n",
      "traing model for CV #2\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.2707WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_test_batch_end` time: 0.0197s). Check your callbacks.\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 1.2707 - val_loss: 0.9736\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.7566 - val_loss: 0.7973\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.4639 - val_loss: 0.8982\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.3246 - val_loss: 0.9908\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2576Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.2576 - val_loss: 1.1015\n",
      "Epoch 00005: early stopping\n",
      "traing model for CV #3\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.3449WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0198s). Check your callbacks.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 1.3449 - val_loss: 1.0725\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.9251 - val_loss: 0.8462\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.5980 - val_loss: 0.7784\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.3982 - val_loss: 0.9347\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.3045 - val_loss: 0.9362\n",
      "Epoch 6/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2505Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.2505 - val_loss: 1.1565\n",
      "Epoch 00006: early stopping\n",
      "traing model for CV #4\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.2972WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_test_batch_end` time: 0.0204s). Check your callbacks.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 1.2972 - val_loss: 1.0507\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.7884 - val_loss: 0.7821\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.4881 - val_loss: 0.8436\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.3393 - val_loss: 0.9474\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2639Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.2639 - val_loss: 1.1836\n",
      "Epoch 00005: early stopping\n",
      "traing model for CV #5\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - 124s 90ms/step - loss: 1.3218 - val_loss: 1.0709\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 124s 90ms/step - loss: 0.8708 - val_loss: 0.8053\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 124s 90ms/step - loss: 0.5280 - val_loss: 0.8181\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 124s 90ms/step - loss: 0.3580 - val_loss: 0.9403\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2792Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 124s 90ms/step - loss: 0.2792 - val_loss: 1.0063\n",
      "Epoch 00005: early stopping\n",
      "end : 3\n",
      "start : 4\n",
      "(54879, 500) (19617, 500)\n",
      "traing model for CV #1\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.2050WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0197s). Check your callbacks.\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 1.2050 - val_loss: 0.9275\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.6919 - val_loss: 0.7371\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.4289 - val_loss: 0.7781\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.3076 - val_loss: 0.9792\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2465Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.2465 - val_loss: 0.9590\n",
      "Epoch 00005: early stopping\n",
      "traing model for CV #2\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.2317WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0036s vs `on_test_batch_end` time: 0.0197s). Check your callbacks.\n",
      "1372/1372 [==============================] - 127s 93ms/step - loss: 1.2317 - val_loss: 0.9262\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.7217 - val_loss: 0.7768\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 127s 92ms/step - loss: 0.4466 - val_loss: 0.7751\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.3180 - val_loss: 0.8535\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.2463 - val_loss: 0.9233\n",
      "Epoch 6/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2041Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 0.2041 - val_loss: 1.2151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: early stopping\n",
      "traing model for CV #3\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.2394WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0037s vs `on_test_batch_end` time: 0.0198s). Check your callbacks.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 1.2394 - val_loss: 0.9326\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.6977 - val_loss: 0.7156\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.4304 - val_loss: 0.7853\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.3051 - val_loss: 0.9144\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2425Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.2425 - val_loss: 1.0594\n",
      "Epoch 00005: early stopping\n",
      "traing model for CV #4\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 1.3167WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0030s vs `on_test_batch_end` time: 0.0205s). Check your callbacks.\n",
      "1372/1372 [==============================] - 126s 92ms/step - loss: 1.3167 - val_loss: 1.1634\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.7873 - val_loss: 0.7476\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.4831 - val_loss: 0.7703\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.3333 - val_loss: 0.9141\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.2633Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 125s 91ms/step - loss: 0.2633 - val_loss: 0.9472\n",
      "Epoch 00005: early stopping\n",
      "traing model for CV #5\n",
      "Epoch 1/200\n",
      "1372/1372 [==============================] - 123s 90ms/step - loss: 1.2162 - val_loss: 0.9233\n",
      "Epoch 2/200\n",
      "1372/1372 [==============================] - 123s 90ms/step - loss: 0.6971 - val_loss: 0.7665\n",
      "Epoch 3/200\n",
      "1372/1372 [==============================] - 123s 90ms/step - loss: 0.4279 - val_loss: 0.7645\n",
      "Epoch 4/200\n",
      "1372/1372 [==============================] - 123s 89ms/step - loss: 0.2998 - val_loss: 0.9524\n",
      "Epoch 5/200\n",
      "1372/1372 [==============================] - 123s 89ms/step - loss: 0.2375 - val_loss: 0.9768\n",
      "Epoch 6/200\n",
      "1372/1372 [==============================] - ETA: 0s - loss: 0.1917Restoring model weights from the end of the best epoch.\n",
      "1372/1372 [==============================] - 123s 90ms/step - loss: 0.1917 - val_loss: 1.0737\n",
      "Epoch 00006: early stopping\n",
      "end : 4\n"
     ]
    }
   ],
   "source": [
    "datasets = [ (Ver1_X, Ver1_test, Ver1_y), (Ver2_X, Ver2_test, Ver2_y),\n",
    "            (Ver3_X, Ver3_test, Ver3_y), (Ver4_X, Ver4_test, Ver3_y)]\n",
    "\n",
    "transformer_oof_preds = []\n",
    "transformer_test_preds = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "\n",
    "for number ,(X, test, y) in enumerate(datasets,1):\n",
    "    print(f'start : {number}')\n",
    "    # 토큰화\n",
    "    X_train = np.array([x for x in X['text']])\n",
    "    X_test = np.array([x for x in test['text']])\n",
    "    y = np.array(y.values)\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words = vocab_size)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    # 시퀸스화 + 패딩\n",
    "    train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "    trn = pad_sequences(train_sequences, padding=padding_type, maxlen=maxlen)\n",
    "    tst = pad_sequences(test_sequences, padding=padding_type, maxlen=maxlen)\n",
    "    print(trn.shape, tst.shape)\n",
    "    \n",
    "    # oof , test 저장\n",
    "    transformer_oof_pred = np.zeros((trn.shape[0], n_class))\n",
    "    transformer_test_pred = np.zeros((tst.shape[0], n_class))\n",
    "    \n",
    "    for i, (i_trn, i_val) in enumerate(cv.split(trn, y), 1):\n",
    "        print(f'traing model for CV #{i}')\n",
    "        clf = get_model()\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
    "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
    "        \n",
    "        clf.fit(trn[i_trn], \n",
    "            to_categorical(y[i_trn]),\n",
    "            validation_data=(trn[i_val], to_categorical(y[i_val])),\n",
    "            epochs=200,\n",
    "            batch_size=32,\n",
    "            callbacks=[es])\n",
    "        \n",
    "        transformer_oof_pred[i_val, :] = clf.predict(trn[i_val])\n",
    "        transformer_test_pred += clf.predict(tst) / n_fold\n",
    "        \n",
    "        del clf\n",
    "        clear_session()\n",
    "        gc.collect()\n",
    "    \n",
    "    transformer_oof_preds.append(transformer_oof_pred)\n",
    "    transformer_test_preds.append(transformer_test_pred)\n",
    "        \n",
    "    print(f'end : {number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ver1 logloss =   0.7455\n",
      "ver1 logloss =  72.5195\n",
      "ver2 logloss =   0.7689\n",
      "ver2 logloss =  72.2426\n",
      "ver3 logloss =   0.7867\n",
      "ver3 logloss =  71.1948\n",
      "ver4 logloss =   0.7480\n",
      "ver4 logloss =  72.9113\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(transformer_oof_preds,1):\n",
    "    print(f'ver{i} logloss = {log_loss(pd.get_dummies(y),j):8.4f}')\n",
    "    print(f'ver{i} accruacy = {accuracy_score(y, np.argmax(j,axis=1))*100:8.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제출 파일 및 기타 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일 생성\n",
    "\n",
    "sub = pd.read_csv(sample_file,index_col=0)\n",
    "\n",
    "# Ver1 \n",
    "sub[sub.columns] = transformer_test_preds[0]\n",
    "sub.to_csv(transformer_submission_ver1_file)\n",
    "\n",
    "# Ver2\n",
    "sub[sub.columns] = transformer_test_preds[1]\n",
    "sub.to_csv(transformer_submission_ver2_file)\n",
    "\n",
    "# Ver3\n",
    "sub[sub.columns] = transformer_test_preds[2]\n",
    "sub.to_csv(transformer_submission_ver3_file)\n",
    "           \n",
    "# Ver4\n",
    "sub[sub.columns] = transformer_test_preds[3]\n",
    "sub.to_csv(transformer_submission_ver4_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_oof_pred 파일 생성\n",
    "\n",
    "# Ver1\n",
    "np.savetxt(transformer_oof_pred_ver1_file, transformer_oof_preds[0],fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(transformer_oof_pred_ver2_file, transformer_oof_preds[1],fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(transformer_oof_pred_ver3_file, transformer_oof_preds[2],fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(transformer_oof_pred_ver4_file, transformer_oof_preds[3],fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_test_pred 파일 생성\n",
    "\n",
    "# Ver1\n",
    "np.savetxt(transformer_test_pred_ver1_file, transformer_test_preds[0],fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(transformer_test_pred_ver2_file, transformer_test_preds[1],fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(transformer_test_pred_ver3_file, transformer_test_preds[2],fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(transformer_test_pred_ver4_file, transformer_test_preds[3],fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
