{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams, pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "rcParams['figure.figsize'] = (16, 8)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2020\n",
    "# np.random.seed(seed)\n",
    "# rn.seed(seed)\n",
    "# tf.random.set_seed(seed)\n",
    "# session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n",
    "#                              inter_op_parallelism_threads=1)\n",
    "# sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "# tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data/dacon-novel-author-classification')\n",
    "feature_dir = Path('../build/feature')\n",
    "val_dir = Path('../build/val')\n",
    "tst_dir = Path('../build/tst')\n",
    "sub_dir = Path('../build/sub')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = 'feature'\n",
    "\n",
    "feature_Ver1_file = feature_dir / f'{feature_name}_Ver1.csv'\n",
    "feature_Ver2_file = feature_dir / f'{feature_name}_Ver2.csv'\n",
    "feature_Ver3_file = feature_dir / f'{feature_name}_Ver3.csv'\n",
    "feature_Ver4_file = feature_dir / f'{feature_name}_Ver4.csv'\n",
    "feature_Ver5_file = feature_dir / f'{feature_name}_Ver5.csv'\n",
    "\n",
    "feature_target_file = feature_dir / f'{feature_name}_target.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변수 생성\n",
    "\n",
    "- stopwords 제거 X -> Ver1\n",
    "- stopwords 제거 O -> Ver2\n",
    "- 중복 Top 100 모두 제거 -> Ver3\n",
    "- 중복 Top 100중 min*2 <= max 라면 제거 -> Ver4\n",
    "- tl-idf PCA -> Ver5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전테 탑 N에 있는 단어들 가져오는 함수\n",
    "def get_top_n_vocab(data, top=None):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    \n",
    "    if top is None:\n",
    "        vocab_size = len(tokenizer.word_index)\n",
    "    else:\n",
    "        vocab_size = min(top,len(tokenizer.word_index))\n",
    "    \n",
    "    vocab = {}\n",
    "    for word, index in tokenizer.word_index.items(): \n",
    "        if index == vocab_size+1:\n",
    "            break\n",
    "        vocab[word] = (index, tokenizer.word_counts[word])\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 탑 100에 있는 단어들 중, 모든 작가에서 발견된 단어들 체크\n",
    "# 그리고 그 단어들 개수 체크\n",
    "# 그리고 작가별 단어 빈도수도 체크\n",
    "def check_vocab_in_author(vocab, vocab_authors):\n",
    "    cnt = 0 # 모든 작가에서 발견된 단어인지 카운트\n",
    "    cnt_frequencys = [] # 작가별 빈도수 카운트\n",
    "    words = {} # 모든 작가에서 발견된 단어들 + 작가별 빈도수\n",
    "    \n",
    "    for key in vocab.keys():\n",
    "        for vocab_author in vocab_authors:\n",
    "            if key in vocab_author:\n",
    "                cnt += 1\n",
    "                cnt_frequencys.append(vocab_author.get(key)[1])\n",
    "        \n",
    "        if cnt==5:\n",
    "            words[key] = tuple(cnt_frequencys) \n",
    "            \n",
    "        cnt = 0\n",
    "        cnt_frequencys.clear()\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#부호를 제거해주는 함수\n",
    "def alpha_num(text):\n",
    "    return re.sub(r\"[^A-Za-z0-9' ]\", '', text)\n",
    "\n",
    "# 불용어 제거해주는 함수\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stopwords:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(trn_file, index_col=0)\n",
    "df_test = pd.read_csv(tst_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 적용\n",
    "df_train['text'] = df_train['text'].str.lower()\n",
    "df_test['text'] = df_test['text'].str.lower()\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(alpha_num)\n",
    "df_test['text'] = df_test['text'].apply(alpha_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ver 1 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ver1 = df_train.copy()\n",
    "df_test_ver1 = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([df_train_ver1,df_test_ver1], axis=0)\n",
    "dataset.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = pd.DataFrame(dataset)\n",
    "feature.to_csv(feature_Ver1_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ver 2 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ver2 = df_train.copy()\n",
    "df_test_ver2 = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \n",
    "             \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \n",
    "             \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \n",
    "             \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \n",
    "             \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \n",
    "             \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n",
    "             \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \n",
    "             \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \n",
    "             \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \n",
    "             \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \n",
    "             \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
    "\n",
    "df_train_ver2['text'] = df_train_ver2['text'].apply(remove_stopwords)\n",
    "df_test_ver2['text'] = df_test_ver2['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword를 제거 했을 때, 문장 하나가 공백이 되는 현상이 발생함.\n",
    "# 스태킹을 하기 위해서, 행을 버리는 것은 부담감이 있음.\n",
    "# 따라서, !!!!를 채운 다음에, 학습 코드에서 제거할 예정\n",
    "df_train_ver2=df_train_ver2.replace(r'','!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([df_train_ver2,df_test_ver2], axis=0)\n",
    "dataset.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = pd.DataFrame(dataset)\n",
    "feature.to_csv(feature_Ver2_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ver3 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ver3 = df_train.copy()\n",
    "df_test_ver3 = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879,) (19617,) (54879,)\n"
     ]
    }
   ],
   "source": [
    "# train test 분리\n",
    "X_train = df_train_ver3['text'].values\n",
    "X_test = df_test_ver3['text'].values\n",
    "y_train = df_train_ver3[target_col].values\n",
    "print(X_train.shape, X_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab = get_top_n_vocab(X_train)\n",
    "\n",
    "author_data = []\n",
    "for i in range(len(df_train_ver3[target_col].unique())):\n",
    "    temp = df_train_ver3[df_train_ver3[target_col]==i]['text']\n",
    "    author_data.append(temp)\n",
    "    \n",
    "author_vocab = []\n",
    "for i in range(len(df_train_ver3[target_col].unique())):\n",
    "    temp = get_top_n_vocab(author_data[i],100)\n",
    "    author_vocab.append(temp)\n",
    "    \n",
    "check_author_vocab = check_vocab_in_author(all_vocab, author_vocab)\n",
    "\n",
    "# 불용어\n",
    "stopwords =[word for word in check_author_vocab.keys()]\n",
    "\n",
    "df_train_ver3['text'] = df_train_ver3['text'].apply(remove_stopwords)\n",
    "df_test_ver3['text'] = df_test_ver3['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(df_train_ver3['text'].values):\n",
    "    if type(j) == float:\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([df_train_ver3,df_test_ver3], axis=0)\n",
    "dataset.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = pd.DataFrame(dataset)\n",
    "feature.to_csv(feature_Ver3_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ver4 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ver4 = df_train.copy()\n",
    "df_test_ver4 = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879,) (19617,) (54879,)\n"
     ]
    }
   ],
   "source": [
    "# train test 분리\n",
    "X_train = df_train_ver4['text'].values\n",
    "X_test = df_test_ver4['text'].values\n",
    "y_train = df_train_ver4[target_col].values\n",
    "print(X_train.shape, X_test.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab = get_top_n_vocab(X_train)\n",
    "\n",
    "author_data = []\n",
    "for i in range(len(df_train_ver4[target_col].unique())):\n",
    "    temp = df_train_ver4[df_train_ver4[target_col]==i]['text']\n",
    "    author_data.append(temp)\n",
    "    \n",
    "author_vocab = []\n",
    "for i in range(len(df_train_ver4[target_col].unique())):\n",
    "    temp = get_top_n_vocab(author_data[i],100)\n",
    "    author_vocab.append(temp)\n",
    "    \n",
    "check_author_vocab = check_vocab_in_author(all_vocab, author_vocab)\n",
    "\n",
    "# 불용어\n",
    "stopwords =[]\n",
    "for word, feq in check_author_vocab.items():\n",
    "    word_min, word_max = min(feq), max(feq)\n",
    "    check = word_min*2 >= word_max\n",
    "    if check:\n",
    "        stopwords.append(word)\n",
    "        \n",
    "df_train_ver4['text'] = df_train_ver4['text'].apply(remove_stopwords)\n",
    "df_test_ver4['text'] = df_test_ver4['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([df_train_ver4,df_test_ver4], axis=0)\n",
    "dataset.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = pd.DataFrame(dataset)\n",
    "feature.to_csv(feature_Ver4_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실제 타겟 값 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train.loc[:, target_col]\n",
    "y.to_csv(feature_target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일 생성\n",
    "sub = pd.read_csv(sample_file,index_col=0)\n",
    "\n",
    "for filename, test_pred in zip(stacking_submission_files, lgbm_test_preds):\n",
    "    sub[sub.columns] = test_pred\n",
    "    sub.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_oof_pred 파일 생성\n",
    "\n",
    "for filename, oof_pred in zip(stacking_oof_pred_files, lgbm_oof_preds):\n",
    "    np.savetxt(filename, oof_pred, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_test_pred 파일 생성\n",
    "\n",
    "for filename, test_pred in zip(stacking_test_pred_files, lgbm_test_preds):\n",
    "    np.savetxt(filename, test_pred, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일 생성\n",
    "sub = pd.read_csv(sample_file,index_col=0)\n",
    "\n",
    "for filename, test_pred in zip(stacking_submission_files, lgbm_test_preds):\n",
    "    sub[sub.columns] = test_pred\n",
    "    sub.to_csv(filename)\n",
    "\n",
    "# stacking_oof_pred 파일 생성\n",
    "\n",
    "for filename, oof_pred in zip(stacking_oof_pred_files, lgbm_oof_preds):\n",
    "    np.savetxt(filename, oof_pred, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# stacking_test_pred 파일 생성\n",
    "\n",
    "for filename, test_pred in zip(stacking_test_pred_files, lgbm_test_preds):\n",
    "    np.savetxt(filename, test_pred, fmt='%.18f', delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
