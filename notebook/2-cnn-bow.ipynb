{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 import 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:22.350713Z",
     "start_time": "2020-11-09T04:32:22.049823Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.188282Z",
     "start_time": "2020-11-09T04:32:22.352714Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.216883Z",
     "start_time": "2020-11-09T04:32:23.190555Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.250120Z",
     "start_time": "2020-11-09T04:32:23.219024Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path('../data/dacon-novel-author-classification')\n",
    "feature_dir = Path('../build/feature')\n",
    "val_dir = Path('../build/val')\n",
    "tst_dir = Path('../build/tst')\n",
    "sub_dir = Path('../build/sub')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.282083Z",
     "start_time": "2020-11-09T04:32:23.252439Z"
    }
   },
   "outputs": [],
   "source": [
    "algo_name = 'cnn'\n",
    "feature_name = 'bow'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "feature_file = feature_dir / f'{feature_name}.csv'\n",
    "\n",
    "p_val_ver1_file = val_dir / f'{model_name}_oof_pred_ver1.csv'\n",
    "p_tst_ver1_file = tst_dir / f'{model_name}_test_pred_ver1.csv'\n",
    "\n",
    "p_val_ver2_file = val_dir / f'{model_name}_oof_pred_ver2.csv'\n",
    "p_tst_ver2_file = tst_dir / f'{model_name}_test_pred_ver2.csv'\n",
    "\n",
    "p_val_ver3_file = val_dir / f'{model_name}_oof_pred_ver3.csv'\n",
    "p_tst_ver3_file = tst_dir / f'{model_name}_test_pred_ver3.csv'\n",
    "\n",
    "p_val_ver4_file = val_dir / f'{model_name}_oof_pred_ver4.csv'\n",
    "p_tst_ver4_file = tst_dir / f'{model_name}_test_pred_ver4.csv'\n",
    "\n",
    "p_val_ver5_file = val_dir / f'{model_name}_oof_pred_ver5.csv'\n",
    "p_tst_ver5_file = tst_dir / f'{model_name}_test_pred_ver5.csv'\n",
    "\n",
    "p_val_ver6_file = val_dir / f'{model_name}_oof_pred_ver6.csv'\n",
    "p_tst_ver6_file = tst_dir / f'{model_name}_test_pred_ver6.csv'\n",
    "\n",
    "sub_ver1_file = sub_dir / f'{model_name}_ver1.csv'\n",
    "sub_ver2_file = sub_dir / f'{model_name}_ver2.csv'\n",
    "sub_ver3_file = sub_dir / f'{model_name}_ver3.csv'\n",
    "sub_ver4_file = sub_dir / f'{model_name}_ver4.csv'\n",
    "sub_ver5_file = sub_dir / f'{model_name}_ver5.csv'\n",
    "sub_ver6_file = sub_dir / f'{model_name}_ver6.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.456017Z",
     "start_time": "2020-11-09T04:32:23.283900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      He was almost choking. There was so much, so m...       3\n",
       "1                 “Your sister asked for it, I suppose?”       2\n",
       "2       She was engaged one day as she walked, in per...       1\n",
       "3      The captain was in the porch, keeping himself ...       4\n",
       "4      “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn = pd.read_csv(trn_file, index_col=0)\n",
    "print(trn.shape)\n",
    "trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.580127Z",
     "start_time": "2020-11-09T04:32:23.458379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19617, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Not at all. I think she is one of the most ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As the lady had stated her intention of scream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“And then suddenly in the silence I heard a so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>His conviction remained unchanged. So far as I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "index                                                   \n",
       "0      “Not at all. I think she is one of the most ch...\n",
       "1      \"No,\" replied he, with sudden consciousness, \"...\n",
       "2      As the lady had stated her intention of scream...\n",
       "3      “And then suddenly in the silence I heard a so...\n",
       "4      His conviction remained unchanged. So far as I..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = pd.read_csv(tst_file, index_col=0)\n",
    "print(tst.shape)\n",
    "tst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 토큰화 비교, 어간 추출과 표제어 추출 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.609289Z",
     "start_time": "2020-11-09T04:32:23.583484Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLTK에 있는 단어 토큰화\n",
    "# -> Don't를 Do 와 n't로 분리, Jone's를 Jone 과 '로 분리.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK에 있는 단어 토큰화 \n",
    "# -> Don't를 Don 과 ' 와 t 로 분리, Jone's를 Jone 과 ' 와 s로 분리.\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# 케라스에 있는 단어 토큰화\n",
    "# -> 모든 알파벳을 소문자로 바꾸고, 온점이나, 컴마, 느낌표 등의 구두점을 제거.\n",
    "# -> 하지만 don't 나 jone's와 같은 경우 아포스트로피를 보존함.\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "# NLTK에 있는 어간 추출(Stemming) 표제어 추출(Lemmatization)\n",
    "\n",
    "# -> WordNetLemmatizer는 기본형을 추출, 속도가 오래 걸리고 복잡함.\n",
    "# -> Pos(Part of Speech)에 대한 설정이 없으면, 제대로된 어간을 추출하지 못할 수 있음.\n",
    "# -> 제대로된 어간을 추출하고 싶다면, 단어의 쓰임새를 알아야 됨.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# -> Poter Stemmer은 대표적인 문법 기준을 뽑아서, 추출하는 방식, 즉 어간 추출함. 어간은 단어의 의미를 담고 있는 핵심 부분임.\n",
    "# -> 영어의 접미사(suffix)를 제거해서, 단어의 의미를 담고있는 어간만 추출함\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# -> LancasterStemmer은 Poter Stemmer와 비슷하지만, 알고리즘이 다름.\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# -> Porter Stemmer의 개선판, Porter Stemmer2라고 보면 됨.\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk의 word_tokenize를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.645042Z",
     "start_time": "2020-11-09T04:32:23.612849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_1 = trn.text[4]\n",
    "print(s_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'his', 'hands', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', '....', 'Oh', ',', 'my', 'God', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_1 = word_tokenize(s_1)\n",
    "print(tokenized_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentleman', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', '....', 'Oh', ',', 'my', 'God', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_1 = [lemmatizer.lemmatize(t) for t in tokenized_word_1]\n",
    "print(tokenized_lemmatizer_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'hi', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'I', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'Oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_1 = [porterStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_porter_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'hav', 'mercy', ',', 'gentlem', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'writ', 'that', ',', 'anyway', ';', 'hav', 'som', 'sham', '.', 'her', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'bef', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_1 = [lancasterStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_lancaster_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_1 = [snowballStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_snowball_word_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk의 WordPunctTokenizer를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_2 = trn.text[4]\n",
    "print(s_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'his', 'hands', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', '....', 'Oh', ',', 'my', 'God', '!”']\n"
     ]
    }
   ],
   "source": [
    "wordPunctTokenizer = WordPunctTokenizer()\n",
    "tokenized_word_2 = wordPunctTokenizer.tokenize(s_2)\n",
    "print(tokenized_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentleman', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', '....', 'Oh', ',', 'my', 'God', '!”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_2 = [lemmatizer.lemmatize(t) for t in tokenized_word_2]\n",
    "print(tokenized_lemmatizer_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'hi', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'I', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'Oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_2 = [porterStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_porter_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'hav', 'mercy', ',', 'gentlem', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'writ', 'that', ',', 'anyway', ';', 'hav', 'som', 'sham', '.', 'her', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'bef', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_2 = [lancasterStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_lancaster_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_2 = [snowballStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_snowball_word_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kerasd의 text_to_word_sequence를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_3 = trn.text[4]\n",
    "print(s_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentlemen', '”', 'odin', 'flung', 'up', 'his', 'hands', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’ve', 'torn', 'my', 'heart', 'asunder', 'before', 'you', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_3 = text_to_word_sequence(s_3)\n",
    "print(tokenized_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentleman', '”', 'odin', 'flung', 'up', 'his', 'hand', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’ve', 'torn', 'my', 'heart', 'asunder', 'before', 'you', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_3 = [lemmatizer.lemmatize(t) for t in tokenized_word_3]\n",
    "print(tokenized_lemmatizer_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'merci', 'gentlemen', '”', 'odin', 'flung', 'up', 'hi', 'hand', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’v', 'torn', 'my', 'heart', 'asund', 'befor', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_3 = [porterStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_porter_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentlem', '”', 'odin', 'flung', 'up', 'his', 'hand', '“don’t', 'writ', 'that', 'anyway', 'hav', 'som', 'sham', 'her', 'i’ve', 'torn', 'my', 'heart', 'asund', 'bef', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_3 = [lancasterStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_lancaster_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'merci', 'gentlemen', '”', 'odin', 'flung', 'up', 'his', 'hand', \"“don't\", 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', \"i'v\", 'torn', 'my', 'heart', 'asund', 'befor', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_3 = [snowballStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_snowball_word_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T14:54:47.082620Z",
     "start_time": "2020-11-04T14:54:47.055487Z"
    }
   },
   "source": [
    "## DTM 피쳐 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 word_tokenize 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.310245Z",
     "start_time": "2020-11-09T04:44:31.758835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2685) (19617, 2685)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 2), min_df=100)\n",
    "X_1 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_1 = vec.transform(tst['text']).toarray()\n",
    "print(X_1.shape, X_tst_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.342347Z",
     "start_time": "2020-11-09T04:45:10.312078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 WordPunctTokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2655) (19617, 2655)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(tokenizer=wordPunctTokenizer.tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 2), min_df=100)\n",
    "X_2 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_2 = vec.transform(tst['text']).toarray()\n",
    "print(X_2.shape, X_tst_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 4, 0, 0, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras의 text_to_word_sequence 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1907) (19617, 1907)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(tokenizer=text_to_word_sequence, stop_words=stopwords.words('english'), ngram_range=(1, 2), min_df=100)\n",
    "X_3 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_3 = vec.transform(tst['text']).toarray()\n",
    "print(X_3.shape, X_tst_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 word_tokenize 사용 , stopword 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.310245Z",
     "start_time": "2020-11-09T04:44:31.758835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 4720) (19617, 4720)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(tokenizer=word_tokenize, ngram_range=(1, 2), min_df=100)\n",
    "X_4 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_4 = vec.transform(tst['text']).toarray()\n",
    "print(X_4.shape, X_tst_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.342347Z",
     "start_time": "2020-11-09T04:45:10.312078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_4[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 WordPunctTokenizer 사용 , stopword 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 4777) (19617, 4777)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(tokenizer=wordPunctTokenizer.tokenize, ngram_range=(1, 2), min_df=100)\n",
    "X_5 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_5 = vec.transform(tst['text']).toarray()\n",
    "print(X_5.shape, X_tst_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_5[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras의 text_to_word_sequence 사용 , stopword 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 4091) (19617, 4091)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(tokenizer=text_to_word_sequence, ngram_range=(1, 2), min_df=100)\n",
    "X_6 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_6 = vec.transform(tst['text']).toarray()\n",
    "print(X_6.shape, X_tst_6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_6[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T15:15:47.430701Z",
     "start_time": "2020-11-04T15:15:47.404265Z"
    }
   },
   "source": [
    "## cnn 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.370865Z",
     "start_time": "2020-11-09T04:45:10.344734Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(number):\n",
    "    inputs = Input(batch_shape=(None, number, 1))\n",
    "    x = Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(inputs)\n",
    "    x = Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(n_class, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.399912Z",
     "start_time": "2020-11-09T04:45:10.373016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = trn.author.values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:30.682036Z",
     "start_time": "2020-11-09T04:45:10.401772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      " 1/86 [..............................] - ETA: 0s - loss: 1.6227WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0160s vs `on_train_batch_end` time: 0.0279s). Check your callbacks.\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 1.4676 - val_loss: 1.3848\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3462 - val_loss: 1.3087\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.2915 - val_loss: 1.2892\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2622 - val_loss: 1.2701\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.2359 - val_loss: 1.2349\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2239 - val_loss: 1.2327\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.2151 - val_loss: 1.2203\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.1965 - val_loss: 1.2261\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1899 - val_loss: 1.2120\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1816 - val_loss: 1.2013\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1752 - val_loss: 1.2123\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1611 - val_loss: 1.1986\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1585 - val_loss: 1.1881\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1530 - val_loss: 1.1914\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.1455 - val_loss: 1.2050\n",
      "Epoch 16/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.1455Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1448 - val_loss: 1.1887\n",
      "Epoch 00016: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      " 1/86 [..............................] - ETA: 0s - loss: 1.6007WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0154s vs `on_train_batch_end` time: 0.0247s). Check your callbacks.\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 1.4655 - val_loss: 1.3748\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.3461 - val_loss: 1.3153\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2960 - val_loss: 1.2927\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2672 - val_loss: 1.2696\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2496 - val_loss: 1.2389\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2264 - val_loss: 1.2553\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2135 - val_loss: 1.2210\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2039 - val_loss: 1.2411\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1943 - val_loss: 1.2059\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1821 - val_loss: 1.2142\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1749 - val_loss: 1.2023\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1729 - val_loss: 1.1848\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1563 - val_loss: 1.2051\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1568 - val_loss: 1.2180\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1451 - val_loss: 1.1818\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1397 - val_loss: 1.1887\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1387 - val_loss: 1.1835\n",
      "Epoch 18/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.1290Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1291 - val_loss: 1.1977\n",
      "Epoch 00018: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      " 1/86 [..............................] - ETA: 0s - loss: 1.6272WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0151s vs `on_train_batch_end` time: 0.0247s). Check your callbacks.\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 1.4645 - val_loss: 1.3665\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3404 - val_loss: 1.3148\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2924 - val_loss: 1.2827\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2627 - val_loss: 1.3090\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2494 - val_loss: 1.2366\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.2263 - val_loss: 1.2276\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2142 - val_loss: 1.2236\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2022 - val_loss: 1.2075\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1960 - val_loss: 1.2041\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1808 - val_loss: 1.2092\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1741 - val_loss: 1.2158\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1703 - val_loss: 1.1887\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1616 - val_loss: 1.1885\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1537 - val_loss: 1.1841\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1477 - val_loss: 1.1821\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1395 - val_loss: 1.1750\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1369 - val_loss: 1.1801\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1310 - val_loss: 1.1763\n",
      "Epoch 19/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.1310Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 4s 47ms/step - loss: 1.1313 - val_loss: 1.1779\n",
      "Epoch 00019: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      " 1/86 [..............................] - ETA: 0s - loss: 1.6185WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0158s vs `on_train_batch_end` time: 0.0249s). Check your callbacks.\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 1.4740 - val_loss: 1.3910\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.3492 - val_loss: 1.3403\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2994 - val_loss: 1.2908\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2692 - val_loss: 1.2679\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2440 - val_loss: 1.2611\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2304 - val_loss: 1.2589\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2163 - val_loss: 1.2597\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2014 - val_loss: 1.2292\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1933 - val_loss: 1.2274\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1864 - val_loss: 1.2151\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1775 - val_loss: 1.2090\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1687 - val_loss: 1.1999\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1597 - val_loss: 1.2078\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 4s 47ms/step - loss: 1.1537 - val_loss: 1.2251\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1500 - val_loss: 1.1975\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1473 - val_loss: 1.2197\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1370 - val_loss: 1.2113\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1384 - val_loss: 1.1859\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1291 - val_loss: 1.1853\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1239 - val_loss: 1.1781\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1193 - val_loss: 1.2009\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1124 - val_loss: 1.1854\n",
      "Epoch 23/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.1125Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1124 - val_loss: 1.1796\n",
      "Epoch 00023: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      " 1/86 [..............................] - ETA: 0s - loss: 1.5941WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_end` time: 0.0248s). Check your callbacks.\n",
      "86/86 [==============================] - 4s 51ms/step - loss: 1.4548 - val_loss: 1.3814\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.3456 - val_loss: 1.3265\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2938 - val_loss: 1.2875\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2661 - val_loss: 1.2602\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2431 - val_loss: 1.2491\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2298 - val_loss: 1.2412\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2112 - val_loss: 1.2448\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2003 - val_loss: 1.2221\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1901 - val_loss: 1.2101\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1788 - val_loss: 1.2055\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1755 - val_loss: 1.2030\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1623 - val_loss: 1.2112\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1594 - val_loss: 1.1942\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1527 - val_loss: 1.1880\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 4s 47ms/step - loss: 1.1419 - val_loss: 1.1908\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1382 - val_loss: 1.1842\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1285 - val_loss: 1.2134\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 4s 47ms/step - loss: 1.1294 - val_loss: 1.1852\n",
      "Epoch 19/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.1233Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 4s 47ms/step - loss: 1.1233 - val_loss: 1.1861\n",
      "Epoch 00019: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 1.4927 - val_loss: 1.4253\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3910 - val_loss: 1.3503\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3343 - val_loss: 1.3241\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.2992 - val_loss: 1.2987\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.2885 - val_loss: 1.2856\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.2660 - val_loss: 1.2683\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.2533 - val_loss: 1.2668\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2415 - val_loss: 1.2658\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2353 - val_loss: 1.2601\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2247 - val_loss: 1.2485\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2211 - val_loss: 1.2523\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2142 - val_loss: 1.2403\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2093 - val_loss: 1.2442\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2002 - val_loss: 1.2443\n",
      "Epoch 15/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.1935Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1931 - val_loss: 1.2436\n",
      "Epoch 00015: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      " 1/86 [..............................] - ETA: 0s - loss: 1.5931WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0156s vs `on_train_batch_end` time: 0.0246s). Check your callbacks.\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 1.4918 - val_loss: 1.4316\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3885 - val_loss: 1.3672\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3425 - val_loss: 1.3311\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3081 - val_loss: 1.3054\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.2908 - val_loss: 1.3246\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2716 - val_loss: 1.2770\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2558 - val_loss: 1.2747\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2502 - val_loss: 1.2686\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2372 - val_loss: 1.2584\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2284 - val_loss: 1.2537\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2268 - val_loss: 1.2655\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2178 - val_loss: 1.2651\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 4s 48ms/step - loss: 1.2155 - val_loss: 1.2433\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2066 - val_loss: 1.2397\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.2007 - val_loss: 1.2405\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1935 - val_loss: 1.2350\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1940 - val_loss: 1.2424\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.1921 - val_loss: 1.2499\n",
      "Epoch 19/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.1804Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.1805 - val_loss: 1.2433\n",
      "Epoch 00019: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 1.5002 - val_loss: 1.4332\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.4025 - val_loss: 1.3772\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3451 - val_loss: 1.3257\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3152 - val_loss: 1.3106\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2943 - val_loss: 1.2918\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2757 - val_loss: 1.2810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2629 - val_loss: 1.2692\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2534 - val_loss: 1.2646\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 4s 47ms/step - loss: 1.2394 - val_loss: 1.2600\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.2332 - val_loss: 1.2432\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2229 - val_loss: 1.2487\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2202 - val_loss: 1.2499\n",
      "Epoch 13/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.2168Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2168 - val_loss: 1.2470\n",
      "Epoch 00013: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      " 1/86 [..............................] - ETA: 0s - loss: 1.6254WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0148s vs `on_train_batch_end` time: 0.0249s). Check your callbacks.\n",
      "86/86 [==============================] - 4s 49ms/step - loss: 1.4960 - val_loss: 1.4313\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3943 - val_loss: 1.3794\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3456 - val_loss: 1.3353\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3159 - val_loss: 1.3194\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2947 - val_loss: 1.3019\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2720 - val_loss: 1.2923\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2590 - val_loss: 1.2951\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2490 - val_loss: 1.2671\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2357 - val_loss: 1.2582\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2240 - val_loss: 1.2621\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2210 - val_loss: 1.2558\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2121 - val_loss: 1.2616\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2100 - val_loss: 1.2638\n",
      "Epoch 14/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.2039Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 4s 48ms/step - loss: 1.2037 - val_loss: 1.2740\n",
      "Epoch 00014: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 4s 50ms/step - loss: 1.4873 - val_loss: 1.4293\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3914 - val_loss: 1.3580\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3392 - val_loss: 1.3315\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.3138 - val_loss: 1.3031\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 1.2960 - val_loss: 1.3087\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2772 - val_loss: 1.2769\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2635 - val_loss: 1.2730\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2541 - val_loss: 1.2732\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2496 - val_loss: 1.2618\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 4s 47ms/step - loss: 1.2433 - val_loss: 1.2644\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 4s 48ms/step - loss: 1.2332 - val_loss: 1.2623\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.2225Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 1.2225 - val_loss: 1.2666\n",
      "Epoch 00012: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 1.5075 - val_loss: 1.4561\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.4359 - val_loss: 1.4168\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.4074 - val_loss: 1.4010\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.3876 - val_loss: 1.3973\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3784 - val_loss: 1.3779\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3682 - val_loss: 1.3841\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3548 - val_loss: 1.3929\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3578 - val_loss: 1.3677\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3433 - val_loss: 1.3932\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3377 - val_loss: 1.3611\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3315 - val_loss: 1.3558\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3271 - val_loss: 1.3634\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3234 - val_loss: 1.3630\n",
      "Epoch 14/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.3204Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3207 - val_loss: 1.3671\n",
      "Epoch 00014: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      " 1/86 [..............................] - ETA: 0s - loss: 1.6082WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0120s vs `on_train_batch_end` time: 0.0201s). Check your callbacks.\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 1.5058 - val_loss: 1.4578\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.4293 - val_loss: 1.4312\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.4014 - val_loss: 1.4010\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.3824 - val_loss: 1.3923\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.3714 - val_loss: 1.3835\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3631 - val_loss: 1.3811\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3535 - val_loss: 1.3731\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3447 - val_loss: 1.3698\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3416 - val_loss: 1.3690\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3354 - val_loss: 1.3693\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3286 - val_loss: 1.3583\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3231 - val_loss: 1.3601\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3207 - val_loss: 1.3532\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3156 - val_loss: 1.3552\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3110 - val_loss: 1.3522\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3063 - val_loss: 1.3560\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3041 - val_loss: 1.3473\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.2996 - val_loss: 1.3476\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.2975 - val_loss: 1.3446\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.2931 - val_loss: 1.3438\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.2932 - val_loss: 1.3559\n",
      "Epoch 22/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.2873Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.2871 - val_loss: 1.3657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00022: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      " 1/86 [..............................] - ETA: 0s - loss: 1.6004WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0125s vs `on_train_batch_end` time: 0.0195s). Check your callbacks.\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 1.5061 - val_loss: 1.4372\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.4279 - val_loss: 1.4126\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.4042 - val_loss: 1.3940\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.3867 - val_loss: 1.3926\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.3716 - val_loss: 1.3772\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3658 - val_loss: 1.3621\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3564 - val_loss: 1.3579\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3488 - val_loss: 1.3530\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3435 - val_loss: 1.3507\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 1.3389 - val_loss: 1.3636\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.3361 - val_loss: 1.3425\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3266 - val_loss: 1.3507\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3272 - val_loss: 1.3511\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 1.3210 - val_loss: 1.3408\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.3178 - val_loss: 1.3472\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3166 - val_loss: 1.3457\n",
      "Epoch 17/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.3112Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.3116 - val_loss: 1.3406\n",
      "Epoch 00017: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      " 1/86 [..............................] - ETA: 0s - loss: 1.6070WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0129s vs `on_train_batch_end` time: 0.0194s). Check your callbacks.\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 1.5073 - val_loss: 1.4636\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.4340 - val_loss: 1.4177\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.4109 - val_loss: 1.3983\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3924 - val_loss: 1.3900\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.3766 - val_loss: 1.3798\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.3665 - val_loss: 1.3714\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3598 - val_loss: 1.3651\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3521 - val_loss: 1.3643\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.3433 - val_loss: 1.3673\n",
      "Epoch 10/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.3434Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3432 - val_loss: 1.3650\n",
      "Epoch 00010: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 3s 37ms/step - loss: 1.5111 - val_loss: 1.4620\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.4328 - val_loss: 1.4363\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.4025 - val_loss: 1.4061\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.3853 - val_loss: 1.4124\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3738 - val_loss: 1.3915\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3622 - val_loss: 1.4093\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 3s 35ms/step - loss: 1.3568 - val_loss: 1.3993\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 3s 33ms/step - loss: 1.3488 - val_loss: 1.3868\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3448 - val_loss: 1.3909\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3361 - val_loss: 1.3756\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3326 - val_loss: 1.3777\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3304 - val_loss: 1.3669\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3231 - val_loss: 1.3696\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3211 - val_loss: 1.3682\n",
      "Epoch 15/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.3143Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 3s 34ms/step - loss: 1.3143 - val_loss: 1.3752\n",
      "Epoch 00015: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      " 2/86 [..............................] - ETA: 3s - loss: 1.5941WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0256s vs `on_train_batch_end` time: 0.0456s). Check your callbacks.\n",
      "86/86 [==============================] - 7s 86ms/step - loss: 1.4945 - val_loss: 1.4247\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.3865 - val_loss: 1.3487\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.3308 - val_loss: 1.3130\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.3000 - val_loss: 1.2962\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.2736 - val_loss: 1.2753\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.2472 - val_loss: 1.2521\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.2333 - val_loss: 1.2401\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.2199 - val_loss: 1.2400\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.2146 - val_loss: 1.2390\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.2036 - val_loss: 1.2349\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 7s 81ms/step - loss: 1.1902 - val_loss: 1.2218\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.1835 - val_loss: 1.2239\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.1765 - val_loss: 1.2141\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.1688 - val_loss: 1.2182\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 7s 81ms/step - loss: 1.1599 - val_loss: 1.2206\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.1593Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 7s 81ms/step - loss: 1.1593 - val_loss: 1.2142\n",
      "Epoch 00016: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      " 2/86 [..............................] - ETA: 3s - loss: 1.5967WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0280s vs `on_train_batch_end` time: 0.0511s). Check your callbacks.\n",
      "86/86 [==============================] - 7s 84ms/step - loss: 1.4979 - val_loss: 1.4415\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 7s 79ms/step - loss: 1.3889 - val_loss: 1.3768\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.3262 - val_loss: 1.3171\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 7s 79ms/step - loss: 1.2918 - val_loss: 1.2920\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 7s 79ms/step - loss: 1.2656 - val_loss: 1.2730\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 7s 79ms/step - loss: 1.2418 - val_loss: 1.2706\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.2293 - val_loss: 1.2508\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.2149 - val_loss: 1.2583\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.2069 - val_loss: 1.2354\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.1953 - val_loss: 1.2402\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.1909 - val_loss: 1.2334\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.1791 - val_loss: 1.2267\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.1765 - val_loss: 1.2144\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 7s 81ms/step - loss: 1.1572 - val_loss: 1.2144\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.1543 - val_loss: 1.2093\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.1496 - val_loss: 1.2387\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.1459 - val_loss: 1.2200\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.1357Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 7s 80ms/step - loss: 1.1357 - val_loss: 1.2101\n",
      "Epoch 00018: early stopping\n",
      "Training model for CV #3\n"
     ]
    }
   ],
   "source": [
    "p_val_ver1 = np.zeros((X_1.shape[0], n_class))\n",
    "p_tst_ver1 = np.zeros((X_tst_1.shape[0], n_class))\n",
    "p_val_ver2 = np.zeros((X_2.shape[0], n_class))\n",
    "p_tst_ver2 = np.zeros((X_tst_2.shape[0], n_class))\n",
    "p_val_ver3 = np.zeros((X_3.shape[0], n_class))\n",
    "p_tst_ver3 = np.zeros((X_tst_3.shape[0], n_class))\n",
    "p_val_ver4 = np.zeros((X_4.shape[0], n_class))\n",
    "p_tst_ver4 = np.zeros((X_tst_4.shape[0], n_class))\n",
    "p_val_ver5 = np.zeros((X_5.shape[0], n_class))\n",
    "p_tst_ver5 = np.zeros((X_tst_5.shape[0], n_class))\n",
    "p_val_ver6 = np.zeros((X_6.shape[0], n_class))\n",
    "p_tst_ver6 = np.zeros((X_tst_6.shape[0], n_class))\n",
    "\n",
    "for X, test in [(X_1, X_tst_1), (X_2, X_tst_2), (X_3, X_tst_3),\n",
    "               (X_4, X_tst_4), (X_5, X_tst_5), (X_6, X_tst_6)]: \n",
    "    for i_cv, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n",
    "        print(f'Training model for CV #{i_cv}')\n",
    "        X_train, X_val = X[i_trn], X[i_val]\n",
    "        y_train, y_val = y[i_trn], y[i_val]\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
    "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
    "        \n",
    "        clf = get_model(X.shape[1])\n",
    "        clf.fit(X[i_trn], \n",
    "            to_categorical(y[i_trn]),\n",
    "            validation_data=(X[i_val], to_categorical(y[i_val])),\n",
    "            epochs=100,\n",
    "            batch_size=512,\n",
    "            callbacks=[es])\n",
    "       \n",
    "        # Predict\n",
    "        if X.shape[1]==2685:\n",
    "            p_val_ver1[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver1 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==2655:\n",
    "            p_val_ver2[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver2 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==1907:\n",
    "            p_val_ver3[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver3 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==4720:\n",
    "            p_val_ver4[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver4 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==4777:\n",
    "            p_val_ver5[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver5 += clf.predict(test) / n_class\n",
    "        else:\n",
    "            p_val_ver6[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver6 += clf.predict(test) / n_class\n",
    "            \n",
    "        del clf\n",
    "        clear_session()\n",
    "        gc.collect()\n",
    "            \n",
    "    print(\"Training has finished\")\n",
    "    print(\"*\"*100)\n",
    "\n",
    "            \n",
    "print(f'lr ver1 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver1, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver1 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver1):8.4f}')\n",
    "print(f'lr ver2 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver2, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver2 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver2):8.4f}')\n",
    "print(f'lr ver3 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver3, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver3 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver3):8.4f}')\n",
    "print(f'lr ver4 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver4, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver4 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver4):8.4f}')\n",
    "print(f'lr ver5 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver5, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver5 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver5):8.4f}')\n",
    "print(f'lr ver6 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver6, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver6 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver6):8.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제출 파일 생성 및 기타 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일 생성\n",
    "\n",
    "sub = pd.read_csv(sample_file, index_col=0)\n",
    "\n",
    "# Ver1\n",
    "sub[sub.columns] = p_tst_ver1\n",
    "sub.to_csv(sub_ver1_file)\n",
    "\n",
    "# Ver2\n",
    "sub[sub.columns] = p_tst_ver2\n",
    "sub.to_csv(sub_ver2_file)\n",
    "\n",
    "# Ver3\n",
    "sub[sub.columns] = p_tst_ver3\n",
    "sub.to_csv(sub_ver3_file)\n",
    "\n",
    "# Ver4\n",
    "sub[sub.columns] = p_tst_ver4\n",
    "sub.to_csv(sub_ver4_file)\n",
    "\n",
    "# Ver5\n",
    "sub[sub.columns] = p_tst_ver5\n",
    "sub.to_csv(sub_ver5_file)\n",
    "\n",
    "# Ver6\n",
    "sub[sub.columns] = p_tst_ver6\n",
    "sub.to_csv(sub_ver6_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:31.272596Z",
     "start_time": "2020-11-09T04:45:31.074976Z"
    }
   },
   "outputs": [],
   "source": [
    "# p_val 파일 생성 -> oof\n",
    "\n",
    "# Ver1\n",
    "np.savetxt(p_val_ver1_file, p_val_ver1, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(p_val_ver2_file, p_val_ver2, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(p_val_ver3_file, p_val_ver3, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(p_val_ver4_file, p_val_ver4, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver5\n",
    "np.savetxt(p_val_ver5_file, p_val_ver5, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver6\n",
    "np.savetxt(p_val_ver6_file, p_val_ver6, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_tst 파일 생성 -> test \n",
    "\n",
    "# Ver1\n",
    "np.savetxt(p_tst_ver1_file, p_tst_ver1, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(p_tst_ver2_file, p_tst_ver2, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(p_tst_ver3_file, p_tst_ver3, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(p_tst_ver4_file, p_tst_ver4, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver5\n",
    "np.savetxt(p_tst_ver5_file, p_tst_ver5, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver6\n",
    "np.savetxt(p_tst_ver6_file, p_tst_ver6, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
