{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data/dacon-novel-author-classification')\n",
    "feature_dir = Path('../build/feature')\n",
    "val_dir = Path('../build/val')\n",
    "tst_dir = Path('../build/tst')\n",
    "sub_dir = Path('../build/sub')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_name = 'rf'\n",
    "\n",
    "feature_names= ['stacking-layer1-stopwords-yes-nn',\n",
    "               'stacking-layer1-stopwords-no-nn',\n",
    "               'stacking-layer1-stopwords-yes-ml',\n",
    "               'stacking-layer1-stopwords-no-ml']\n",
    "\n",
    "feature_target_file = feature_dir / f'feature_target.csv'\n",
    "\n",
    "model_names = []\n",
    "for feature_name in feature_names:\n",
    "    model_names.append(f'{algorithm_name}_{feature_name}')\n",
    "    \n",
    "stacking_oof_pred_files=[]\n",
    "for model_name in model_names:\n",
    "    stacking_oof_pred_files.append( val_dir / f'{model_name}_oof_pred.csv')\n",
    "    \n",
    "stacking_test_pred_files=[]\n",
    "for model_name in model_names:\n",
    "    stacking_test_pred_files.append( tst_dir / f'{model_name}_test_pred.csv')\n",
    "    \n",
    "stacking_submission_files=[]\n",
    "for model_name in model_names:\n",
    "    stacking_submission_files.append( sub_dir / f'{model_name}_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking feature 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(model_names, number_of_ver=None, kind=None):\n",
    "    oof_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    if number_of_ver==None or kind==None:\n",
    "        print('error')\n",
    "        return None\n",
    "    \n",
    "    # 딥러닝 시리즈 4가지 버전\n",
    "    if kind == 0:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            for i in range(1,number_of_ver+1):\n",
    "                oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv', delimiter=','))\n",
    "                test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "    \n",
    "    # 로지스틱 회귀 6가지 버전\n",
    "    elif kind == 1:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            for i in range(1, number_of_ver+1):\n",
    "                oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv', delimiter=','))\n",
    "                test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "\n",
    "    # 신경망 기반 불용어 처리 21가지 버전 또는 머신러닝 기반 불용어 처리 18가지 버전\n",
    "    elif kind == 2:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            if model.find('feature') != -1:\n",
    "                for i in range(2,5):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('tfidf') != -1:\n",
    "                for i in range(1,4):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('hashing') != -1:\n",
    "                for i in range(1,4):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('bow') != -1:\n",
    "                for i in range(1,4):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            else:\n",
    "                print('not found')\n",
    "    \n",
    "    # 신경만 기반 불용어 처리 X 13가지 버전 또는 머신러닝 기반 불용어 처리 X 18가지 버전\n",
    "    elif kind == 3:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            if model.find('feature') != -1:\n",
    "                for i in range(1,2):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('tfidf') != -1:\n",
    "                for i in range(4,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('hashing') != -1:\n",
    "                for i in range(4,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('bow') != -1:\n",
    "                for i in range(4,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            else:\n",
    "                print('not found')\n",
    "                \n",
    "    # 모든 버전 가져오기\n",
    "    elif kind == 4:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            if model.find('feature') != -1:\n",
    "                for i in range(1,5):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('tfidf') != -1:\n",
    "                for i in range(1,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('hashing') != -1:\n",
    "                for i in range(1,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('bow') != -1:\n",
    "                for i in range(1,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            else:\n",
    "                print('not found')\n",
    "    \n",
    "    return oof_list, test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cnn_feature_cv\n",
      "load lstm_feature_cv\n",
      "load mlp_feature_cv\n",
      "load transformer_feature_cv\n",
      "load transformer_v2_feature_cv\n",
      "load cnn_tfidf_cv\n",
      "load cnn_hashing_cv\n",
      "load cnn_bow_cv\n",
      "nn_yes shape : (54879, 120), (19617, 120)\n",
      "load cnn_feature_cv\n",
      "load lstm_feature_cv\n",
      "load mlp_feature_cv\n",
      "load transformer_feature_cv\n",
      "load transformer_v2_feature_cv\n",
      "load cnn_tfidf_cv\n",
      "load cnn_hashing_cv\n",
      "load cnn_bow_cv\n",
      "nn_yes shape : (54879, 70), (19617, 70)\n",
      "load mlp_tfidf_cv\n",
      "load mlp_hashing_cv\n",
      "load mlp_bow_cv\n",
      "load lr_tfidf_cv\n",
      "load lr_hashing_cv\n",
      "load lr_bow_cv\n",
      "nn_yes shape : (54879, 90), (19617, 90)\n",
      "load mlp_tfidf_cv\n",
      "load mlp_hashing_cv\n",
      "load mlp_bow_cv\n",
      "load lr_tfidf_cv\n",
      "load lr_hashing_cv\n",
      "load lr_bow_cv\n",
      "nn_yes shape : (54879, 90), (19617, 90)\n"
     ]
    }
   ],
   "source": [
    "nn_model_names= ['cnn_feature', 'lstm_feature' , 'mlp_feature', 'transformer_feature','transformer_v2_feature',\n",
    "               'cnn_tfidf', 'cnn_hashing', 'cnn_bow']\n",
    "\n",
    "ml_model_names= ['mlp_tfidf', 'mlp_hashing', 'mlp_bow','lr_tfidf','lr_hashing','lr_bow']\n",
    "\n",
    "\n",
    "trash = -1 # 의미없는 값\n",
    "stopwords_yes_kind = 2 # 의미있는 값\n",
    "stopwords_no_kind = 3 # 의미있는 값\n",
    "\n",
    "\n",
    "# stopwords-yes-nn 버전\n",
    "nn_yes_oof, nn_yes_test = load_feature(nn_model_names, trash, stopwords_yes_kind)\n",
    "nn_yes_oof = np.concatenate(nn_yes_oof, axis=1)\n",
    "nn_yes_test = np.concatenate(nn_yes_test, axis=1)\n",
    "print(f'nn_yes shape : {nn_yes_oof.shape}, {nn_yes_test.shape}')\n",
    "\n",
    "# stopwords-no-nn 버전\n",
    "nn_no_oof, nn_no_test = load_feature(nn_model_names, trash, stopwords_no_kind)\n",
    "nn_no_oof = np.concatenate(nn_no_oof, axis=1)\n",
    "nn_no_test = np.concatenate(nn_no_test, axis=1)\n",
    "print(f'nn_yes shape : {nn_no_oof.shape}, {nn_no_test.shape}')\n",
    "\n",
    "\n",
    "# stopwords-yes-ml 버전\n",
    "ml_yes_oof, ml_yes_test = load_feature(ml_model_names, trash, stopwords_yes_kind)\n",
    "ml_yes_oof = np.concatenate(ml_yes_oof, axis=1)\n",
    "ml_yes_test = np.concatenate(ml_yes_test, axis=1)\n",
    "print(f'nn_yes shape : {ml_yes_oof.shape}, {ml_yes_test.shape}')\n",
    "\n",
    "\n",
    "# stopwords-no-ml 버전\n",
    "ml_no_oof, ml_no_test = load_feature(ml_model_names, trash, stopwords_no_kind)\n",
    "ml_no_oof = np.concatenate(ml_no_oof, axis=1)\n",
    "ml_no_test = np.concatenate(ml_no_test, axis=1)\n",
    "print(f'nn_yes shape : {ml_no_oof.shape}, {ml_no_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(feature_target_file, index_col=0, usecols=['index',target_col]).values.flatten()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스태킹\n",
    "\n",
    "- 각 oof 마다 fold별로 logloos 변동이 있으므로 최대한 정보를 뽑아내고자 스태킹을 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 1000,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose' : 0,\n",
    "    'random_state': seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start : 1\n",
      "training model for CV #1\n",
      "training model for CV #2\n",
      "training model for CV #3\n",
      "training model for CV #4\n",
      "training model for CV #5\n",
      "end : 1\n",
      "start : 2\n",
      "training model for CV #1\n",
      "training model for CV #2\n",
      "training model for CV #3\n",
      "training model for CV #4\n",
      "training model for CV #5\n",
      "end : 2\n",
      "start : 3\n",
      "training model for CV #1\n",
      "training model for CV #2\n",
      "training model for CV #3\n",
      "training model for CV #4\n",
      "training model for CV #5\n",
      "end : 3\n",
      "start : 4\n",
      "training model for CV #1\n",
      "training model for CV #2\n",
      "training model for CV #3\n",
      "training model for CV #4\n",
      "training model for CV #5\n",
      "end : 4\n"
     ]
    }
   ],
   "source": [
    "datasrf = [(nn_yes_oof, nn_yes_test, y),\n",
    "            (nn_no_oof, nn_no_test, y),\n",
    "            (ml_yes_oof, ml_yes_test, y),\n",
    "            (ml_no_oof, ml_no_test, y)]\n",
    "\n",
    "mlogloss = []\n",
    "\n",
    "rf_oof_preds = []\n",
    "rf_test_preds = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "\n",
    "for number, (X, test , y) in enumerate(datasrf, 1):\n",
    "    print(f'start : {number}')\n",
    "    \n",
    "    rf_oof_pred = np.zeros((X.shape[0], n_class))\n",
    "    rf_test_pred = np.zeros((test.shape[0], n_class))\n",
    "    \n",
    "    for i, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n",
    "        print(f'training model for CV #{i}')\n",
    "        \n",
    "        X_train , X_val = X[i_trn], X[i_val]\n",
    "        y_train, y_val = y[i_trn], y[i_val]\n",
    "        \n",
    "        clf = RandomForestClassifier(**rf_params)\n",
    "        clf.fit(X_train,y_train)\n",
    "                \n",
    "        rf_oof_pred[i_val, :] = clf.predict_proba(X_val)\n",
    "        rf_test_pred += clf.predict_proba(test) / n_fold\n",
    "        mlogloss.append(log_loss(y_val,rf_oof_pred[i_val]))\n",
    "    rf_oof_preds.append(rf_oof_pred)\n",
    "    rf_test_preds.append(rf_test_pred)\n",
    "    \n",
    "    print(f'end : {number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss =   0.5673\n",
      "accuracy =  80.7996\n",
      "logloss =   0.5778\n",
      "accuracy =  80.4570\n",
      "logloss =   0.6200\n",
      "accuracy =  77.9205\n",
      "logloss =   0.5371\n",
      "accuracy =  81.3353\n",
      "mean logloss =  0.5755550022863718\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(rf_oof_preds,1):\n",
    "    print(f'logloss = {log_loss(pd.get_dummies(y),j):8.4f}')\n",
    "    print(f'accuracy = {accuracy_score(y, np.argmax(j,axis=1))*100:8.4f}')\n",
    "print('mean logloss = ',np.mean(mlogloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제출 파일 및 기타 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일 생성\n",
    "sub = pd.read_csv(sample_file,index_col=0)\n",
    "\n",
    "for filename, test_pred in zip(stacking_submission_files, rf_test_preds):\n",
    "    sub[sub.columns] = test_pred\n",
    "    sub.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_oof_pred 파일 생성\n",
    "\n",
    "for filename, oof_pred in zip(stacking_oof_pred_files, rf_oof_preds):\n",
    "    np.savetxt(filename, oof_pred, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_test_pred 파일 생성\n",
    "\n",
    "for filename, test_pred in zip(stacking_test_pred_files, rf_test_preds):\n",
    "    np.savetxt(filename, test_pred, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
