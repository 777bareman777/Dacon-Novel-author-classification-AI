{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data/dacon-novel-author-classification')\n",
    "feature_dir = Path('../build/feature')\n",
    "val_dir = Path('../build/val')\n",
    "tst_dir = Path('../build/tst')\n",
    "sub_dir = Path('../build/sub')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_name = 'xgb'\n",
    "\n",
    "feature_names= ['stacking-layer2']\n",
    "\n",
    "feature_target_file = feature_dir / f'feature_target.csv'\n",
    "\n",
    "model_names = []\n",
    "for feature_name in feature_names:\n",
    "    model_names.append(f'{algorithm_name}_{feature_name}')\n",
    "    \n",
    "stacking_oof_pred_files=[]\n",
    "for model_name in model_names:\n",
    "    stacking_oof_pred_files.append( val_dir / f'{model_name}_oof_pred.csv')\n",
    "    \n",
    "stacking_test_pred_files=[]\n",
    "for model_name in model_names:\n",
    "    stacking_test_pred_files.append( tst_dir / f'{model_name}_test_pred.csv')\n",
    "    \n",
    "stacking_submission_files=[]\n",
    "for model_name in model_names:\n",
    "    stacking_submission_files.append( sub_dir / f'{model_name}_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking feature 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(model_names, number_of_ver=None, kind=None):\n",
    "    oof_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    if number_of_ver==None or kind==None:\n",
    "        print('error')\n",
    "        return None\n",
    "    \n",
    "    # 딥러닝 시리즈 4가지 버전\n",
    "    if kind == 0:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            for i in range(1,number_of_ver+1):\n",
    "                oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv', delimiter=','))\n",
    "                test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "    \n",
    "    # 로지스틱 회귀 6가지 버전\n",
    "    elif kind == 1:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            for i in range(1, number_of_ver+1):\n",
    "                oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv', delimiter=','))\n",
    "                test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "\n",
    "    # 신경망 기반 불용어 처리 21가지 버전 또는 머신러닝 기반 불용어 처리 18가지 버전\n",
    "    elif kind == 2:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            if model.find('feature') != -1:\n",
    "                for i in range(2,5):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('tfidf') != -1:\n",
    "                for i in range(1,4):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('hashing') != -1:\n",
    "                for i in range(1,4):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('bow') != -1:\n",
    "                for i in range(1,4):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            else:\n",
    "                print('not found')\n",
    "    \n",
    "    # 신경만 기반 불용어 처리 X 13가지 버전 또는 머신러닝 기반 불용어 처리 X 18가지 버전\n",
    "    elif kind == 3:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            if model.find('feature') != -1:\n",
    "                for i in range(1,2):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('tfidf') != -1:\n",
    "                for i in range(4,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('hashing') != -1:\n",
    "                for i in range(4,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('bow') != -1:\n",
    "                for i in range(4,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            else:\n",
    "                print('not found')\n",
    "                \n",
    "    # 첫번째 레이어를 학습하기 위한 데이터셋 모두 가져오기 버전\n",
    "    elif kind == 4:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            if model.find('feature') != -1:\n",
    "                for i in range(1,5):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('tfidf') != -1:\n",
    "                for i in range(1,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('hashing') != -1:\n",
    "                for i in range(1,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('bow') != -1:\n",
    "                for i in range(1,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            else:\n",
    "                print('not found')\n",
    "    \n",
    "    # 두번째 레이어를 학습하기 위한 데이터셋 모두 가져오기 버전\n",
    "    elif kind == 5:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            if model.find('stacking') != -1:\n",
    "                for feature in ['stopwords-yes-nn','stopwords-no-nn','stopwords-no-ml', 'stopwords-no-ml'] :\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}-{feature}_oof_pred.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}-{feature}_test_pred.csv',delimiter=','))\n",
    "            elif model.find('all') != -1:\n",
    "                oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred.csv',delimiter=','))\n",
    "                test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred.csv',delimiter=','))\n",
    "    \n",
    "    return oof_list, test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ets_stacking-layer1_cv\n",
      "load gb_stacking-layer1_cv\n",
      "load lgbm_stacking-layer1_cv\n",
      "load mlp_stacking-layer1_cv\n",
      "load rf_stacking-layer1_cv\n",
      "load xgb_stacking-layer1_cv\n",
      "load ada_stacking-layer1_cv\n",
      "load xgb_all_cv\n",
      "load lgbm_all_cv\n",
      "nn_yes shape : (54879, 150), (19617, 150)\n"
     ]
    }
   ],
   "source": [
    "model_names = ['ets_stacking-layer1','gb_stacking-layer1','lgbm_stacking-layer1','mlp_stacking-layer1',\n",
    "              'rf_stacking-layer1','xgb_stacking-layer1','ada_stacking-layer1',\n",
    "              'xgb_all','lgbm_all']\n",
    "\n",
    "all_oof, all_test = load_feature(model_names, -1, 5)\n",
    "all_oof = np.concatenate(all_oof, axis=1)\n",
    "all_test = np.concatenate(all_test, axis=1)\n",
    "print(f'nn_yes shape : {all_oof.shape}, {all_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(feature_target_file, index_col=0, usecols=['index',target_col]).values.flatten()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스태킹\n",
    "\n",
    "- 각 oof 마다 fold별로 logloos 변동이 있으므로 최대한 정보를 뽑아내고자 스태킹을 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost\n",
    "xgb_params = {\n",
    "    'booster': 'gbtree',\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 1000,\n",
    "    'n_jobs':-1,\n",
    "    'num_class': n_class,\n",
    "    'objective': 'multi:softprob',\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'random_state': seed,\n",
    "    'tree_method': 'gpu_hist',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start : 1\n",
      "training model for CV #1\n",
      "[22:09:22] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.42836\tval-mlogloss:1.43096\n",
      "Multiple eval metrics have been passed: 'val-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[67]\ttrain-mlogloss:0.31719\tval-mlogloss:0.41389\n",
      "\n",
      "training model for CV #2\n",
      "[22:09:28] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.42811\tval-mlogloss:1.43325\n",
      "Multiple eval metrics have been passed: 'val-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[73]\ttrain-mlogloss:0.30951\tval-mlogloss:0.41821\n",
      "\n",
      "training model for CV #3\n",
      "[22:09:34] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.42904\tval-mlogloss:1.42993\n",
      "Multiple eval metrics have been passed: 'val-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[77]\ttrain-mlogloss:0.31286\tval-mlogloss:0.39769\n",
      "\n",
      "training model for CV #4\n",
      "[22:09:40] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.42831\tval-mlogloss:1.43249\n",
      "Multiple eval metrics have been passed: 'val-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[81]\ttrain-mlogloss:0.30358\tval-mlogloss:0.41807\n",
      "\n",
      "training model for CV #5\n",
      "[22:09:46] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { n_estimators } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.42841\tval-mlogloss:1.43106\n",
      "Multiple eval metrics have been passed: 'val-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-mlogloss hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[67]\ttrain-mlogloss:0.31539\tval-mlogloss:0.41475\n",
      "\n",
      "end : 1\n"
     ]
    }
   ],
   "source": [
    "datasets = [(all_oof, all_test, y)]\n",
    "\n",
    "mlogloss = []\n",
    "\n",
    "xgb_oof_preds = []\n",
    "xgb_test_preds = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "\n",
    "for number, (X, test , y) in enumerate(datasets, 1):\n",
    "    print(f'start : {number}')\n",
    "    \n",
    "    xgb_oof_pred = np.zeros((X.shape[0], n_class))\n",
    "    xgb_test_pred = np.zeros((test.shape[0], n_class))\n",
    "    \n",
    "    for i, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n",
    "        print(f'training model for CV #{i}')\n",
    "        \n",
    "        X_train , X_val = X[i_trn], X[i_val]\n",
    "        y_train, y_val = y[i_trn], y[i_val]\n",
    "        \n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dval, 'val')]\n",
    "        \n",
    "        clf = xgb.train(xgb_params, dtrain, 5000, evals=watchlist, early_stopping_rounds=50, verbose_eval=5000)\n",
    "        \n",
    "        dtest = xgb.DMatrix(test)\n",
    "        xgb_oof_pred[i_val, :] = clf.predict(dval)\n",
    "        xgb_test_pred += clf.predict(dtest) / n_fold\n",
    "        mlogloss.append(clf.best_score)\n",
    "    xgb_oof_preds.append(xgb_oof_pred)\n",
    "    xgb_test_preds.append(xgb_test_pred)\n",
    "    \n",
    "    print(f'end : {number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss =   0.4159\n",
      "accuracy =  85.0927\n",
      "mean logloss =  0.4125206\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(xgb_oof_preds,1):\n",
    "    print(f'logloss = {log_loss(pd.get_dummies(y),j):8.4f}')\n",
    "    print(f'accuracy = {accuracy_score(y, np.argmax(j,axis=1))*100:8.4f}')\n",
    "print('mean logloss = ',np.mean(mlogloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제출 파일 및 기타 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일 생성\n",
    "sub = pd.read_csv(sample_file,index_col=0)\n",
    "\n",
    "for filename, test_pred in zip(stacking_submission_files, xgb_test_preds):\n",
    "    sub[sub.columns] = test_pred\n",
    "    sub.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_oof_pred 파일 생성\n",
    "\n",
    "for filename, oof_pred in zip(stacking_oof_pred_files, xgb_oof_preds):\n",
    "    np.savetxt(filename, oof_pred, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_test_pred 파일 생성\n",
    "\n",
    "for filename, test_pred in zip(stacking_test_pred_files, xgb_test_preds):\n",
    "    np.savetxt(filename, test_pred, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
