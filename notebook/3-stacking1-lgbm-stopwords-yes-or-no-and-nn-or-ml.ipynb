{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data/dacon-novel-author-classification')\n",
    "feature_dir = Path('../build/feature')\n",
    "val_dir = Path('../build/val')\n",
    "tst_dir = Path('../build/tst')\n",
    "sub_dir = Path('../build/sub')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_name = 'lgbm'\n",
    "\n",
    "feature_names= ['stacking-layer1-stopwords-yes-nn',\n",
    "               'stacking-layer1-stopwords-no-nn',\n",
    "               'stacking-layer1-stopwords-yes-ml',\n",
    "               'stacking-layer1-stopwords-no-ml']\n",
    "\n",
    "feature_target_file = feature_dir / f'feature_target.csv'\n",
    "\n",
    "model_names = []\n",
    "for feature_name in feature_names:\n",
    "    model_names.append(f'{algorithm_name}_{feature_name}')\n",
    "    \n",
    "stacking_oof_pred_files=[]\n",
    "for model_name in model_names:\n",
    "    stacking_oof_pred_files.append( val_dir / f'{model_name}_oof_pred.csv')\n",
    "    \n",
    "stacking_test_pred_files=[]\n",
    "for model_name in model_names:\n",
    "    stacking_test_pred_files.append( tst_dir / f'{model_name}_test_pred.csv')\n",
    "    \n",
    "stacking_submission_files=[]\n",
    "for model_name in model_names:\n",
    "    stacking_submission_files.append( sub_dir / f'{model_name}_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking feature 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(model_names, number_of_ver=None, kind=None):\n",
    "    oof_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    if number_of_ver==None or kind==None:\n",
    "        print('error')\n",
    "        return None\n",
    "    \n",
    "    # 딥러닝 시리즈 4가지 버전\n",
    "    if kind == 0:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            for i in range(1,number_of_ver+1):\n",
    "                oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv', delimiter=','))\n",
    "                test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "    \n",
    "    # 로지스틱 회귀 6가지 버전\n",
    "    elif kind == 1:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            for i in range(1, number_of_ver+1):\n",
    "                oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv', delimiter=','))\n",
    "                test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "\n",
    "    # 신경망 기반 불용어 처리 21가지 버전 또는 머신러닝 기반 불용어 처리 18가지 버전\n",
    "    elif kind == 2:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            if model.find('feature') != -1:\n",
    "                for i in range(2,5):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('tfidf') != -1:\n",
    "                for i in range(1,4):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('hashing') != -1:\n",
    "                for i in range(1,4):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('bow') != -1:\n",
    "                for i in range(1,4):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            else:\n",
    "                print('not found')\n",
    "    \n",
    "    # 신경만 기반 불용어 처리 X 13가지 버전 또는 머신러닝 기반 불용어 처리 X 18가지 버전\n",
    "    elif kind == 3:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            if model.find('feature') != -1:\n",
    "                for i in range(1,2):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('tfidf') != -1:\n",
    "                for i in range(4,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('hashing') != -1:\n",
    "                for i in range(4,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('bow') != -1:\n",
    "                for i in range(4,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            else:\n",
    "                print('not found')\n",
    "                \n",
    "    # 모든 버전 가져오기\n",
    "    elif kind == 4:\n",
    "        for model in model_names:\n",
    "            print(f'load {model}_cv')\n",
    "            if model.find('feature') != -1:\n",
    "                for i in range(1,5):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('tfidf') != -1:\n",
    "                for i in range(1,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('hashing') != -1:\n",
    "                for i in range(1,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            elif model.find('bow') != -1:\n",
    "                for i in range(1,7):\n",
    "                    oof_list.append(np.loadtxt(val_dir / f'{model}_oof_pred_ver{i}.csv',delimiter=','))\n",
    "                    test_list.append(np.loadtxt(tst_dir / f'{model}_test_pred_ver{i}.csv', delimiter=','))\n",
    "            else:\n",
    "                print('not found')\n",
    "    \n",
    "    return oof_list, test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cnn_feature_cv\n",
      "load lstm_feature_cv\n",
      "load mlp_feature_cv\n",
      "load transformer_feature_cv\n",
      "load transformer_v2_feature_cv\n",
      "load cnn_tfidf_cv\n",
      "load cnn_hashing_cv\n",
      "load cnn_bow_cv\n",
      "nn_yes shape : (54879, 120), (19617, 120)\n",
      "load cnn_feature_cv\n",
      "load lstm_feature_cv\n",
      "load mlp_feature_cv\n",
      "load transformer_feature_cv\n",
      "load transformer_v2_feature_cv\n",
      "load cnn_tfidf_cv\n",
      "load cnn_hashing_cv\n",
      "load cnn_bow_cv\n",
      "nn_yes shape : (54879, 70), (19617, 70)\n",
      "load mlp_tfidf_cv\n",
      "load mlp_hashing_cv\n",
      "load mlp_bow_cv\n",
      "load lr_tfidf_cv\n",
      "load lr_hashing_cv\n",
      "load lr_bow_cv\n",
      "nn_yes shape : (54879, 90), (19617, 90)\n",
      "load mlp_tfidf_cv\n",
      "load mlp_hashing_cv\n",
      "load mlp_bow_cv\n",
      "load lr_tfidf_cv\n",
      "load lr_hashing_cv\n",
      "load lr_bow_cv\n",
      "nn_yes shape : (54879, 90), (19617, 90)\n"
     ]
    }
   ],
   "source": [
    "nn_model_names= ['cnn_feature', 'lstm_feature' , 'mlp_feature', 'transformer_feature','transformer_v2_feature',\n",
    "               'cnn_tfidf', 'cnn_hashing', 'cnn_bow']\n",
    "\n",
    "ml_model_names= ['mlp_tfidf', 'mlp_hashing', 'mlp_bow','lr_tfidf','lr_hashing','lr_bow']\n",
    "\n",
    "\n",
    "trash = -1 # 의미없는 값\n",
    "stopwords_yes_kind = 2 # 의미있는 값\n",
    "stopwords_no_kind = 3 # 의미있는 값\n",
    "\n",
    "\n",
    "# stopwords-yes-nn 버전\n",
    "nn_yes_oof, nn_yes_test = load_feature(nn_model_names, trash, stopwords_yes_kind)\n",
    "nn_yes_oof = np.concatenate(nn_yes_oof, axis=1)\n",
    "nn_yes_test = np.concatenate(nn_yes_test, axis=1)\n",
    "print(f'nn_yes shape : {nn_yes_oof.shape}, {nn_yes_test.shape}')\n",
    "\n",
    "# stopwords-no-nn 버전\n",
    "nn_no_oof, nn_no_test = load_feature(nn_model_names, trash, stopwords_no_kind)\n",
    "nn_no_oof = np.concatenate(nn_no_oof, axis=1)\n",
    "nn_no_test = np.concatenate(nn_no_test, axis=1)\n",
    "print(f'nn_yes shape : {nn_no_oof.shape}, {nn_no_test.shape}')\n",
    "\n",
    "\n",
    "# stopwords-yes-ml 버전\n",
    "ml_yes_oof, ml_yes_test = load_feature(ml_model_names, trash, stopwords_yes_kind)\n",
    "ml_yes_oof = np.concatenate(ml_yes_oof, axis=1)\n",
    "ml_yes_test = np.concatenate(ml_yes_test, axis=1)\n",
    "print(f'nn_yes shape : {ml_yes_oof.shape}, {ml_yes_test.shape}')\n",
    "\n",
    "\n",
    "# stopwords-no-ml 버전\n",
    "ml_no_oof, ml_no_test = load_feature(ml_model_names, trash, stopwords_no_kind)\n",
    "ml_no_oof = np.concatenate(ml_no_oof, axis=1)\n",
    "ml_no_test = np.concatenate(ml_no_test, axis=1)\n",
    "print(f'nn_yes shape : {ml_no_oof.shape}, {ml_no_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv(feature_target_file, index_col=0, usecols=['index',target_col]).values.flatten()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스태킹\n",
    "\n",
    "- 각 oof 마다 fold별로 logloos 변동이 있으므로 최대한 정보를 뽑아내고자 스태킹을 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm\n",
    "lgbm_params = {\n",
    "    'num_threads': -1, # aliases: n_jobs\n",
    "    'num_iterations': 1000, # aliases: n_estimators\n",
    "    'metric': 'multi_logloss',\n",
    "    'learning_rate': 0.3, # aliases: eta\n",
    "    'boosting': 'gbdt', # aliases: boosting_type\n",
    "    'objective': 'multiclass', # aliases: softmax\n",
    "    'num_class': n_class,\n",
    "    'random_state': seed,\n",
    "#     'device_type': 'gpu', # aliases: device\n",
    "#     'gpu_use_dp': 'true',\n",
    "    'verbosity': 0, # aliases: verbose\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start : 1\n",
      "training model for CV #1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021904 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's multi_logloss: 0.400496\tvalid_1's multi_logloss: 0.526596\n",
      "training model for CV #2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's multi_logloss: 0.377987\tvalid_1's multi_logloss: 0.53551\n",
      "training model for CV #3\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's multi_logloss: 0.384389\tvalid_1's multi_logloss: 0.520265\n",
      "training model for CV #4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019723 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's multi_logloss: 0.397309\tvalid_1's multi_logloss: 0.537919\n",
      "training model for CV #5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttraining's multi_logloss: 0.390417\tvalid_1's multi_logloss: 0.52813\n",
      "end : 1\n",
      "start : 2\n",
      "training model for CV #1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's multi_logloss: 0.423731\tvalid_1's multi_logloss: 0.54945\n",
      "training model for CV #2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's multi_logloss: 0.426799\tvalid_1's multi_logloss: 0.546575\n",
      "training model for CV #3\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's multi_logloss: 0.42433\tvalid_1's multi_logloss: 0.54992\n",
      "training model for CV #4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's multi_logloss: 0.432339\tvalid_1's multi_logloss: 0.5484\n",
      "training model for CV #5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\ttraining's multi_logloss: 0.39906\tvalid_1's multi_logloss: 0.545258\n",
      "end : 2\n",
      "start : 3\n",
      "training model for CV #1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's multi_logloss: 0.493714\tvalid_1's multi_logloss: 0.611779\n",
      "training model for CV #2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttraining's multi_logloss: 0.482247\tvalid_1's multi_logloss: 0.616558\n",
      "training model for CV #3\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016015 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's multi_logloss: 0.49897\tvalid_1's multi_logloss: 0.585454\n",
      "training model for CV #4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\ttraining's multi_logloss: 0.494746\tvalid_1's multi_logloss: 0.608412\n",
      "training model for CV #5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's multi_logloss: 0.501676\tvalid_1's multi_logloss: 0.617723\n",
      "end : 3\n",
      "start : 4\n",
      "training model for CV #1\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's multi_logloss: 0.418795\tvalid_1's multi_logloss: 0.526226\n",
      "training model for CV #2\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's multi_logloss: 0.427465\tvalid_1's multi_logloss: 0.532075\n",
      "training model for CV #3\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's multi_logloss: 0.42316\tvalid_1's multi_logloss: 0.50777\n",
      "training model for CV #4\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's multi_logloss: 0.427625\tvalid_1's multi_logloss: 0.524388\n",
      "training model for CV #5\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's multi_logloss: 0.418416\tvalid_1's multi_logloss: 0.525976\n",
      "end : 4\n"
     ]
    }
   ],
   "source": [
    "datasets = [(nn_yes_oof, nn_yes_test, y),\n",
    "            (nn_no_oof, nn_no_test, y),\n",
    "            (ml_yes_oof, ml_yes_test, y),\n",
    "            (ml_no_oof, ml_no_test, y)]\n",
    "\n",
    "mlogloss = []\n",
    "\n",
    "lgbm_oof_preds = []\n",
    "lgbm_test_preds = []\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "\n",
    "for number, (X, test , y) in enumerate(datasets, 1):\n",
    "    print(f'start : {number}')\n",
    "    \n",
    "    lgbm_oof_pred = np.zeros((X.shape[0], n_class))\n",
    "    lgbm_test_pred = np.zeros((test.shape[0], n_class))\n",
    "    \n",
    "    for i, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n",
    "        print(f'training model for CV #{i}')\n",
    "        \n",
    "        X_train , X_val = X[i_trn], X[i_val]\n",
    "        y_train, y_val = y[i_trn], y[i_val]\n",
    "        \n",
    "        dtrain = lgbm.Dataset(X_train, label=y_train)\n",
    "        dval = lgbm.Dataset(X_val, label=y_val)\n",
    "        watchlist = [(dtrain, 'train'), (dval, 'val')]\n",
    "        \n",
    "        clf = lgbm.train(params=lgbm_params, train_set=dtrain, num_boost_round=5000,\n",
    "                         valid_sets=[dtrain,dval], early_stopping_rounds=50, verbose_eval=5000)\n",
    "        \n",
    "        lgbm_oof_pred[i_val, :] = clf.predict(X_val)\n",
    "        lgbm_test_pred += clf.predict(test) / n_fold\n",
    "        mlogloss.append(clf.best_score['valid_1']['multi_logloss'])\n",
    "        \n",
    "    lgbm_oof_preds.append(lgbm_oof_pred)\n",
    "    lgbm_test_preds.append(lgbm_test_pred)\n",
    "    \n",
    "    print(f'end : {number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss =   0.5297\n",
      "accuracy =  80.8561\n",
      "logloss =   0.5479\n",
      "accuracy =  80.1964\n",
      "logloss =   0.6080\n",
      "accuracy =  77.5269\n",
      "logloss =   0.5233\n",
      "accuracy =  81.0237\n",
      "mean logloss =  0.5522192942045507\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(lgbm_oof_preds,1):\n",
    "    print(f'logloss = {log_loss(pd.get_dummies(y),j):8.4f}')\n",
    "    print(f'accuracy = {accuracy_score(y, np.argmax(j,axis=1))*100:8.4f}')\n",
    "print('mean logloss = ',np.mean(mlogloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제출 파일 및 기타 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일 생성\n",
    "sub = pd.read_csv(sample_file,index_col=0)\n",
    "\n",
    "for filename, test_pred in zip(stacking_submission_files, lgbm_test_preds):\n",
    "    sub[sub.columns] = test_pred\n",
    "    sub.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_oof_pred 파일 생성\n",
    "\n",
    "for filename, oof_pred in zip(stacking_oof_pred_files, lgbm_oof_preds):\n",
    "    np.savetxt(filename, oof_pred, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking_test_pred 파일 생성\n",
    "\n",
    "for filename, test_pred in zip(stacking_test_pred_files, lgbm_test_preds):\n",
    "    np.savetxt(filename, test_pred, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
