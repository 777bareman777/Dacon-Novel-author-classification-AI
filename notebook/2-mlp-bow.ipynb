{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 import 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:22.350713Z",
     "start_time": "2020-11-09T04:32:22.049823Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.188282Z",
     "start_time": "2020-11-09T04:32:22.352714Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.216883Z",
     "start_time": "2020-11-09T04:32:23.190555Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.250120Z",
     "start_time": "2020-11-09T04:32:23.219024Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path('../data/dacon-novel-author-classification')\n",
    "feature_dir = Path('../build/feature')\n",
    "val_dir = Path('../build/val')\n",
    "tst_dir = Path('../build/tst')\n",
    "sub_dir = Path('../build/sub')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.282083Z",
     "start_time": "2020-11-09T04:32:23.252439Z"
    }
   },
   "outputs": [],
   "source": [
    "algo_name = 'mlp'\n",
    "feature_name = 'bow'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "feature_file = feature_dir / f'{feature_name}.csv'\n",
    "\n",
    "p_val_ver1_file = val_dir / f'{model_name}_oof_pred_ver1.csv'\n",
    "p_tst_ver1_file = tst_dir / f'{model_name}_test_pred_ver1.csv'\n",
    "\n",
    "p_val_ver2_file = val_dir / f'{model_name}_oof_pred_ver2.csv'\n",
    "p_tst_ver2_file = tst_dir / f'{model_name}_test_pred_ver2.csv'\n",
    "\n",
    "p_val_ver3_file = val_dir / f'{model_name}_oof_pred_ver3.csv'\n",
    "p_tst_ver3_file = tst_dir / f'{model_name}_test_pred_ver3.csv'\n",
    "\n",
    "p_val_ver4_file = val_dir / f'{model_name}_oof_pred_ver4.csv'\n",
    "p_tst_ver4_file = tst_dir / f'{model_name}_test_pred_ver4.csv'\n",
    "\n",
    "p_val_ver5_file = val_dir / f'{model_name}_oof_pred_ver5.csv'\n",
    "p_tst_ver5_file = tst_dir / f'{model_name}_test_pred_ver5.csv'\n",
    "\n",
    "p_val_ver6_file = val_dir / f'{model_name}_oof_pred_ver6.csv'\n",
    "p_tst_ver6_file = tst_dir / f'{model_name}_test_pred_ver6.csv'\n",
    "\n",
    "sub_ver1_file = sub_dir / f'{model_name}_ver1.csv'\n",
    "sub_ver2_file = sub_dir / f'{model_name}_ver2.csv'\n",
    "sub_ver3_file = sub_dir / f'{model_name}_ver3.csv'\n",
    "sub_ver4_file = sub_dir / f'{model_name}_ver4.csv'\n",
    "sub_ver5_file = sub_dir / f'{model_name}_ver5.csv'\n",
    "sub_ver6_file = sub_dir / f'{model_name}_ver6.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.456017Z",
     "start_time": "2020-11-09T04:32:23.283900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      He was almost choking. There was so much, so m...       3\n",
       "1                 “Your sister asked for it, I suppose?”       2\n",
       "2       She was engaged one day as she walked, in per...       1\n",
       "3      The captain was in the porch, keeping himself ...       4\n",
       "4      “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn = pd.read_csv(trn_file, index_col=0)\n",
    "print(trn.shape)\n",
    "trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.580127Z",
     "start_time": "2020-11-09T04:32:23.458379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19617, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Not at all. I think she is one of the most ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As the lady had stated her intention of scream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“And then suddenly in the silence I heard a so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>His conviction remained unchanged. So far as I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "index                                                   \n",
       "0      “Not at all. I think she is one of the most ch...\n",
       "1      \"No,\" replied he, with sudden consciousness, \"...\n",
       "2      As the lady had stated her intention of scream...\n",
       "3      “And then suddenly in the silence I heard a so...\n",
       "4      His conviction remained unchanged. So far as I..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = pd.read_csv(tst_file, index_col=0)\n",
    "print(tst.shape)\n",
    "tst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 토큰화 비교, 어간 추출과 표제어 추출 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.609289Z",
     "start_time": "2020-11-09T04:32:23.583484Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLTK에 있는 단어 토큰화\n",
    "# -> Don't를 Do 와 n't로 분리, Jone's를 Jone 과 '로 분리.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK에 있는 단어 토큰화 \n",
    "# -> Don't를 Don 과 ' 와 t 로 분리, Jone's를 Jone 과 ' 와 s로 분리.\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# 케라스에 있는 단어 토큰화\n",
    "# -> 모든 알파벳을 소문자로 바꾸고, 온점이나, 컴마, 느낌표 등의 구두점을 제거.\n",
    "# -> 하지만 don't 나 jone's와 같은 경우 아포스트로피를 보존함.\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "# NLTK에 있는 어간 추출(Stemming) 표제어 추출(Lemmatization)\n",
    "\n",
    "# -> WordNetLemmatizer는 기본형을 추출, 속도가 오래 걸리고 복잡함.\n",
    "# -> Pos(Part of Speech)에 대한 설정이 없으면, 제대로된 어간을 추출하지 못할 수 있음.\n",
    "# -> 제대로된 어간을 추출하고 싶다면, 단어의 쓰임새를 알아야 됨.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# -> Poter Stemmer은 대표적인 문법 기준을 뽑아서, 추출하는 방식, 즉 어간 추출함. 어간은 단어의 의미를 담고 있는 핵심 부분임.\n",
    "# -> 영어의 접미사(suffix)를 제거해서, 단어의 의미를 담고있는 어간만 추출함\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# -> LancasterStemmer은 Poter Stemmer와 비슷하지만, 알고리즘이 다름.\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# -> Porter Stemmer의 개선판, Porter Stemmer2라고 보면 됨.\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk의 word_tokenize를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.645042Z",
     "start_time": "2020-11-09T04:32:23.612849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_1 = trn.text[4]\n",
    "print(s_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'his', 'hands', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', '....', 'Oh', ',', 'my', 'God', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_1 = word_tokenize(s_1)\n",
    "print(tokenized_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentleman', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', '....', 'Oh', ',', 'my', 'God', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_1 = [lemmatizer.lemmatize(t) for t in tokenized_word_1]\n",
    "print(tokenized_lemmatizer_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'hi', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'I', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'Oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_1 = [porterStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_porter_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'hav', 'mercy', ',', 'gentlem', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'writ', 'that', ',', 'anyway', ';', 'hav', 'som', 'sham', '.', 'her', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'bef', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_1 = [lancasterStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_lancaster_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_1 = [snowballStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_snowball_word_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk의 WordPunctTokenizer를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_2 = trn.text[4]\n",
    "print(s_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'his', 'hands', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', '....', 'Oh', ',', 'my', 'God', '!”']\n"
     ]
    }
   ],
   "source": [
    "wordPunctTokenizer = WordPunctTokenizer()\n",
    "tokenized_word_2 = wordPunctTokenizer.tokenize(s_2)\n",
    "print(tokenized_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentleman', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', '....', 'Oh', ',', 'my', 'God', '!”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_2 = [lemmatizer.lemmatize(t) for t in tokenized_word_2]\n",
    "print(tokenized_lemmatizer_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'hi', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'I', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'Oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_2 = [porterStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_porter_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'hav', 'mercy', ',', 'gentlem', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'writ', 'that', ',', 'anyway', ';', 'hav', 'som', 'sham', '.', 'her', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'bef', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_2 = [lancasterStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_lancaster_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_2 = [snowballStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_snowball_word_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kerasd의 text_to_word_sequence를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_3 = trn.text[4]\n",
    "print(s_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentlemen', '”', 'odin', 'flung', 'up', 'his', 'hands', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’ve', 'torn', 'my', 'heart', 'asunder', 'before', 'you', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_3 = text_to_word_sequence(s_3)\n",
    "print(tokenized_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentleman', '”', 'odin', 'flung', 'up', 'his', 'hand', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’ve', 'torn', 'my', 'heart', 'asunder', 'before', 'you', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_3 = [lemmatizer.lemmatize(t) for t in tokenized_word_3]\n",
    "print(tokenized_lemmatizer_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'merci', 'gentlemen', '”', 'odin', 'flung', 'up', 'hi', 'hand', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’v', 'torn', 'my', 'heart', 'asund', 'befor', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_3 = [porterStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_porter_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentlem', '”', 'odin', 'flung', 'up', 'his', 'hand', '“don’t', 'writ', 'that', 'anyway', 'hav', 'som', 'sham', 'her', 'i’ve', 'torn', 'my', 'heart', 'asund', 'bef', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_3 = [lancasterStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_lancaster_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'merci', 'gentlemen', '”', 'odin', 'flung', 'up', 'his', 'hand', \"“don't\", 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', \"i'v\", 'torn', 'my', 'heart', 'asund', 'befor', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_3 = [snowballStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_snowball_word_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T14:54:47.082620Z",
     "start_time": "2020-11-04T14:54:47.055487Z"
    }
   },
   "source": [
    "## DTM 피쳐 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 word_tokenize 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.310245Z",
     "start_time": "2020-11-09T04:44:31.758835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2685) (19617, 2685)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 2), min_df=100)\n",
    "X_1 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_1 = vec.transform(tst['text']).toarray()\n",
    "print(X_1.shape, X_tst_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.342347Z",
     "start_time": "2020-11-09T04:45:10.312078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 WordPunctTokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2655) (19617, 2655)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(tokenizer=wordPunctTokenizer.tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 2), min_df=100)\n",
    "X_2 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_2 = vec.transform(tst['text']).toarray()\n",
    "print(X_2.shape, X_tst_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 4, 0, 0, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras의 text_to_word_sequence 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1907) (19617, 1907)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(tokenizer=text_to_word_sequence, stop_words=stopwords.words('english'), ngram_range=(1, 2), min_df=100)\n",
    "X_3 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_3 = vec.transform(tst['text']).toarray()\n",
    "print(X_3.shape, X_tst_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 word_tokenize 사용 , stopword 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.310245Z",
     "start_time": "2020-11-09T04:44:31.758835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 4720) (19617, 4720)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(tokenizer=word_tokenize, ngram_range=(1, 2), min_df=100)\n",
    "X_4 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_4 = vec.transform(tst['text']).toarray()\n",
    "print(X_4.shape, X_tst_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.342347Z",
     "start_time": "2020-11-09T04:45:10.312078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_4[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 WordPunctTokenizer 사용 , stopword 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 4777) (19617, 4777)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(tokenizer=wordPunctTokenizer.tokenize, ngram_range=(1, 2), min_df=100)\n",
    "X_5 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_5 = vec.transform(tst['text']).toarray()\n",
    "print(X_5.shape, X_tst_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_5[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras의 text_to_word_sequence 사용 , stopword 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 4091) (19617, 4091)\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(tokenizer=text_to_word_sequence, ngram_range=(1, 2), min_df=100)\n",
    "X_6 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_6 = vec.transform(tst['text']).toarray()\n",
    "print(X_6.shape, X_tst_6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_6[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T15:15:47.430701Z",
     "start_time": "2020-11-04T15:15:47.404265Z"
    }
   },
   "source": [
    "## mlp 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.370865Z",
     "start_time": "2020-11-09T04:45:10.344734Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(number):\n",
    "    inputs = Input(shape=(number,))\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    outputs = Dense(n_class, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.399912Z",
     "start_time": "2020-11-09T04:45:10.373016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = trn.author.values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:30.682036Z",
     "start_time": "2020-11-09T04:45:10.401772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.1075 - val_loss: 0.8189\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7186 - val_loss: 0.7328\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6305 - val_loss: 0.7122\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5849 - val_loss: 0.7078\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5537 - val_loss: 0.7098\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5289 - val_loss: 0.7144\n",
      "Epoch 7/100\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.5070Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5073 - val_loss: 0.7204\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.0732 - val_loss: 0.8145\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.7129 - val_loss: 0.7329\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6279 - val_loss: 0.7175\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.5843 - val_loss: 0.7156\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.5539 - val_loss: 0.7196\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5311 - val_loss: 0.7242\n",
      "Epoch 7/100\n",
      "80/86 [==========================>...] - ETA: 0s - loss: 0.5098Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5107 - val_loss: 0.7336\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.0651 - val_loss: 0.7834\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7152 - val_loss: 0.7022\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6336 - val_loss: 0.6846\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.5935 - val_loss: 0.6843\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5653 - val_loss: 0.6906\n",
      "Epoch 6/100\n",
      "79/86 [==========================>...] - ETA: 0s - loss: 0.5402Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5437 - val_loss: 0.6997\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.0615 - val_loss: 0.8094\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7111 - val_loss: 0.7296\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6281 - val_loss: 0.7112\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5863 - val_loss: 0.7092\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5578 - val_loss: 0.7137\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5357 - val_loss: 0.7209\n",
      "Epoch 7/100\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.5147Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.5165 - val_loss: 0.7239\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.0706 - val_loss: 0.8084\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7110 - val_loss: 0.7255\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6264 - val_loss: 0.7109\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5817 - val_loss: 0.7100\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5498 - val_loss: 0.7121\n",
      "Epoch 6/100\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.5251Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5271 - val_loss: 0.7202\n",
      "Epoch 00006: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 1.0905 - val_loss: 0.8146\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7120 - val_loss: 0.7228\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6253 - val_loss: 0.7056\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5812 - val_loss: 0.7024\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.5524 - val_loss: 0.7061\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.5295 - val_loss: 0.7156\n",
      "Epoch 7/100\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.5095Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5097 - val_loss: 0.7206\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.0816 - val_loss: 0.8148\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7055 - val_loss: 0.7301\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6196 - val_loss: 0.7175\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5756 - val_loss: 0.7148\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.5475 - val_loss: 0.7233\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5243 - val_loss: 0.7306\n",
      "Epoch 7/100\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.5025Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5046 - val_loss: 0.7343\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.0995 - val_loss: 0.7873\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7160 - val_loss: 0.7014\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6302 - val_loss: 0.6817\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.5863 - val_loss: 0.6801\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5555 - val_loss: 0.6883\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5335 - val_loss: 0.6913\n",
      "Epoch 7/100\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.5130Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5132 - val_loss: 0.6984\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.0731 - val_loss: 0.8057\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7059 - val_loss: 0.7185\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6249 - val_loss: 0.7043\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5840 - val_loss: 0.7055\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.5564 - val_loss: 0.7115\n",
      "Epoch 6/100\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.5318Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5341 - val_loss: 0.7177\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 1.0757 - val_loss: 0.8058\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.7093 - val_loss: 0.7211\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.6228 - val_loss: 0.7090\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5779 - val_loss: 0.7046\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5480 - val_loss: 0.7123\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5237 - val_loss: 0.7171\n",
      "Epoch 7/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.5023Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.5032 - val_loss: 0.7252\n",
      "Epoch 00007: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1962 - val_loss: 0.9328\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.8388 - val_loss: 0.8482\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7594 - val_loss: 0.8344\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7231 - val_loss: 0.8358\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7001 - val_loss: 0.8388\n",
      "Epoch 6/100\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.6796Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.6795 - val_loss: 0.8415\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.2109 - val_loss: 0.9493\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.8384 - val_loss: 0.8569\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7550 - val_loss: 0.8435\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.7178 - val_loss: 0.8427\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.6929 - val_loss: 0.8486\n",
      "Epoch 6/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.6738Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.6735 - val_loss: 0.8520\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1997 - val_loss: 0.9177\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.8442 - val_loss: 0.8273\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7635 - val_loss: 0.8138\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7274 - val_loss: 0.8123\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7028 - val_loss: 0.8144\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.6828 - val_loss: 0.8184\n",
      "Epoch 7/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.6646Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6648 - val_loss: 0.8218\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.2121 - val_loss: 0.9452\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.8423 - val_loss: 0.8461\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7610 - val_loss: 0.8317\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7235 - val_loss: 0.8321\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7000 - val_loss: 0.8359\n",
      "Epoch 6/100\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.6803Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.6812 - val_loss: 0.8364\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 1.1982 - val_loss: 0.9405\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.8374 - val_loss: 0.8514\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7578 - val_loss: 0.8350\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7203 - val_loss: 0.8342\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.6969 - val_loss: 0.8385\n",
      "Epoch 6/100\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.6749Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.6764 - val_loss: 0.8443\n",
      "Epoch 00006: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.9472 - val_loss: 0.6969\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5855 - val_loss: 0.6321\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4893 - val_loss: 0.6259\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4344 - val_loss: 0.6226\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3934 - val_loss: 0.6365\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3600 - val_loss: 0.6549\n",
      "Epoch 7/100\n",
      "80/86 [==========================>...] - ETA: 0s - loss: 0.3303Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3314 - val_loss: 0.6794\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.9674 - val_loss: 0.7200\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5915 - val_loss: 0.6328\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4888 - val_loss: 0.6121\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4299 - val_loss: 0.6131\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3859 - val_loss: 0.6244\n",
      "Epoch 6/100\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.3479Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3495 - val_loss: 0.6383\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.9435 - val_loss: 0.6761\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5844 - val_loss: 0.6031\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4913 - val_loss: 0.5896\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4364 - val_loss: 0.5960\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3999 - val_loss: 0.6104\n",
      "Epoch 6/100\n",
      "79/86 [==========================>...] - ETA: 0s - loss: 0.3710Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3703 - val_loss: 0.6235\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.9363 - val_loss: 0.6950\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5828 - val_loss: 0.6292\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4882 - val_loss: 0.6196\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4341 - val_loss: 0.6165\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3961 - val_loss: 0.6276\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3648 - val_loss: 0.6439\n",
      "Epoch 7/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.3377Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3383 - val_loss: 0.6567\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.9513 - val_loss: 0.6929\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5893 - val_loss: 0.6188\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4893 - val_loss: 0.5958\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4332 - val_loss: 0.5991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3927 - val_loss: 0.6135\n",
      "Epoch 6/100\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.3577Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3591 - val_loss: 0.6222\n",
      "Epoch 00006: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.9538 - val_loss: 0.6959\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5806 - val_loss: 0.6315\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4834 - val_loss: 0.6150\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4280 - val_loss: 0.6269\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 10ms/step - loss: 0.3884 - val_loss: 0.6382\n",
      "Epoch 6/100\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.3559Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3578 - val_loss: 0.6548\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.9493 - val_loss: 0.7006\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5832 - val_loss: 0.6293\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4868 - val_loss: 0.6170\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4309 - val_loss: 0.6181\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3924 - val_loss: 0.6325\n",
      "Epoch 6/100\n",
      "79/86 [==========================>...] - ETA: 0s - loss: 0.3598Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3606 - val_loss: 0.6529\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.9654 - val_loss: 0.6782\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5885 - val_loss: 0.6059\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4902 - val_loss: 0.5925\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4315 - val_loss: 0.5904\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3907 - val_loss: 0.6029\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3578 - val_loss: 0.6219\n",
      "Epoch 7/100\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.3291Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3291 - val_loss: 0.6313\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.9509 - val_loss: 0.6995\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5816 - val_loss: 0.6247\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4844 - val_loss: 0.6098\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4267 - val_loss: 0.6148\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3845 - val_loss: 0.6203\n",
      "Epoch 6/100\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.3493Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3495 - val_loss: 0.6440\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 14ms/step - loss: 0.9488 - val_loss: 0.6912\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5838 - val_loss: 0.6165\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4893 - val_loss: 0.5998\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4356 - val_loss: 0.6048\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3992 - val_loss: 0.6114\n",
      "Epoch 6/100\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.3686Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.3700 - val_loss: 0.6312\n",
      "Epoch 00006: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 1.0959 - val_loss: 0.8253\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.7065 - val_loss: 0.7429\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.6047 - val_loss: 0.7349\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.5510 - val_loss: 0.7409\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.5122 - val_loss: 0.7585\n",
      "Epoch 6/100\n",
      "80/86 [==========================>...] - ETA: 0s - loss: 0.4792Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4808 - val_loss: 0.7741\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 1.0652 - val_loss: 0.8226\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.6988 - val_loss: 0.7494\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.6031 - val_loss: 0.7372\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5513 - val_loss: 0.7459\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.5147 - val_loss: 0.7573\n",
      "Epoch 6/100\n",
      "80/86 [==========================>...] - ETA: 0s - loss: 0.4839Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.4861 - val_loss: 0.7739\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 1.0737 - val_loss: 0.7960\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.7049 - val_loss: 0.7198\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.6092 - val_loss: 0.7077\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.5575 - val_loss: 0.7107\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.5215 - val_loss: 0.7242\n",
      "Epoch 6/100\n",
      "81/86 [===========================>..] - ETA: 0s - loss: 0.4902Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.4912 - val_loss: 0.7429\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 13ms/step - loss: 1.0744 - val_loss: 0.8230\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.7004 - val_loss: 0.7407\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.5996 - val_loss: 0.7313\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.5455 - val_loss: 0.7422\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.5066 - val_loss: 0.7526\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4742Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.4742 - val_loss: 0.7732\n",
      "Epoch 00006: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 12ms/step - loss: 1.0627 - val_loss: 0.8143\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.7027 - val_loss: 0.7370\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.6060 - val_loss: 0.7242\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.5533 - val_loss: 0.7301\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 1s 8ms/step - loss: 0.5159 - val_loss: 0.7425\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.4854Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 1s 9ms/step - loss: 0.4854 - val_loss: 0.7621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "lr ver1 Accuracy (CV):  73.5855%\n",
      "lr ver1 Log Loss (CV):   0.7056\n",
      "lr ver2 Accuracy (CV):  73.8862%\n",
      "lr ver2 Log Loss (CV):   0.7012\n",
      "lr ver3 Accuracy (CV):  68.4652%\n",
      "lr ver3 Log Loss (CV):   0.8314\n",
      "lr ver4 Accuracy (CV):  77.9351%\n",
      "lr ver4 Log Loss (CV):   0.6073\n",
      "lr ver5 Accuracy (CV):  78.0572%\n",
      "lr ver5 Log Loss (CV):   0.6064\n",
      "lr ver6 Accuracy (CV):  73.2102%\n",
      "lr ver6 Log Loss (CV):   0.7271\n"
     ]
    }
   ],
   "source": [
    "p_val_ver1 = np.zeros((X_1.shape[0], n_class))\n",
    "p_tst_ver1 = np.zeros((X_tst_1.shape[0], n_class))\n",
    "p_val_ver2 = np.zeros((X_2.shape[0], n_class))\n",
    "p_tst_ver2 = np.zeros((X_tst_2.shape[0], n_class))\n",
    "p_val_ver3 = np.zeros((X_3.shape[0], n_class))\n",
    "p_tst_ver3 = np.zeros((X_tst_3.shape[0], n_class))\n",
    "p_val_ver4 = np.zeros((X_4.shape[0], n_class))\n",
    "p_tst_ver4 = np.zeros((X_tst_4.shape[0], n_class))\n",
    "p_val_ver5 = np.zeros((X_5.shape[0], n_class))\n",
    "p_tst_ver5 = np.zeros((X_tst_5.shape[0], n_class))\n",
    "p_val_ver6 = np.zeros((X_6.shape[0], n_class))\n",
    "p_tst_ver6 = np.zeros((X_tst_6.shape[0], n_class))\n",
    "\n",
    "for X, test in [(X_1, X_tst_1), (X_2, X_tst_2), (X_3, X_tst_3),\n",
    "               (X_4, X_tst_4), (X_5, X_tst_5), (X_6, X_tst_6)]: \n",
    "    for i_cv, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n",
    "        print(f'Training model for CV #{i_cv}')\n",
    "        X_train, X_val = X[i_trn], X[i_val]\n",
    "        y_train, y_val = y[i_trn], y[i_val]\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
    "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
    "        \n",
    "        clf = get_model(X.shape[1])\n",
    "        clf.fit(X[i_trn], \n",
    "            to_categorical(y[i_trn]),\n",
    "            validation_data=(X[i_val], to_categorical(y[i_val])),\n",
    "            epochs=100,\n",
    "            batch_size=512,\n",
    "            callbacks=[es])\n",
    "       \n",
    "        # Predict\n",
    "        if X.shape[1]==2685:\n",
    "            p_val_ver1[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver1 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==2655:\n",
    "            p_val_ver2[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver2 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==1907:\n",
    "            p_val_ver3[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver3 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==4720:\n",
    "            p_val_ver4[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver4 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==4777:\n",
    "            p_val_ver5[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver5 += clf.predict(test) / n_class\n",
    "        else:\n",
    "            p_val_ver6[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver6 += clf.predict(test) / n_class\n",
    "            \n",
    "        del clf\n",
    "        clear_session()\n",
    "        gc.collect()\n",
    "            \n",
    "    print(\"Training has finished\")\n",
    "    print(\"*\"*100)\n",
    "\n",
    "            \n",
    "print(f'lr ver1 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver1, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver1 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver1):8.4f}')\n",
    "print(f'lr ver2 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver2, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver2 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver2):8.4f}')\n",
    "print(f'lr ver3 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver3, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver3 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver3):8.4f}')\n",
    "print(f'lr ver4 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver4, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver4 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver4):8.4f}')\n",
    "print(f'lr ver5 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver5, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver5 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver5):8.4f}')\n",
    "print(f'lr ver6 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver6, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver6 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver6):8.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제출 파일 생성 및 기타 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일 생성\n",
    "\n",
    "sub = pd.read_csv(sample_file, index_col=0)\n",
    "\n",
    "# Ver1\n",
    "sub[sub.columns] = p_tst_ver1\n",
    "sub.to_csv(sub_ver1_file)\n",
    "\n",
    "# Ver2\n",
    "sub[sub.columns] = p_tst_ver2\n",
    "sub.to_csv(sub_ver2_file)\n",
    "\n",
    "# Ver3\n",
    "sub[sub.columns] = p_tst_ver3\n",
    "sub.to_csv(sub_ver3_file)\n",
    "\n",
    "# Ver4\n",
    "sub[sub.columns] = p_tst_ver4\n",
    "sub.to_csv(sub_ver4_file)\n",
    "\n",
    "# Ver5\n",
    "sub[sub.columns] = p_tst_ver5\n",
    "sub.to_csv(sub_ver5_file)\n",
    "\n",
    "# Ver6\n",
    "sub[sub.columns] = p_tst_ver6\n",
    "sub.to_csv(sub_ver6_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:31.272596Z",
     "start_time": "2020-11-09T04:45:31.074976Z"
    }
   },
   "outputs": [],
   "source": [
    "# p_val 파일 생성 -> oof\n",
    "\n",
    "# Ver1\n",
    "np.savetxt(p_val_ver1_file, p_val_ver1, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(p_val_ver2_file, p_val_ver2, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(p_val_ver3_file, p_val_ver3, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(p_val_ver4_file, p_val_ver4, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver5\n",
    "np.savetxt(p_val_ver5_file, p_val_ver5, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver6\n",
    "np.savetxt(p_val_ver6_file, p_val_ver6, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_tst 파일 생성 -> test \n",
    "\n",
    "# Ver1\n",
    "np.savetxt(p_tst_ver1_file, p_tst_ver1, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(p_tst_ver2_file, p_tst_ver2, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(p_tst_ver3_file, p_tst_ver3, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(p_tst_ver4_file, p_tst_ver4, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver5\n",
    "np.savetxt(p_tst_ver5_file, p_tst_ver5, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver6\n",
    "np.savetxt(p_tst_ver6_file, p_tst_ver6, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
