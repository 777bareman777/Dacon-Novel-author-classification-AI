{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 import 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:22.350713Z",
     "start_time": "2020-11-09T04:32:22.049823Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.188282Z",
     "start_time": "2020-11-09T04:32:22.352714Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.216883Z",
     "start_time": "2020-11-09T04:32:23.190555Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.250120Z",
     "start_time": "2020-11-09T04:32:23.219024Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path('../data/dacon-novel-author-classification')\n",
    "feature_dir = Path('../build/feature')\n",
    "val_dir = Path('../build/val')\n",
    "tst_dir = Path('../build/tst')\n",
    "sub_dir = Path('../build/sub')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.282083Z",
     "start_time": "2020-11-09T04:32:23.252439Z"
    }
   },
   "outputs": [],
   "source": [
    "algo_name = 'cnn'\n",
    "feature_name = 'tfidf'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "feature_file = feature_dir / f'{feature_name}.csv'\n",
    "\n",
    "p_val_ver1_file = val_dir / f'{model_name}_oof_pred_ver1.csv'\n",
    "p_tst_ver1_file = tst_dir / f'{model_name}_test_pred_ver1.csv'\n",
    "\n",
    "p_val_ver2_file = val_dir / f'{model_name}_oof_pred_ver2.csv'\n",
    "p_tst_ver2_file = tst_dir / f'{model_name}_test_pred_ver2.csv'\n",
    "\n",
    "p_val_ver3_file = val_dir / f'{model_name}_oof_pred_ver3.csv'\n",
    "p_tst_ver3_file = tst_dir / f'{model_name}_test_pred_ver3.csv'\n",
    "\n",
    "p_val_ver4_file = val_dir / f'{model_name}_oof_pred_ver4.csv'\n",
    "p_tst_ver4_file = tst_dir / f'{model_name}_test_pred_ver4.csv'\n",
    "\n",
    "p_val_ver5_file = val_dir / f'{model_name}_oof_pred_ver5.csv'\n",
    "p_tst_ver5_file = tst_dir / f'{model_name}_test_pred_ver5.csv'\n",
    "\n",
    "p_val_ver6_file = val_dir / f'{model_name}_oof_pred_ver6.csv'\n",
    "p_tst_ver6_file = tst_dir / f'{model_name}_test_pred_ver6.csv'\n",
    "\n",
    "sub_ver1_file = sub_dir / f'{model_name}_ver1.csv'\n",
    "sub_ver2_file = sub_dir / f'{model_name}_ver2.csv'\n",
    "sub_ver3_file = sub_dir / f'{model_name}_ver3.csv'\n",
    "sub_ver4_file = sub_dir / f'{model_name}_ver4.csv'\n",
    "sub_ver5_file = sub_dir / f'{model_name}_ver5.csv'\n",
    "sub_ver6_file = sub_dir / f'{model_name}_ver6.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.456017Z",
     "start_time": "2020-11-09T04:32:23.283900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      He was almost choking. There was so much, so m...       3\n",
       "1                 “Your sister asked for it, I suppose?”       2\n",
       "2       She was engaged one day as she walked, in per...       1\n",
       "3      The captain was in the porch, keeping himself ...       4\n",
       "4      “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn = pd.read_csv(trn_file, index_col=0)\n",
    "print(trn.shape)\n",
    "trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.580127Z",
     "start_time": "2020-11-09T04:32:23.458379Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19617, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Not at all. I think she is one of the most ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As the lady had stated her intention of scream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“And then suddenly in the silence I heard a so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>His conviction remained unchanged. So far as I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "index                                                   \n",
       "0      “Not at all. I think she is one of the most ch...\n",
       "1      \"No,\" replied he, with sudden consciousness, \"...\n",
       "2      As the lady had stated her intention of scream...\n",
       "3      “And then suddenly in the silence I heard a so...\n",
       "4      His conviction remained unchanged. So far as I..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = pd.read_csv(tst_file, index_col=0)\n",
    "print(tst.shape)\n",
    "tst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 토큰화 비교, 어간 추출과 표제어 추출 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.609289Z",
     "start_time": "2020-11-09T04:32:23.583484Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLTK에 있는 단어 토큰화\n",
    "# -> Don't를 Do 와 n't로 분리, Jone's를 Jone 과 '로 분리.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK에 있는 단어 토큰화 \n",
    "# -> Don't를 Don 과 ' 와 t 로 분리, Jone's를 Jone 과 ' 와 s로 분리.\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# 케라스에 있는 단어 토큰화\n",
    "# -> 모든 알파벳을 소문자로 바꾸고, 온점이나, 컴마, 느낌표 등의 구두점을 제거.\n",
    "# -> 하지만 don't 나 jone's와 같은 경우 아포스트로피를 보존함.\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "# NLTK에 있는 어간 추출(Stemming) 표제어 추출(Lemmatization)\n",
    "\n",
    "# -> WordNetLemmatizer는 기본형을 추출, 속도가 오래 걸리고 복잡함.\n",
    "# -> Pos(Part of Speech)에 대한 설정이 없으면, 제대로된 어간을 추출하지 못할 수 있음.\n",
    "# -> 제대로된 어간을 추출하고 싶다면, 단어의 쓰임새를 알아야 됨.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# -> Poter Stemmer은 대표적인 문법 기준을 뽑아서, 추출하는 방식, 즉 어간 추출함. 어간은 단어의 의미를 담고 있는 핵심 부분임.\n",
    "# -> 영어의 접미사(suffix)를 제거해서, 단어의 의미를 담고있는 어간만 추출함\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# -> LancasterStemmer은 Poter Stemmer와 비슷하지만, 알고리즘이 다름.\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# -> Porter Stemmer의 개선판, Porter Stemmer2라고 보면 됨.\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk의 word_tokenize를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.645042Z",
     "start_time": "2020-11-09T04:32:23.612849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_1 = trn.text[4]\n",
    "print(s_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'his', 'hands', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', '....', 'Oh', ',', 'my', 'God', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_1 = word_tokenize(s_1)\n",
    "print(tokenized_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentleman', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', '....', 'Oh', ',', 'my', 'God', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_1 = [lemmatizer.lemmatize(t) for t in tokenized_word_1]\n",
    "print(tokenized_lemmatizer_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'hi', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'I', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'Oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_1 = [porterStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_porter_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'hav', 'mercy', ',', 'gentlem', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'writ', 'that', ',', 'anyway', ';', 'hav', 'som', 'sham', '.', 'her', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'bef', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_1 = [lancasterStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_lancaster_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_1 = [snowballStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_snowball_word_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk의 WordPunctTokenizer를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_2 = trn.text[4]\n",
    "print(s_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'his', 'hands', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', '....', 'Oh', ',', 'my', 'God', '!”']\n"
     ]
    }
   ],
   "source": [
    "wordPunctTokenizer = WordPunctTokenizer()\n",
    "tokenized_word_2 = wordPunctTokenizer.tokenize(s_2)\n",
    "print(tokenized_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentleman', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', '....', 'Oh', ',', 'my', 'God', '!”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_2 = [lemmatizer.lemmatize(t) for t in tokenized_word_2]\n",
    "print(tokenized_lemmatizer_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'hi', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'I', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'Oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_2 = [porterStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_porter_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'hav', 'mercy', ',', 'gentlem', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'writ', 'that', ',', 'anyway', ';', 'hav', 'som', 'sham', '.', 'her', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'bef', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_2 = [lancasterStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_lancaster_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_2 = [snowballStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_snowball_word_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras의 text_to_word_sequence를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_3 = trn.text[4]\n",
    "print(s_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentlemen', '”', 'odin', 'flung', 'up', 'his', 'hands', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’ve', 'torn', 'my', 'heart', 'asunder', 'before', 'you', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_3 = text_to_word_sequence(s_3)\n",
    "print(tokenized_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentleman', '”', 'odin', 'flung', 'up', 'his', 'hand', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’ve', 'torn', 'my', 'heart', 'asunder', 'before', 'you', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_3 = [lemmatizer.lemmatize(t) for t in tokenized_word_3]\n",
    "print(tokenized_lemmatizer_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'merci', 'gentlemen', '”', 'odin', 'flung', 'up', 'hi', 'hand', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’v', 'torn', 'my', 'heart', 'asund', 'befor', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_3 = [porterStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_porter_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentlem', '”', 'odin', 'flung', 'up', 'his', 'hand', '“don’t', 'writ', 'that', 'anyway', 'hav', 'som', 'sham', 'her', 'i’ve', 'torn', 'my', 'heart', 'asund', 'bef', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_3 = [lancasterStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_lancaster_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'merci', 'gentlemen', '”', 'odin', 'flung', 'up', 'his', 'hand', \"“don't\", 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', \"i'v\", 'torn', 'my', 'heart', 'asund', 'befor', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_3 = [snowballStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_snowball_word_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T14:54:47.082620Z",
     "start_time": "2020-11-04T14:54:47.055487Z"
    }
   },
   "source": [
    "## TF-IDF 피쳐 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 word_tokenize 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.310245Z",
     "start_time": "2020-11-09T04:44:31.758835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 5897) (19617, 5897)\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "X_1 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_1 = vec.transform(tst['text']).toarray()\n",
    "print(X_1.shape, X_tst_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.342347Z",
     "start_time": "2020-11-09T04:45:10.312078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 WordPunctTokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 5772) (19617, 5772)\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(tokenizer=wordPunctTokenizer.tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "X_2 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_2 = vec.transform(tst['text']).toarray()\n",
    "print(X_2.shape, X_tst_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras의 text_to_word_sequence 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 3745) (19617, 3745)\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(tokenizer=text_to_word_sequence, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "X_3 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_3 = vec.transform(tst['text']).toarray()\n",
    "print(X_3.shape, X_tst_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 word_tokenize 사용, stopword 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.310245Z",
     "start_time": "2020-11-09T04:44:31.758835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 12254) (19617, 12254)\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3), min_df=50)\n",
    "X_4 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_4 = vec.transform(tst['text']).toarray()\n",
    "print(X_4.shape, X_tst_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.342347Z",
     "start_time": "2020-11-09T04:45:10.312078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_4[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 WordPunctTokenizer 사용, stopword 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(tokenizer=wordPunctTokenizer.tokenize, ngram_range=(1, 3), min_df=50)\n",
    "X_5 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_5 = vec.transform(tst['text']).toarray()\n",
    "print(X_5.shape, X_tst_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_5[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras의 text_to_word_sequence 사용, stopword 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(tokenizer=text_to_word_sequence, ngram_range=(1, 3), min_df=50)\n",
    "X_6 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_6 = vec.transform(tst['text']).toarray()\n",
    "print(X_6.shape, X_tst_6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_6[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T15:15:47.430701Z",
     "start_time": "2020-11-04T15:15:47.404265Z"
    }
   },
   "source": [
    "## cnn 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.370865Z",
     "start_time": "2020-11-09T04:45:10.344734Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(number):\n",
    "    inputs = Input(batch_shape=(None, number, 1))\n",
    "    x = Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(inputs)\n",
    "    x = Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(n_class, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.399912Z",
     "start_time": "2020-11-09T04:45:10.373016Z"
    }
   },
   "outputs": [],
   "source": [
    "y = trn.author.values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:30.682036Z",
     "start_time": "2020-11-09T04:45:10.401772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "  1/172 [..............................] - ETA: 0s - loss: 1.6063WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0172s vs `on_train_batch_end` time: 0.0288s). Check your callbacks.\n",
      "172/172 [==============================] - 9s 52ms/step - loss: 1.5582 - val_loss: 1.5428\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.5275 - val_loss: 1.5036\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4911 - val_loss: 1.4750\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4591 - val_loss: 1.4610\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4320 - val_loss: 1.4364\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4079 - val_loss: 1.4354\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3813 - val_loss: 1.3942\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3502 - val_loss: 1.3609\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3321 - val_loss: 1.3436\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3145 - val_loss: 1.3329\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3019 - val_loss: 1.3335\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2877 - val_loss: 1.3204\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2758 - val_loss: 1.3248\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2701 - val_loss: 1.3144\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2592 - val_loss: 1.3112\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2507 - val_loss: 1.2965\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2412 - val_loss: 1.2825\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2323 - val_loss: 1.3030\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.2313 - val_loss: 1.2756\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2178 - val_loss: 1.2859\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2167 - val_loss: 1.2714\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2108 - val_loss: 1.2718\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2029 - val_loss: 1.2714\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1992 - val_loss: 1.2620\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1921 - val_loss: 1.2874\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1871 - val_loss: 1.2539\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1838 - val_loss: 1.2622\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1821 - val_loss: 1.2540\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.1759Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1759 - val_loss: 1.2756\n",
      "Epoch 00029: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "  1/172 [..............................] - ETA: 0s - loss: 1.6096WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0170s vs `on_train_batch_end` time: 0.0273s). Check your callbacks.\n",
      "172/172 [==============================] - 9s 52ms/step - loss: 1.5608 - val_loss: 1.5469\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.5350 - val_loss: 1.5164\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.5003 - val_loss: 1.4816\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4692 - val_loss: 1.4552\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4371 - val_loss: 1.4316\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4029 - val_loss: 1.4009\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3718 - val_loss: 1.3885\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3512 - val_loss: 1.3637\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3359 - val_loss: 1.3481\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3232 - val_loss: 1.3471\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3104 - val_loss: 1.3392\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2998 - val_loss: 1.3452\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2888 - val_loss: 1.3355\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2840 - val_loss: 1.3170\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2764 - val_loss: 1.3241\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2681 - val_loss: 1.3051\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2590 - val_loss: 1.3068\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2508 - val_loss: 1.3062\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2489 - val_loss: 1.2923\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2395 - val_loss: 1.2944\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2312 - val_loss: 1.3043\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2282 - val_loss: 1.2879\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2220 - val_loss: 1.2880\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2173 - val_loss: 1.2851\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2133 - val_loss: 1.2865\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2088 - val_loss: 1.2766\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2019 - val_loss: 1.2772\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1964 - val_loss: 1.2774\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1942 - val_loss: 1.2678\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1893 - val_loss: 1.2896\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1853 - val_loss: 1.2696\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.1797Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1797 - val_loss: 1.2864\n",
      "Epoch 00032: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "  1/172 [..............................] - ETA: 0s - loss: 1.6097WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0166s vs `on_train_batch_end` time: 0.0274s). Check your callbacks.\n",
      "172/172 [==============================] - 9s 52ms/step - loss: 1.5608 - val_loss: 1.5443\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.5306 - val_loss: 1.5047\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4971 - val_loss: 1.4754\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4664 - val_loss: 1.4438\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4347 - val_loss: 1.4419\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4063 - val_loss: 1.3877\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3750 - val_loss: 1.3625\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3497 - val_loss: 1.3481\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3308 - val_loss: 1.3292\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3200 - val_loss: 1.3185\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3019 - val_loss: 1.3172\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2925 - val_loss: 1.3051\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2861 - val_loss: 1.3264\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2783 - val_loss: 1.3116\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2663 - val_loss: 1.2870\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2534 - val_loss: 1.3017\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2522 - val_loss: 1.2814\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2417 - val_loss: 1.2774\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2342 - val_loss: 1.2802\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2256 - val_loss: 1.2876\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2214 - val_loss: 1.2699\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.2134 - val_loss: 1.2716\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.2064 - val_loss: 1.2709\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2003 - val_loss: 1.2543\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1938 - val_loss: 1.2600\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1918 - val_loss: 1.2468\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.1851 - val_loss: 1.2543\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1818 - val_loss: 1.2640\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.1764Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.1764 - val_loss: 1.2546\n",
      "Epoch 00029: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "  1/172 [..............................] - ETA: 0s - loss: 1.6095WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0164s vs `on_train_batch_end` time: 0.0272s). Check your callbacks.\n",
      "172/172 [==============================] - 9s 51ms/step - loss: 1.5620 - val_loss: 1.5472\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.5306 - val_loss: 1.5210\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4946 - val_loss: 1.4911\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4673 - val_loss: 1.4576\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4349 - val_loss: 1.4467\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4033 - val_loss: 1.4086\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3747 - val_loss: 1.3872\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3492 - val_loss: 1.3791\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3317 - val_loss: 1.3501\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3216 - val_loss: 1.3531\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3061 - val_loss: 1.3354\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2992 - val_loss: 1.3414\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2850 - val_loss: 1.3309\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.2778 - val_loss: 1.3222\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2668 - val_loss: 1.3114\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2601 - val_loss: 1.3018\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2505 - val_loss: 1.3092\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2438 - val_loss: 1.2970\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2392 - val_loss: 1.2962\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2336 - val_loss: 1.2867\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2274 - val_loss: 1.2841\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.2193 - val_loss: 1.2963\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.2143 - val_loss: 1.2863\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.2070Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2070 - val_loss: 1.2869\n",
      "Epoch 00024: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "  1/172 [..............................] - ETA: 0s - loss: 1.6102WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0166s vs `on_train_batch_end` time: 0.0272s). Check your callbacks.\n",
      "172/172 [==============================] - 9s 52ms/step - loss: 1.5624 - val_loss: 1.5537\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.5331 - val_loss: 1.5279\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4977 - val_loss: 1.4912\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4635 - val_loss: 1.4569\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.4248 - val_loss: 1.4157\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3926 - val_loss: 1.3972\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3675 - val_loss: 1.3794\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3460 - val_loss: 1.3640\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3293 - val_loss: 1.3471\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.3157 - val_loss: 1.3490\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.3035 - val_loss: 1.3264\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2924 - val_loss: 1.3304\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 9s 49ms/step - loss: 1.2801 - val_loss: 1.3149\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2725 - val_loss: 1.3120\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2675 - val_loss: 1.3081\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2571 - val_loss: 1.2963\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2472 - val_loss: 1.3025\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2427 - val_loss: 1.2947\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2363 - val_loss: 1.2831\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2262 - val_loss: 1.2835\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2261 - val_loss: 1.2872\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.2178Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.2178 - val_loss: 1.2962\n",
      "Epoch 00022: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "  1/172 [..............................] - ETA: 0s - loss: 1.6101WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0164s vs `on_train_batch_end` time: 0.0267s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 9s 51ms/step - loss: 1.5622 - val_loss: 1.5459\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.5275 - val_loss: 1.5104\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4918 - val_loss: 1.4874\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4615 - val_loss: 1.4546\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4402 - val_loss: 1.4381\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4247 - val_loss: 1.4476\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4098 - val_loss: 1.4412\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3963 - val_loss: 1.4186\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3813 - val_loss: 1.3952\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3693 - val_loss: 1.3898\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3594 - val_loss: 1.3953\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3520 - val_loss: 1.3817\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3422 - val_loss: 1.3716\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3315 - val_loss: 1.3680\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3260 - val_loss: 1.3684\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3226 - val_loss: 1.3657\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3096 - val_loss: 1.3542\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2966 - val_loss: 1.3463\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2864 - val_loss: 1.3406\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2773 - val_loss: 1.3261\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2705 - val_loss: 1.3202\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2647 - val_loss: 1.3236\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2600 - val_loss: 1.3284\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2555 - val_loss: 1.3174\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2480 - val_loss: 1.3146\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2455 - val_loss: 1.3097\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2457 - val_loss: 1.3159\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2381 - val_loss: 1.3086\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2355 - val_loss: 1.3102\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2294 - val_loss: 1.3134\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2253 - val_loss: 1.3069\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2194 - val_loss: 1.3014\n",
      "Epoch 33/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2171 - val_loss: 1.3055\n",
      "Epoch 34/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2158 - val_loss: 1.2983\n",
      "Epoch 35/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2128 - val_loss: 1.3073\n",
      "Epoch 36/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2074 - val_loss: 1.3002\n",
      "Epoch 37/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.2052Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2053 - val_loss: 1.3002\n",
      "Epoch 00037: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "172/172 [==============================] - 9s 51ms/step - loss: 1.5585 - val_loss: 1.5370\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.5137 - val_loss: 1.4984\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4786 - val_loss: 1.4746\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4530 - val_loss: 1.4808\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4318 - val_loss: 1.4461\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4174 - val_loss: 1.4311\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3979 - val_loss: 1.4203\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3823 - val_loss: 1.3973\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3716 - val_loss: 1.3963\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3619 - val_loss: 1.3843\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3570 - val_loss: 1.3819\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3442 - val_loss: 1.3796\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3333 - val_loss: 1.3791\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3263 - val_loss: 1.3632\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3206 - val_loss: 1.3601\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3121 - val_loss: 1.3625\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3060 - val_loss: 1.3526\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3057 - val_loss: 1.3648\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2961 - val_loss: 1.3720\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2893 - val_loss: 1.3470\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2838 - val_loss: 1.3439\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2817 - val_loss: 1.3346\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2785 - val_loss: 1.3451\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2693 - val_loss: 1.3379\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.2660Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2660 - val_loss: 1.3499\n",
      "Epoch 00025: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "172/172 [==============================] - 9s 50ms/step - loss: 1.5600 - val_loss: 1.5408\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.5236 - val_loss: 1.5079\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4912 - val_loss: 1.4734\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4603 - val_loss: 1.4567\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4348 - val_loss: 1.4258\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4108 - val_loss: 1.4049\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3861 - val_loss: 1.3920\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3656 - val_loss: 1.3683\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3528 - val_loss: 1.3825\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3405 - val_loss: 1.3773\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3335 - val_loss: 1.3535\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3205 - val_loss: 1.3355\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3128 - val_loss: 1.3550\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3073 - val_loss: 1.3320\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3009 - val_loss: 1.3536\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2956 - val_loss: 1.3187\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2919 - val_loss: 1.3206\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2801 - val_loss: 1.3203\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2748 - val_loss: 1.3078\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2721 - val_loss: 1.3053\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2639 - val_loss: 1.3088\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2603 - val_loss: 1.2972\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2539 - val_loss: 1.2957\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2482 - val_loss: 1.2912\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2438 - val_loss: 1.2985\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2409 - val_loss: 1.2943\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.2369Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2369 - val_loss: 1.2978\n",
      "Epoch 00027: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "  1/172 [..............................] - ETA: 0s - loss: 1.6102WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0157s vs `on_train_batch_end` time: 0.0268s). Check your callbacks.\n",
      "172/172 [==============================] - 9s 51ms/step - loss: 1.5559 - val_loss: 1.5302\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.5150 - val_loss: 1.5037\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4829 - val_loss: 1.4655\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4556 - val_loss: 1.4526\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4332 - val_loss: 1.4268\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4135 - val_loss: 1.4232\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3878 - val_loss: 1.3970\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3694 - val_loss: 1.3778\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3521 - val_loss: 1.3664\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3375 - val_loss: 1.3574\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3270 - val_loss: 1.3407\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3165 - val_loss: 1.3386\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3064 - val_loss: 1.3284\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3032 - val_loss: 1.3275\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2946 - val_loss: 1.3274\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2842 - val_loss: 1.3236\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2769 - val_loss: 1.3200\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2749 - val_loss: 1.3106\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2683 - val_loss: 1.3135\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2590 - val_loss: 1.3111\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.2554Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2554 - val_loss: 1.3122\n",
      "Epoch 00021: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "  1/172 [..............................] - ETA: 0s - loss: 1.6095WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0162s vs `on_train_batch_end` time: 0.0266s). Check your callbacks.\n",
      "172/172 [==============================] - 9s 51ms/step - loss: 1.5539 - val_loss: 1.5405\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.5108 - val_loss: 1.4895\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4744 - val_loss: 1.4641\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.4534 - val_loss: 1.4466\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4300 - val_loss: 1.4328\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.4142 - val_loss: 1.4327\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3972 - val_loss: 1.4008\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3875 - val_loss: 1.3977\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3734 - val_loss: 1.3973\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3642 - val_loss: 1.3813\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3538 - val_loss: 1.3941\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3490 - val_loss: 1.3816\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3432 - val_loss: 1.3639\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3369 - val_loss: 1.3644\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3283 - val_loss: 1.3599\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3232 - val_loss: 1.3643\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3140 - val_loss: 1.3576\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.3100 - val_loss: 1.3552\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.3043 - val_loss: 1.3565\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2964 - val_loss: 1.3513\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2930 - val_loss: 1.3558\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2903 - val_loss: 1.3428\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2818 - val_loss: 1.3345\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2799 - val_loss: 1.3363\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2750 - val_loss: 1.3334\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2693 - val_loss: 1.3435\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2642 - val_loss: 1.3344\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2592 - val_loss: 1.3273\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2541 - val_loss: 1.3235\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 8s 49ms/step - loss: 1.2544 - val_loss: 1.3263\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2476 - val_loss: 1.3252\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2419 - val_loss: 1.3190\n",
      "Epoch 33/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2438 - val_loss: 1.3341\n",
      "Epoch 34/100\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2380 - val_loss: 1.3242\n",
      "Epoch 35/100\n",
      "172/172 [==============================] - ETA: 0s - loss: 1.2358Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 8s 48ms/step - loss: 1.2358 - val_loss: 1.3310\n",
      "Epoch 00035: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 1.5550 - val_loss: 1.5242\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.5104 - val_loss: 1.4897\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4846 - val_loss: 1.4727\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4674 - val_loss: 1.4610\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4554 - val_loss: 1.4536\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4460 - val_loss: 1.4471\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4357 - val_loss: 1.4433\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4292 - val_loss: 1.4291\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4189 - val_loss: 1.4345\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4102 - val_loss: 1.4154\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3942 - val_loss: 1.4061\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3880 - val_loss: 1.3993\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3765 - val_loss: 1.3949\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3706 - val_loss: 1.3958\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3662 - val_loss: 1.3905\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3637 - val_loss: 1.3897\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3594 - val_loss: 1.3814\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3529 - val_loss: 1.3810\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3507 - val_loss: 1.3735\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3466 - val_loss: 1.3707\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3425 - val_loss: 1.3876\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3363 - val_loss: 1.3767\n",
      "Epoch 23/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.3383Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3381 - val_loss: 1.3802\n",
      "Epoch 00023: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "  1/172 [..............................] - ETA: 0s - loss: 1.6097WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0115s vs `on_train_batch_end` time: 0.0174s). Check your callbacks.\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 1.5538 - val_loss: 1.5196\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.5046 - val_loss: 1.4894\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4772 - val_loss: 1.4733\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4616 - val_loss: 1.4599\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4485 - val_loss: 1.4586\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4369 - val_loss: 1.4427\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4276 - val_loss: 1.4541\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4180 - val_loss: 1.4262\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 6s 34ms/step - loss: 1.4092 - val_loss: 1.4234\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4007 - val_loss: 1.4327\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3906 - val_loss: 1.4072\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3788 - val_loss: 1.4062\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3689 - val_loss: 1.3945\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3618 - val_loss: 1.3926\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3531 - val_loss: 1.3883\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3481 - val_loss: 1.3750\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3420 - val_loss: 1.3728\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3378 - val_loss: 1.3709\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3347 - val_loss: 1.3677\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3300 - val_loss: 1.3810\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3249 - val_loss: 1.3732\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3228 - val_loss: 1.3663\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3150 - val_loss: 1.3590\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3141 - val_loss: 1.3711\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3100 - val_loss: 1.3560\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3067 - val_loss: 1.3599\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3015 - val_loss: 1.3710\n",
      "Epoch 28/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.2992Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.2995 - val_loss: 1.3590\n",
      "Epoch 00028: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "  1/172 [..............................] - ETA: 0s - loss: 1.6105WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0114s vs `on_train_batch_end` time: 0.0172s). Check your callbacks.\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 1.5569 - val_loss: 1.5254\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.5118 - val_loss: 1.4916\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4862 - val_loss: 1.4839\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4739 - val_loss: 1.4659\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4595 - val_loss: 1.4561\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4515 - val_loss: 1.4468\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4430 - val_loss: 1.4394\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4317 - val_loss: 1.4382\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4218 - val_loss: 1.4235\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4026 - val_loss: 1.3988\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3929 - val_loss: 1.3992\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3854 - val_loss: 1.3926\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3793 - val_loss: 1.3901\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3708 - val_loss: 1.4002\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3663 - val_loss: 1.3871\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3619 - val_loss: 1.3733\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3600 - val_loss: 1.3766\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3547 - val_loss: 1.3840\n",
      "Epoch 19/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.3505Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3507 - val_loss: 1.3781\n",
      "Epoch 00019: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 6s 35ms/step - loss: 1.5593 - val_loss: 1.5367\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.5127 - val_loss: 1.5029\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4841 - val_loss: 1.4873\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4685 - val_loss: 1.4718\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4555 - val_loss: 1.4670\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4485 - val_loss: 1.4688\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4380 - val_loss: 1.4497\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4304 - val_loss: 1.4465\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4221 - val_loss: 1.4380\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4170 - val_loss: 1.4382\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4083 - val_loss: 1.4394\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4031 - val_loss: 1.4293\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3947 - val_loss: 1.4309\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3926 - val_loss: 1.4177\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3866 - val_loss: 1.4291\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3823 - val_loss: 1.4098\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3743 - val_loss: 1.4155\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3724 - val_loss: 1.4087\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3652 - val_loss: 1.4106\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3621 - val_loss: 1.4059\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3585 - val_loss: 1.4037\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3525 - val_loss: 1.3935\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3433 - val_loss: 1.3964\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3399 - val_loss: 1.3929\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3310 - val_loss: 1.3854\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3267 - val_loss: 1.3777\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3235 - val_loss: 1.3748\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3166 - val_loss: 1.3738\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3125 - val_loss: 1.4014\n",
      "Epoch 30/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.3149Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3150 - val_loss: 1.3956\n",
      "Epoch 00030: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "  1/172 [..............................] - ETA: 0s - loss: 1.6104WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0113s vs `on_train_batch_end` time: 0.0173s). Check your callbacks.\n",
      "172/172 [==============================] - 6s 35ms/step - loss: 1.5554 - val_loss: 1.5269\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.5078 - val_loss: 1.4978\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4910 - val_loss: 1.4855\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4739 - val_loss: 1.4751\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4661 - val_loss: 1.4638\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4518 - val_loss: 1.4555\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 6s 34ms/step - loss: 1.4447 - val_loss: 1.4593\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4368 - val_loss: 1.4569\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4282 - val_loss: 1.4433\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4164 - val_loss: 1.4611\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.4077 - val_loss: 1.4210\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3883 - val_loss: 1.4176\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3852 - val_loss: 1.4081\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3721 - val_loss: 1.4195\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3680 - val_loss: 1.3901\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3601 - val_loss: 1.4018\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3554 - val_loss: 1.3913\n",
      "Epoch 18/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.3498Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 6s 33ms/step - loss: 1.3501 - val_loss: 1.4106\n",
      "Epoch 00018: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 7s - loss: 1.6095WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0320s vs `on_train_batch_end` time: 0.0568s). Check your callbacks.\n",
      "172/172 [==============================] - 17s 101ms/step - loss: 1.5654 - val_loss: 1.5562\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 17s 96ms/step - loss: 1.5465 - val_loss: 1.5326\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5238 - val_loss: 1.5058\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4977 - val_loss: 1.4825\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4752 - val_loss: 1.4673\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4550 - val_loss: 1.4539\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4378 - val_loss: 1.4413\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4213 - val_loss: 1.4374\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4071 - val_loss: 1.4284\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3952 - val_loss: 1.4116\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.3832 - val_loss: 1.3999\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3707 - val_loss: 1.3977\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3627 - val_loss: 1.3854\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3483 - val_loss: 1.3732\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3414 - val_loss: 1.3689\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3222 - val_loss: 1.3692\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3132 - val_loss: 1.3538\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.3033 - val_loss: 1.3377\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2926 - val_loss: 1.3466\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2868 - val_loss: 1.3396\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2845 - val_loss: 1.3278\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2767 - val_loss: 1.3213\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2699 - val_loss: 1.3326\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2630 - val_loss: 1.3359\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.2583 - val_loss: 1.3133\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2558 - val_loss: 1.3153\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2529 - val_loss: 1.3303\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2455 - val_loss: 1.3084\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2388 - val_loss: 1.3172\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2373 - val_loss: 1.3038\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2292 - val_loss: 1.3021\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2294 - val_loss: 1.3036\n",
      "Epoch 33/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2247 - val_loss: 1.3087\n",
      "Epoch 34/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.2222Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2219 - val_loss: 1.3030\n",
      "Epoch 00034: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 7s - loss: 1.6085WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0325s vs `on_train_batch_end` time: 0.0609s). Check your callbacks.\n",
      "172/172 [==============================] - 17s 100ms/step - loss: 1.5642 - val_loss: 1.5569\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 17s 96ms/step - loss: 1.5462 - val_loss: 1.5361\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5224 - val_loss: 1.5106\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.5046 - val_loss: 1.4985\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4889 - val_loss: 1.4865\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4684 - val_loss: 1.4616\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4524 - val_loss: 1.4679\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4344 - val_loss: 1.4450\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4213 - val_loss: 1.4280\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4076 - val_loss: 1.4453\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3958 - val_loss: 1.4127\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3815 - val_loss: 1.4246\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3730 - val_loss: 1.3939\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3618 - val_loss: 1.4119\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3515 - val_loss: 1.3862\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3427 - val_loss: 1.3710\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3362 - val_loss: 1.3701\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3225 - val_loss: 1.3754\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3185 - val_loss: 1.3667\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3112 - val_loss: 1.3697\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3033 - val_loss: 1.3532\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2967 - val_loss: 1.3682\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.2904 - val_loss: 1.3504\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2875 - val_loss: 1.3503\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2816 - val_loss: 1.3412\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2718 - val_loss: 1.3492\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.2697 - val_loss: 1.3398\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.2639 - val_loss: 1.3459\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2617 - val_loss: 1.3380\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2566 - val_loss: 1.3470\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.2505 - val_loss: 1.3451\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2516 - val_loss: 1.3286\n",
      "Epoch 33/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.2425 - val_loss: 1.3377\n",
      "Epoch 34/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2410 - val_loss: 1.3505\n",
      "Epoch 35/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.2350Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2346 - val_loss: 1.3326\n",
      "Epoch 00035: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 8s - loss: 1.6058WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0356s vs `on_train_batch_end` time: 0.0611s). Check your callbacks.\n",
      "172/172 [==============================] - 17s 100ms/step - loss: 1.5636 - val_loss: 1.5552\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 17s 96ms/step - loss: 1.5482 - val_loss: 1.5410\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5290 - val_loss: 1.5202\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.5066 - val_loss: 1.4983\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4848 - val_loss: 1.4769\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4674 - val_loss: 1.4673\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4519 - val_loss: 1.4571\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4341 - val_loss: 1.4480\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4207 - val_loss: 1.4317\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4083 - val_loss: 1.4222\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3953 - val_loss: 1.4153\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3835 - val_loss: 1.4059\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3723 - val_loss: 1.4064\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3598 - val_loss: 1.4039\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3557 - val_loss: 1.3827\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3396 - val_loss: 1.3833\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.3316 - val_loss: 1.3851\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3243 - val_loss: 1.3616\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3170 - val_loss: 1.3722\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3097 - val_loss: 1.3549\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3018 - val_loss: 1.3657\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2981 - val_loss: 1.3802\n",
      "Epoch 23/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.2879Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2877 - val_loss: 1.3631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00023: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 7s - loss: 1.6083WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0333s vs `on_train_batch_end` time: 0.0587s). Check your callbacks.\n",
      "172/172 [==============================] - 17s 100ms/step - loss: 1.5643 - val_loss: 1.5556\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 17s 96ms/step - loss: 1.5474 - val_loss: 1.5404\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5216 - val_loss: 1.5148\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5015 - val_loss: 1.5075\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4795 - val_loss: 1.4816\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4619 - val_loss: 1.4755\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4425 - val_loss: 1.4559\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4232 - val_loss: 1.4449\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4072 - val_loss: 1.4391\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.3892 - val_loss: 1.4070\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.3663 - val_loss: 1.3996\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3562 - val_loss: 1.3763\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3338 - val_loss: 1.3753\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3293 - val_loss: 1.3751\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3170 - val_loss: 1.3598\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.3129 - val_loss: 1.3628\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.2997 - val_loss: 1.3686\n",
      "Epoch 18/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.2917Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2916 - val_loss: 1.3593\n",
      "Epoch 00018: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 7s - loss: 1.6075WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0334s vs `on_train_batch_end` time: 0.0595s). Check your callbacks.\n",
      "172/172 [==============================] - 17s 101ms/step - loss: 1.5655 - val_loss: 1.5601\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5512 - val_loss: 1.5474\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5322 - val_loss: 1.5239\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5082 - val_loss: 1.5073\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4842 - val_loss: 1.4815\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4631 - val_loss: 1.4661\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4451 - val_loss: 1.4508\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4245 - val_loss: 1.4368\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4080 - val_loss: 1.4324\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3964 - val_loss: 1.4148\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3845 - val_loss: 1.4067\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3720 - val_loss: 1.3937\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3556 - val_loss: 1.4062\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3473 - val_loss: 1.3853\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3350 - val_loss: 1.3885\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3264 - val_loss: 1.3644\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3207 - val_loss: 1.3872\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3110 - val_loss: 1.3586\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2951 - val_loss: 1.3528\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2872 - val_loss: 1.3575\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.2798 - val_loss: 1.3796\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2723 - val_loss: 1.3425\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2707 - val_loss: 1.3415\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2581 - val_loss: 1.3348\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2574 - val_loss: 1.3456\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2458 - val_loss: 1.3288\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2437 - val_loss: 1.3251\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2365 - val_loss: 1.3281\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2329 - val_loss: 1.3321\n",
      "Epoch 30/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.2310Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.2307 - val_loss: 1.3263\n",
      "Epoch 00030: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 7s - loss: 1.6085WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0324s vs `on_train_batch_end` time: 0.0579s). Check your callbacks.\n",
      "172/172 [==============================] - 17s 101ms/step - loss: 1.5641 - val_loss: 1.5558\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 17s 96ms/step - loss: 1.5432 - val_loss: 1.5372\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5235 - val_loss: 1.5108\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4942 - val_loss: 1.4872\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4708 - val_loss: 1.4658\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4518 - val_loss: 1.4585\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4345 - val_loss: 1.4410\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4174 - val_loss: 1.4533\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4064 - val_loss: 1.4338\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3916 - val_loss: 1.4177\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3816 - val_loss: 1.4217\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3661 - val_loss: 1.3969\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.3529 - val_loss: 1.3906\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3442 - val_loss: 1.3883\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3355 - val_loss: 1.3798\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3247 - val_loss: 1.3733\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3157 - val_loss: 1.3654\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3072 - val_loss: 1.3717\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3039 - val_loss: 1.3536\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 17s 98ms/step - loss: 1.2945 - val_loss: 1.3610\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2913 - val_loss: 1.3483\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.2858 - val_loss: 1.3496\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2794 - val_loss: 1.3526\n",
      "Epoch 24/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.2734Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2735 - val_loss: 1.3531\n",
      "Epoch 00024: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 8s - loss: 1.6111WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0339s vs `on_train_batch_end` time: 0.0622s). Check your callbacks.\n",
      "172/172 [==============================] - 17s 100ms/step - loss: 1.5654 - val_loss: 1.5565\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5475 - val_loss: 1.5408\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.5251 - val_loss: 1.5157\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4944 - val_loss: 1.4935\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4663 - val_loss: 1.4716\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4473 - val_loss: 1.4605\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4294 - val_loss: 1.4405\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4166 - val_loss: 1.4630\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4062 - val_loss: 1.4364\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3939 - val_loss: 1.4144\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3812 - val_loss: 1.4119\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3690 - val_loss: 1.4047\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3595 - val_loss: 1.3912\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3518 - val_loss: 1.3861\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3392 - val_loss: 1.3802\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3252 - val_loss: 1.3669\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3185 - val_loss: 1.3665\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3081 - val_loss: 1.3581\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2951 - val_loss: 1.3496\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2891 - val_loss: 1.3631\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2820 - val_loss: 1.3432\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.2770 - val_loss: 1.3320\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2687 - val_loss: 1.3458\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2634 - val_loss: 1.3357\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2614 - val_loss: 1.3251\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2530 - val_loss: 1.3306\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2519 - val_loss: 1.3182\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2465 - val_loss: 1.3366\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2445 - val_loss: 1.3230\n",
      "Epoch 30/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.2379Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2378 - val_loss: 1.3392\n",
      "Epoch 00030: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 8s - loss: 1.6096WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0326s vs `on_train_batch_end` time: 0.0621s). Check your callbacks.\n",
      "172/172 [==============================] - 17s 100ms/step - loss: 1.5630 - val_loss: 1.5483\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5410 - val_loss: 1.5305\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.5147 - val_loss: 1.5000\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4860 - val_loss: 1.4732\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4633 - val_loss: 1.4551\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4448 - val_loss: 1.4519\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4282 - val_loss: 1.4353\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4129 - val_loss: 1.4198\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4013 - val_loss: 1.4102\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3874 - val_loss: 1.4141\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3779 - val_loss: 1.3878\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3658 - val_loss: 1.3811\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3530 - val_loss: 1.3780\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3437 - val_loss: 1.3748\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3335 - val_loss: 1.3636\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3237 - val_loss: 1.3655\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3157 - val_loss: 1.3604\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3094 - val_loss: 1.3511\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3035 - val_loss: 1.3447\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2944 - val_loss: 1.3468\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2872 - val_loss: 1.3360\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2797 - val_loss: 1.3470\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2782 - val_loss: 1.3316\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2700 - val_loss: 1.3433\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2681 - val_loss: 1.3400\n",
      "Epoch 26/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.2637Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2638 - val_loss: 1.3423\n",
      "Epoch 00026: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 7s - loss: 1.6074WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0339s vs `on_train_batch_end` time: 0.0597s). Check your callbacks.\n",
      "172/172 [==============================] - 17s 100ms/step - loss: 1.5645 - val_loss: 1.5536\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5452 - val_loss: 1.5315\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5200 - val_loss: 1.5015\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4901 - val_loss: 1.4904\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4698 - val_loss: 1.4666\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4542 - val_loss: 1.4504\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4375 - val_loss: 1.4362\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4220 - val_loss: 1.4373\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4118 - val_loss: 1.4149\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3987 - val_loss: 1.4307\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3874 - val_loss: 1.4156\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3756 - val_loss: 1.4121\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3656 - val_loss: 1.3937\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3563 - val_loss: 1.3964\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.3479 - val_loss: 1.3939\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3380 - val_loss: 1.3795\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3296 - val_loss: 1.3794\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3192 - val_loss: 1.3808\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3146 - val_loss: 1.3726\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3095 - val_loss: 1.3671\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3013 - val_loss: 1.3710\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2912 - val_loss: 1.3565\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2873 - val_loss: 1.3511\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.2821 - val_loss: 1.3469\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2760 - val_loss: 1.3489\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2741 - val_loss: 1.3469\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2658 - val_loss: 1.3420\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2600 - val_loss: 1.3446\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2568 - val_loss: 1.3485\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2555 - val_loss: 1.3378\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2529 - val_loss: 1.3593\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2474 - val_loss: 1.3353\n",
      "Epoch 33/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2387 - val_loss: 1.3395\n",
      "Epoch 34/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2353 - val_loss: 1.3351\n",
      "Epoch 35/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.2308Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.2306 - val_loss: 1.3736\n",
      "Epoch 00035: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 8s - loss: 1.6093WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0375s vs `on_train_batch_end` time: 0.0586s). Check your callbacks.\n",
      "172/172 [==============================] - 17s 101ms/step - loss: 1.5639 - val_loss: 1.5522\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.5395 - val_loss: 1.5280\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.5106 - val_loss: 1.5054\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4846 - val_loss: 1.4839\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4646 - val_loss: 1.4661\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4462 - val_loss: 1.4613\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4333 - val_loss: 1.4451\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.4188 - val_loss: 1.4454\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.4039 - val_loss: 1.4365\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3902 - val_loss: 1.4240\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3804 - val_loss: 1.4083\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.3680 - val_loss: 1.4090\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3543 - val_loss: 1.4110\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3480 - val_loss: 1.3857\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3373 - val_loss: 1.3865\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.3282 - val_loss: 1.3785\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3181 - val_loss: 1.3798\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 1.3147 - val_loss: 1.3804\n",
      "Epoch 19/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.3080Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 1.3084 - val_loss: 1.3809\n",
      "Epoch 00019: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 6s - loss: 1.6067WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0255s vs `on_train_batch_end` time: 0.0483s). Check your callbacks.\n",
      "172/172 [==============================] - 14s 82ms/step - loss: 1.5595 - val_loss: 1.5385\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 1.5233 - val_loss: 1.5114\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.5034 - val_loss: 1.4996\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4917 - val_loss: 1.4910\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4778 - val_loss: 1.4794\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4684 - val_loss: 1.4668\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4556 - val_loss: 1.4645\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4462 - val_loss: 1.4633\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4383 - val_loss: 1.4425\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4241 - val_loss: 1.4505\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4110 - val_loss: 1.4306\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4007 - val_loss: 1.4339\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3971 - val_loss: 1.4362\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3862 - val_loss: 1.4154\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3779 - val_loss: 1.4128\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3710 - val_loss: 1.4070\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3655 - val_loss: 1.4069\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3626 - val_loss: 1.4045\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3549 - val_loss: 1.4056\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3481 - val_loss: 1.3993\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3440 - val_loss: 1.3970\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3413 - val_loss: 1.3960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3356 - val_loss: 1.4015\n",
      "Epoch 24/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.3331Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3333 - val_loss: 1.3961\n",
      "Epoch 00024: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 6s - loss: 1.6074WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0283s vs `on_train_batch_end` time: 0.0513s). Check your callbacks.\n",
      "172/172 [==============================] - 14s 82ms/step - loss: 1.5614 - val_loss: 1.5488\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 1.5307 - val_loss: 1.5311\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.5109 - val_loss: 1.5088\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4972 - val_loss: 1.4983\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4863 - val_loss: 1.4886\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4738 - val_loss: 1.4849\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4677 - val_loss: 1.4752\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4570 - val_loss: 1.4733\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4474 - val_loss: 1.4743\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4384 - val_loss: 1.4700\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4293 - val_loss: 1.4594\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4240 - val_loss: 1.4527\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4150 - val_loss: 1.4477\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4093 - val_loss: 1.4496\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4019 - val_loss: 1.4481\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3925 - val_loss: 1.4348\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3848 - val_loss: 1.4310\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3789 - val_loss: 1.4399\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3727 - val_loss: 1.4365\n",
      "Epoch 20/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.3682Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3684 - val_loss: 1.4349\n",
      "Epoch 00020: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 6s - loss: 1.6084WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0265s vs `on_train_batch_end` time: 0.0513s). Check your callbacks.\n",
      "172/172 [==============================] - 14s 82ms/step - loss: 1.5629 - val_loss: 1.5471\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 1.5354 - val_loss: 1.5199\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.5111 - val_loss: 1.4981\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4985 - val_loss: 1.4902\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4854 - val_loss: 1.4816\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4778 - val_loss: 1.4748\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4646 - val_loss: 1.4698\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4549 - val_loss: 1.4718\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4484 - val_loss: 1.4519\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4408 - val_loss: 1.4537\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4317 - val_loss: 1.4490\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4250 - val_loss: 1.4408\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4163 - val_loss: 1.4380\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4092 - val_loss: 1.4457\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4013 - val_loss: 1.4307\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3958 - val_loss: 1.4403\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3846 - val_loss: 1.4162\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3799 - val_loss: 1.4156\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3761 - val_loss: 1.4129\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3718 - val_loss: 1.4162\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3639 - val_loss: 1.4068\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3625 - val_loss: 1.4132\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3574 - val_loss: 1.4034\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3530 - val_loss: 1.4029\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3468 - val_loss: 1.3995\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3463 - val_loss: 1.4051\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3431 - val_loss: 1.3964\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3355 - val_loss: 1.4062\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3312 - val_loss: 1.3961\n",
      "Epoch 30/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.3308Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3309 - val_loss: 1.3958\n",
      "Epoch 00030: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 6s - loss: 1.6087WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0263s vs `on_train_batch_end` time: 0.0512s). Check your callbacks.\n",
      "172/172 [==============================] - 14s 82ms/step - loss: 1.5612 - val_loss: 1.5424\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.5226 - val_loss: 1.5189\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 14s 78ms/step - loss: 1.5001 - val_loss: 1.4983\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4831 - val_loss: 1.4856\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4703 - val_loss: 1.4788\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4577 - val_loss: 1.4757\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4465 - val_loss: 1.4640\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4380 - val_loss: 1.4612\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4280 - val_loss: 1.4497\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4188 - val_loss: 1.4474\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4105 - val_loss: 1.4439\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4029 - val_loss: 1.4326\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3927 - val_loss: 1.4223\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3814 - val_loss: 1.4287\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3769 - val_loss: 1.4142\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3669 - val_loss: 1.4204\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3624 - val_loss: 1.4088\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3592 - val_loss: 1.4070\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3546 - val_loss: 1.3970\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3488 - val_loss: 1.3982\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3438 - val_loss: 1.3963\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3369 - val_loss: 1.3907\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3356 - val_loss: 1.3982\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3300 - val_loss: 1.3928\n",
      "Epoch 25/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.3278Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.3278 - val_loss: 1.3954\n",
      "Epoch 00025: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "  2/172 [..............................] - ETA: 6s - loss: 1.6081WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0289s vs `on_train_batch_end` time: 0.0511s). Check your callbacks.\n",
      "172/172 [==============================] - 14s 82ms/step - loss: 1.5621 - val_loss: 1.5490\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 13s 78ms/step - loss: 1.5295 - val_loss: 1.5289\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.5077 - val_loss: 1.5115\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4949 - val_loss: 1.5064\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4854 - val_loss: 1.4949\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4767 - val_loss: 1.4848\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4646 - val_loss: 1.4773\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4562 - val_loss: 1.4847\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4485 - val_loss: 1.4805\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4391 - val_loss: 1.4644\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4330 - val_loss: 1.4677\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4234 - val_loss: 1.4598\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4180 - val_loss: 1.4608\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4089 - val_loss: 1.4610\n",
      "Epoch 15/100\n",
      "171/172 [============================>.] - ETA: 0s - loss: 1.4073Restoring model weights from the end of the best epoch.\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 1.4071 - val_loss: 1.4595\n",
      "Epoch 00015: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "lr ver1 Accuracy (CV):  48.5632%\n",
      "lr ver1 Log Loss (CV):   1.2671\n",
      "lr ver2 Accuracy (CV):  46.4094%\n",
      "lr ver2 Log Loss (CV):   1.3107\n",
      "lr ver3 Accuracy (CV):  41.4093%\n",
      "lr ver3 Log Loss (CV):   1.3730\n",
      "lr ver4 Accuracy (CV):  44.9261%\n",
      "lr ver4 Log Loss (CV):   1.3341\n",
      "lr ver5 Accuracy (CV):  43.9239%\n",
      "lr ver5 Log Loss (CV):   1.3424\n",
      "lr ver6 Accuracy (CV):  39.2081%\n",
      "lr ver6 Log Loss (CV):   1.4150\n"
     ]
    }
   ],
   "source": [
    "p_val_ver1 = np.zeros((X_1.shape[0], n_class))\n",
    "p_tst_ver1 = np.zeros((X_tst_1.shape[0], n_class))\n",
    "p_val_ver2 = np.zeros((X_2.shape[0], n_class))\n",
    "p_tst_ver2 = np.zeros((X_tst_2.shape[0], n_class))\n",
    "p_val_ver3 = np.zeros((X_3.shape[0], n_class))\n",
    "p_tst_ver3 = np.zeros((X_tst_3.shape[0], n_class))\n",
    "p_val_ver4 = np.zeros((X_4.shape[0], n_class))\n",
    "p_tst_ver4 = np.zeros((X_tst_4.shape[0], n_class))\n",
    "p_val_ver5 = np.zeros((X_5.shape[0], n_class))\n",
    "p_tst_ver5 = np.zeros((X_tst_5.shape[0], n_class))\n",
    "p_val_ver6 = np.zeros((X_6.shape[0], n_class))\n",
    "p_tst_ver6 = np.zeros((X_tst_6.shape[0], n_class))\n",
    "\n",
    "for X, test in [(X_1, X_tst_1), (X_2, X_tst_2), (X_3, X_tst_3),\n",
    "               (X_4, X_tst_4), (X_5, X_tst_5), (X_6, X_tst_6)]:  \n",
    "    for i_cv, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n",
    "        print(f'Training model for CV #{i_cv}')\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
    "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
    "        \n",
    "        X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "        test = test.reshape(test.shape[0], test.shape[1],1)\n",
    "        \n",
    "        clf = get_model(X.shape[1])\n",
    "        clf.fit(X[i_trn], \n",
    "            to_categorical(y[i_trn]),\n",
    "            validation_data=(X[i_val], to_categorical(y[i_val])),\n",
    "            epochs=100,\n",
    "            batch_size=256,\n",
    "            callbacks=[es])\n",
    "       \n",
    "        # Predict\n",
    "        if X.shape[1]==5897:\n",
    "            p_val_ver1[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver1 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==5772:\n",
    "            p_val_ver2[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver2 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==3745:\n",
    "            p_val_ver3[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver3 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==12254:\n",
    "            p_val_ver4[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver4 += clf.predict(test) / n_class\n",
    "        elif X.shape[1]==12267:\n",
    "            p_val_ver5[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver5 += clf.predict(test) / n_class\n",
    "        else:\n",
    "            p_val_ver6[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver6 += clf.predict(test) / n_class\n",
    "            \n",
    "        del clf\n",
    "        clear_session()\n",
    "        gc.collect()\n",
    "            \n",
    "    print(\"Training has finished\")\n",
    "    print(\"*\"*100)\n",
    "\n",
    "            \n",
    "print(f'lr ver1 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver1, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver1 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver1):8.4f}')\n",
    "print(f'lr ver2 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver2, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver2 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver2):8.4f}')\n",
    "print(f'lr ver3 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver3, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver3 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver3):8.4f}')\n",
    "print(f'lr ver4 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver4, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver4 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver4):8.4f}')\n",
    "print(f'lr ver5 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver5, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver5 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver5):8.4f}')\n",
    "print(f'lr ver6 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver6, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver6 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver6):8.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제출 파일 생성 및 기타 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일 생성\n",
    "\n",
    "sub = pd.read_csv(sample_file, index_col=0)\n",
    "\n",
    "# Ver1\n",
    "sub[sub.columns] = p_tst_ver1\n",
    "sub.to_csv(sub_ver1_file)\n",
    "\n",
    "# Ver2\n",
    "sub[sub.columns] = p_tst_ver2\n",
    "sub.to_csv(sub_ver2_file)\n",
    "\n",
    "# Ver3\n",
    "sub[sub.columns] = p_tst_ver3\n",
    "sub.to_csv(sub_ver3_file)\n",
    "\n",
    "# Ver4\n",
    "sub[sub.columns] = p_tst_ver4\n",
    "sub.to_csv(sub_ver4_file)\n",
    "\n",
    "# Ver5\n",
    "sub[sub.columns] = p_tst_ver5\n",
    "sub.to_csv(sub_ver5_file)\n",
    "\n",
    "# Ver6\n",
    "sub[sub.columns] = p_tst_ver6\n",
    "sub.to_csv(sub_ver6_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:31.272596Z",
     "start_time": "2020-11-09T04:45:31.074976Z"
    }
   },
   "outputs": [],
   "source": [
    "# p_val 파일 생성 -> oof\n",
    "\n",
    "# Ver1\n",
    "np.savetxt(p_val_ver1_file, p_val_ver1, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(p_val_ver2_file, p_val_ver2, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(p_val_ver3_file, p_val_ver3, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(p_val_ver4_file, p_val_ver4, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver5\n",
    "np.savetxt(p_val_ver5_file, p_val_ver5, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver6\n",
    "np.savetxt(p_val_ver6_file, p_val_ver6, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_tst 파일 생성 -> test \n",
    "\n",
    "# Ver1\n",
    "np.savetxt(p_tst_ver1_file, p_tst_ver1, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(p_tst_ver2_file, p_tst_ver2, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(p_tst_ver3_file, p_tst_ver3, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(p_tst_ver4_file, p_tst_ver4, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver5\n",
    "np.savetxt(p_tst_ver5_file, p_tst_ver5, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver6\n",
    "np.savetxt(p_tst_ver6_file, p_tst_ver6, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
