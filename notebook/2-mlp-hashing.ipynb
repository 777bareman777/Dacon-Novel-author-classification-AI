{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 import 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:22.350713Z",
     "start_time": "2020-11-09T04:32:22.049823Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.188282Z",
     "start_time": "2020-11-09T04:32:22.352714Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.216883Z",
     "start_time": "2020-11-09T04:32:23.190555Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.250120Z",
     "start_time": "2020-11-09T04:32:23.219024Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path('../data/dacon-novel-author-classification')\n",
    "feature_dir = Path('../build/feature')\n",
    "val_dir = Path('../build/val')\n",
    "tst_dir = Path('../build/tst')\n",
    "sub_dir = Path('../build/sub')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.282083Z",
     "start_time": "2020-11-09T04:32:23.252439Z"
    }
   },
   "outputs": [],
   "source": [
    "algo_name = 'mlp'\n",
    "feature_name = 'hashing'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "feature_file = feature_dir / f'{feature_name}.csv'\n",
    "\n",
    "p_val_ver1_file = val_dir / f'{model_name}_oof_pred_ver1.csv'\n",
    "p_tst_ver1_file = tst_dir / f'{model_name}_test_pred_ver1.csv'\n",
    "\n",
    "p_val_ver2_file = val_dir / f'{model_name}_oof_pred_ver2.csv'\n",
    "p_tst_ver2_file = tst_dir / f'{model_name}_test_pred_ver2.csv'\n",
    "\n",
    "p_val_ver3_file = val_dir / f'{model_name}_oof_pred_ver3.csv'\n",
    "p_tst_ver3_file = tst_dir / f'{model_name}_test_pred_ver3.csv'\n",
    "\n",
    "p_val_ver4_file = val_dir / f'{model_name}_oof_pred_ver4.csv'\n",
    "p_tst_ver4_file = tst_dir / f'{model_name}_test_pred_ver4.csv'\n",
    "\n",
    "p_val_ver5_file = val_dir / f'{model_name}_oof_pred_ver5.csv'\n",
    "p_tst_ver5_file = tst_dir / f'{model_name}_test_pred_ver5.csv'\n",
    "\n",
    "p_val_ver6_file = val_dir / f'{model_name}_oof_pred_ver6.csv'\n",
    "p_tst_ver6_file = tst_dir / f'{model_name}_test_pred_ver6.csv'\n",
    "\n",
    "sub_ver1_file = sub_dir / f'{model_name}_ver1.csv'\n",
    "sub_ver2_file = sub_dir / f'{model_name}_ver2.csv'\n",
    "sub_ver3_file = sub_dir / f'{model_name}_ver3.csv'\n",
    "sub_ver4_file = sub_dir / f'{model_name}_ver4.csv'\n",
    "sub_ver5_file = sub_dir / f'{model_name}_ver5.csv'\n",
    "sub_ver6_file = sub_dir / f'{model_name}_ver6.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.456017Z",
     "start_time": "2020-11-09T04:32:23.283900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      He was almost choking. There was so much, so m...       3\n",
       "1                 “Your sister asked for it, I suppose?”       2\n",
       "2       She was engaged one day as she walked, in per...       1\n",
       "3      The captain was in the porch, keeping himself ...       4\n",
       "4      “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn = pd.read_csv(trn_file, index_col=0)\n",
    "print(trn.shape)\n",
    "trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.580127Z",
     "start_time": "2020-11-09T04:32:23.458379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19617, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Not at all. I think she is one of the most ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As the lady had stated her intention of scream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“And then suddenly in the silence I heard a so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>His conviction remained unchanged. So far as I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "index                                                   \n",
       "0      “Not at all. I think she is one of the most ch...\n",
       "1      \"No,\" replied he, with sudden consciousness, \"...\n",
       "2      As the lady had stated her intention of scream...\n",
       "3      “And then suddenly in the silence I heard a so...\n",
       "4      His conviction remained unchanged. So far as I..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = pd.read_csv(tst_file, index_col=0)\n",
    "print(tst.shape)\n",
    "tst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 토큰화 비교, 어간 추출과 표제어 추출 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.609289Z",
     "start_time": "2020-11-09T04:32:23.583484Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLTK에 있는 단어 토큰화\n",
    "# -> Don't를 Do 와 n't로 분리, Jone's를 Jone 과 '로 분리.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK에 있는 단어 토큰화 \n",
    "# -> Don't를 Don 과 ' 와 t 로 분리, Jone's를 Jone 과 ' 와 s로 분리.\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# 케라스에 있는 단어 토큰화\n",
    "# -> 모든 알파벳을 소문자로 바꾸고, 온점이나, 컴마, 느낌표 등의 구두점을 제거.\n",
    "# -> 하지만 don't 나 jone's와 같은 경우 아포스트로피를 보존함.\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "# NLTK에 있는 어간 추출(Stemming) 표제어 추출(Lemmatization)\n",
    "\n",
    "# -> WordNetLemmatizer는 기본형을 추출, 속도가 오래 걸리고 복잡함.\n",
    "# -> Pos(Part of Speech)에 대한 설정이 없으면, 제대로된 어간을 추출하지 못할 수 있음.\n",
    "# -> 제대로된 어간을 추출하고 싶다면, 단어의 쓰임새를 알아야 됨.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# -> Poter Stemmer은 대표적인 문법 기준을 뽑아서, 추출하는 방식, 즉 어간 추출함. 어간은 단어의 의미를 담고 있는 핵심 부분임.\n",
    "# -> 영어의 접미사(suffix)를 제거해서, 단어의 의미를 담고있는 어간만 추출함\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# -> LancasterStemmer은 Poter Stemmer와 비슷하지만, 알고리즘이 다름.\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# -> Porter Stemmer의 개선판, Porter Stemmer2라고 보면 됨.\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk의 word_tokenize를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.645042Z",
     "start_time": "2020-11-09T04:32:23.612849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_1 = trn.text[4]\n",
    "print(s_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'his', 'hands', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', '....', 'Oh', ',', 'my', 'God', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_1 = word_tokenize(s_1)\n",
    "print(tokenized_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentleman', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', '....', 'Oh', ',', 'my', 'God', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_1 = [lemmatizer.lemmatize(t) for t in tokenized_word_1]\n",
    "print(tokenized_lemmatizer_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'hi', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'I', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'Oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_1 = [porterStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_porter_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'hav', 'mercy', ',', 'gentlem', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'writ', 'that', ',', 'anyway', ';', 'hav', 'som', 'sham', '.', 'her', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'bef', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_1 = [lancasterStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_lancaster_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_1 = [snowballStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_snowball_word_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk의 WordPunctTokenizer를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_2 = trn.text[4]\n",
    "print(s_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'his', 'hands', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', '....', 'Oh', ',', 'my', 'God', '!”']\n"
     ]
    }
   ],
   "source": [
    "wordPunctTokenizer = WordPunctTokenizer()\n",
    "tokenized_word_2 = wordPunctTokenizer.tokenize(s_2)\n",
    "print(tokenized_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentleman', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', '....', 'Oh', ',', 'my', 'God', '!”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_2 = [lemmatizer.lemmatize(t) for t in tokenized_word_2]\n",
    "print(tokenized_lemmatizer_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'hi', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'I', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'Oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_2 = [porterStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_porter_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'hav', 'mercy', ',', 'gentlem', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'writ', 'that', ',', 'anyway', ';', 'hav', 'som', 'sham', '.', 'her', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'bef', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_2 = [lancasterStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_lancaster_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_2 = [snowballStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_snowball_word_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kerasd의 text_to_word_sequence를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_3 = trn.text[4]\n",
    "print(s_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentlemen', '”', 'odin', 'flung', 'up', 'his', 'hands', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’ve', 'torn', 'my', 'heart', 'asunder', 'before', 'you', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_3 = text_to_word_sequence(s_3)\n",
    "print(tokenized_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentleman', '”', 'odin', 'flung', 'up', 'his', 'hand', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’ve', 'torn', 'my', 'heart', 'asunder', 'before', 'you', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_3 = [lemmatizer.lemmatize(t) for t in tokenized_word_3]\n",
    "print(tokenized_lemmatizer_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'merci', 'gentlemen', '”', 'odin', 'flung', 'up', 'hi', 'hand', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’v', 'torn', 'my', 'heart', 'asund', 'befor', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_3 = [porterStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_porter_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentlem', '”', 'odin', 'flung', 'up', 'his', 'hand', '“don’t', 'writ', 'that', 'anyway', 'hav', 'som', 'sham', 'her', 'i’ve', 'torn', 'my', 'heart', 'asund', 'bef', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_3 = [lancasterStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_lancaster_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'merci', 'gentlemen', '”', 'odin', 'flung', 'up', 'his', 'hand', \"“don't\", 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', \"i'v\", 'torn', 'my', 'heart', 'asund', 'befor', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_3 = [snowballStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_snowball_word_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T14:54:47.082620Z",
     "start_time": "2020-11-04T14:54:47.055487Z"
    }
   },
   "source": [
    "## Hashing 피쳐 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 word_tokenize 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.310245Z",
     "start_time": "2020-11-09T04:44:31.758835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1024) (19617, 1024)\n"
     ]
    }
   ],
   "source": [
    "vec = HashingVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), n_features=2**10)\n",
    "X_1 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_1 = vec.transform(tst['text']).toarray()\n",
    "print(X_1.shape, X_tst_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.342347Z",
     "start_time": "2020-11-09T04:45:10.312078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.09950372,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.09950372,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.09950372,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 WordPunctTokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1024) (19617, 1024)\n"
     ]
    }
   ],
   "source": [
    "vec = HashingVectorizer(tokenizer=wordPunctTokenizer.tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), n_features=2**10)\n",
    "X_2 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_2 = vec.transform(tst['text']).toarray()\n",
    "print(X_2.shape, X_tst_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.09950372,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.09950372,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.09950372,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras의 text_to_word_sequence 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1024) (19617, 1024)\n"
     ]
    }
   ],
   "source": [
    "vec = HashingVectorizer(tokenizer=text_to_word_sequence, stop_words=stopwords.words('english'), ngram_range=(1, 3), n_features=2**10)\n",
    "X_3 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_3 = vec.transform(tst['text']).toarray()\n",
    "print(X_3.shape, X_tst_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.13245324,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.13245324,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.13245324,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 word_tokenize 사용, stopword 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.310245Z",
     "start_time": "2020-11-09T04:44:31.758835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1024) (19617, 1024)\n"
     ]
    }
   ],
   "source": [
    "vec = HashingVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3), n_features=2**10)\n",
    "X_4 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_4 = vec.transform(tst['text']).toarray()\n",
    "print(X_4.shape, X_tst_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.342347Z",
     "start_time": "2020-11-09T04:45:10.312078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.06917145,  0.        ,\n",
       "        0.        ,  0.06917145,  0.        , -0.06917145, -0.06917145,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.06917145,\n",
       "        0.        , -0.06917145,  0.        ,  0.        ,  0.06917145,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.06917145,  0.        ,  0.06917145,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_4[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 WordPunctTokenizer 사용, stopword 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1024) (19617, 1024)\n"
     ]
    }
   ],
   "source": [
    "vec = HashingVectorizer(tokenizer=wordPunctTokenizer.tokenize, ngram_range=(1, 3), n_features=2**10)\n",
    "X_5 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_5 = vec.transform(tst['text']).toarray()\n",
    "print(X_5.shape, X_tst_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.06917145,  0.        ,\n",
       "        0.        ,  0.06917145,  0.        , -0.06917145, -0.06917145,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.06917145,\n",
       "        0.        , -0.06917145,  0.        ,  0.        ,  0.06917145,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.06917145,  0.        ,  0.06917145,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_5[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras의 text_to_word_sequence 사용, stopword 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1024) (19617, 1024)\n"
     ]
    }
   ],
   "source": [
    "vec = HashingVectorizer(tokenizer=text_to_word_sequence, ngram_range=(1, 3), n_features=2**10)\n",
    "X_6 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_6 = vec.transform(tst['text']).toarray()\n",
    "print(X_6.shape, X_tst_6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.07881104,  0.        , -0.07881104,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.07881104,\n",
       "        0.        , -0.07881104,  0.        ,  0.        ,  0.07881104,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.07881104,  0.        ,  0.07881104,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_6[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T15:15:47.430701Z",
     "start_time": "2020-11-04T15:15:47.404265Z"
    }
   },
   "source": [
    "## mlp 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.370865Z",
     "start_time": "2020-11-09T04:45:10.344734Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(number):\n",
    "    inputs = Input(shape=(number,))\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    outputs = Dense(n_class, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.399912Z",
     "start_time": "2020-11-09T04:45:10.373016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = trn.author.values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:30.682036Z",
     "start_time": "2020-11-09T04:45:10.401772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4758 - val_loss: 1.3263\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2150 - val_loss: 1.1631\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1022 - val_loss: 1.1208\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0626 - val_loss: 1.1094\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0423 - val_loss: 1.1042\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0282 - val_loss: 1.1022\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0172 - val_loss: 1.0973\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0056 - val_loss: 1.0929\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9931 - val_loss: 1.0899\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9817 - val_loss: 1.0847\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9696 - val_loss: 1.0817\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9574 - val_loss: 1.0791\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9452 - val_loss: 1.0758\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9317 - val_loss: 1.0727\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9192 - val_loss: 1.0696\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9059 - val_loss: 1.0682\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8925 - val_loss: 1.0668\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8793 - val_loss: 1.0646\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8656 - val_loss: 1.0625\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8518 - val_loss: 1.0626\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8375 - val_loss: 1.0620\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8228 - val_loss: 1.0606\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8081 - val_loss: 1.0604\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7927 - val_loss: 1.0615\n",
      "Epoch 25/100\n",
      "70/86 [=======================>......] - ETA: 0s - loss: 0.7750Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7769 - val_loss: 1.0609\n",
      "Epoch 00025: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4832 - val_loss: 1.3390\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2223 - val_loss: 1.1735\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1039 - val_loss: 1.1262\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0632 - val_loss: 1.1147\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0447 - val_loss: 1.1126\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0319 - val_loss: 1.1086\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0217 - val_loss: 1.1062\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0118 - val_loss: 1.1016\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0017 - val_loss: 1.0977\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9906 - val_loss: 1.0943\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9790 - val_loss: 1.0903\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9675 - val_loss: 1.0861\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9550 - val_loss: 1.0827\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9425 - val_loss: 1.0789\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9299 - val_loss: 1.0761\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9166 - val_loss: 1.0725\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9033 - val_loss: 1.0704\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8894 - val_loss: 1.0688\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8755 - val_loss: 1.0648\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8611 - val_loss: 1.0641\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8465 - val_loss: 1.0617\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8314 - val_loss: 1.0619\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8161 - val_loss: 1.0600\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8000 - val_loss: 1.0586\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7844 - val_loss: 1.0592\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7683 - val_loss: 1.0581\n",
      "Epoch 27/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.7512Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7517 - val_loss: 1.0589\n",
      "Epoch 00027: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4760 - val_loss: 1.3271\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2237 - val_loss: 1.1565\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1100 - val_loss: 1.1082\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0696 - val_loss: 1.0936\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0502 - val_loss: 1.0904\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0379 - val_loss: 1.0857\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0275 - val_loss: 1.0820\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0178 - val_loss: 1.0789\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0075 - val_loss: 1.0754\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9974 - val_loss: 1.0718\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9864 - val_loss: 1.0687\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9755 - val_loss: 1.0639\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9636 - val_loss: 1.0607\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9517 - val_loss: 1.0583\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9392 - val_loss: 1.0565\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9269 - val_loss: 1.0525\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9138 - val_loss: 1.0489\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9004 - val_loss: 1.0465\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8867 - val_loss: 1.0442\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8726 - val_loss: 1.0416\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8580 - val_loss: 1.0415\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8433 - val_loss: 1.0380\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8281 - val_loss: 1.0364\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8124 - val_loss: 1.0361\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7960 - val_loss: 1.0367\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7802 - val_loss: 1.0346\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7642 - val_loss: 1.0352\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7464 - val_loss: 1.0356\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.7286Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7286 - val_loss: 1.0359\n",
      "Epoch 00029: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.4773 - val_loss: 1.3310\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2144 - val_loss: 1.1700\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1012 - val_loss: 1.1265\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0622 - val_loss: 1.1160\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0436 - val_loss: 1.1131\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0317 - val_loss: 1.1068\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0214 - val_loss: 1.1054\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0110 - val_loss: 1.1023\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0009 - val_loss: 1.0991\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9908 - val_loss: 1.0959\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9799 - val_loss: 1.0911\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9681 - val_loss: 1.0880\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9568 - val_loss: 1.0847\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9447 - val_loss: 1.0832\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9323 - val_loss: 1.0813\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9195 - val_loss: 1.0754\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9064 - val_loss: 1.0733\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8931 - val_loss: 1.0715\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8793 - val_loss: 1.0703\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8652 - val_loss: 1.0682\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8509 - val_loss: 1.0664\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8358 - val_loss: 1.0661\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8205 - val_loss: 1.0646\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8053 - val_loss: 1.0649\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7892 - val_loss: 1.0631\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7731 - val_loss: 1.0636\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7561 - val_loss: 1.0641\n",
      "Epoch 28/100\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.7392Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7395 - val_loss: 1.0632\n",
      "Epoch 00028: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4673 - val_loss: 1.3265\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2168 - val_loss: 1.1728\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1045 - val_loss: 1.1298\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0636 - val_loss: 1.1203\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0451 - val_loss: 1.1187\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0328 - val_loss: 1.1169\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0219 - val_loss: 1.1143\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0117 - val_loss: 1.1109\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0016 - val_loss: 1.1071\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9910 - val_loss: 1.1031\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9799 - val_loss: 1.0989\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9680 - val_loss: 1.0954\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9563 - val_loss: 1.0921\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9441 - val_loss: 1.0885\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9317 - val_loss: 1.0863\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9188 - val_loss: 1.0830\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9061 - val_loss: 1.0802\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8929 - val_loss: 1.0780\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8797 - val_loss: 1.0766\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8657 - val_loss: 1.0735\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8520 - val_loss: 1.0716\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8376 - val_loss: 1.0710\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8229 - val_loss: 1.0702\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8079 - val_loss: 1.0683\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7929 - val_loss: 1.0712\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7776 - val_loss: 1.0694\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.7617Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7617 - val_loss: 1.0697\n",
      "Epoch 00027: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4795 - val_loss: 1.3337\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2181 - val_loss: 1.1722\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1071 - val_loss: 1.1334\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0707 - val_loss: 1.1260\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0545 - val_loss: 1.1242\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0445 - val_loss: 1.1240\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0360 - val_loss: 1.1217\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0275 - val_loss: 1.1201\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0192 - val_loss: 1.1169\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0102 - val_loss: 1.1158\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0004 - val_loss: 1.1129\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9904 - val_loss: 1.1104\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9800 - val_loss: 1.1082\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9688 - val_loss: 1.1046\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9571 - val_loss: 1.1026\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9449 - val_loss: 1.1010\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9321 - val_loss: 1.0993\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9189 - val_loss: 1.0957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9049 - val_loss: 1.0926\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8904 - val_loss: 1.0912\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8752 - val_loss: 1.0896\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8593 - val_loss: 1.0880\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8432 - val_loss: 1.0860\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8265 - val_loss: 1.0868\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8097 - val_loss: 1.0874\n",
      "Epoch 26/100\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.7910Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7914 - val_loss: 1.0852\n",
      "Epoch 00026: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4938 - val_loss: 1.3482\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2284 - val_loss: 1.1733\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1100 - val_loss: 1.1312\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0709 - val_loss: 1.1204\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0535 - val_loss: 1.1173\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0415 - val_loss: 1.1155\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0317 - val_loss: 1.1129\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0225 - val_loss: 1.1115\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0124 - val_loss: 1.1081\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0025 - val_loss: 1.1051\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9919 - val_loss: 1.1023\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9814 - val_loss: 1.0993\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9706 - val_loss: 1.0959\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9587 - val_loss: 1.0940\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9471 - val_loss: 1.0914\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9341 - val_loss: 1.0889\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9215 - val_loss: 1.0866\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9087 - val_loss: 1.0857\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8952 - val_loss: 1.0826\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8810 - val_loss: 1.0814\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8670 - val_loss: 1.0804\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8522 - val_loss: 1.0798\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8370 - val_loss: 1.0780\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8213 - val_loss: 1.0786\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8052 - val_loss: 1.0779\n",
      "Epoch 26/100\n",
      "82/86 [===========================>..] - ETA: 0s - loss: 0.7879Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7885 - val_loss: 1.0785\n",
      "Epoch 00026: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 1.4847 - val_loss: 1.3348\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2283 - val_loss: 1.1543\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1152 - val_loss: 1.1082\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0778 - val_loss: 1.0969\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0602 - val_loss: 1.0910\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0493 - val_loss: 1.0891\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0392 - val_loss: 1.0862\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0304 - val_loss: 1.0829\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0203 - val_loss: 1.0806\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0103 - val_loss: 1.0765\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9997 - val_loss: 1.0727\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9888 - val_loss: 1.0694\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9767 - val_loss: 1.0674\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9644 - val_loss: 1.0630\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9520 - val_loss: 1.0605\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9385 - val_loss: 1.0573\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9249 - val_loss: 1.0568\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9110 - val_loss: 1.0520\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8968 - val_loss: 1.0509\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8817 - val_loss: 1.0480\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8662 - val_loss: 1.0465\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8508 - val_loss: 1.0455\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8348 - val_loss: 1.0441\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8183 - val_loss: 1.0440\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8013 - val_loss: 1.0435\n",
      "Epoch 26/100\n",
      "70/86 [=======================>......] - ETA: 0s - loss: 0.7820Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7837 - val_loss: 1.0435\n",
      "Epoch 00026: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4925 - val_loss: 1.3515\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2223 - val_loss: 1.1781\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1046 - val_loss: 1.1383\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0677 - val_loss: 1.1292\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0508 - val_loss: 1.1259\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0397 - val_loss: 1.1237\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0301 - val_loss: 1.1211\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0210 - val_loss: 1.1188\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0111 - val_loss: 1.1160\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0007 - val_loss: 1.1142\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9903 - val_loss: 1.1101\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9789 - val_loss: 1.1077\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9673 - val_loss: 1.1055\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9546 - val_loss: 1.1020\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9417 - val_loss: 1.0990\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9285 - val_loss: 1.0969\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9146 - val_loss: 1.0956\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9005 - val_loss: 1.0950\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8862 - val_loss: 1.0925\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8708 - val_loss: 1.0917\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8561 - val_loss: 1.0911\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8402 - val_loss: 1.0899\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8243 - val_loss: 1.0890\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8077 - val_loss: 1.0893\n",
      "Epoch 25/100\n",
      "74/86 [========================>.....] - ETA: 0s - loss: 0.7875Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7907 - val_loss: 1.0897\n",
      "Epoch 00025: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4829 - val_loss: 1.3410\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2181 - val_loss: 1.1711\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1035 - val_loss: 1.1343\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0661 - val_loss: 1.1264\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0481 - val_loss: 1.1238\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0360 - val_loss: 1.1223\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0258 - val_loss: 1.1196\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0153 - val_loss: 1.1171\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0047 - val_loss: 1.1149\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9940 - val_loss: 1.1124\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9827 - val_loss: 1.1094\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9709 - val_loss: 1.1075\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9584 - val_loss: 1.1055\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9457 - val_loss: 1.1012\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9319 - val_loss: 1.0991\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9180 - val_loss: 1.0971\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9039 - val_loss: 1.0955\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8891 - val_loss: 1.0931\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8736 - val_loss: 1.0901\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8576 - val_loss: 1.0891\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8416 - val_loss: 1.0900\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8249 - val_loss: 1.0876\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8079 - val_loss: 1.0871\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7908 - val_loss: 1.0858\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7728 - val_loss: 1.0872\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7541 - val_loss: 1.0875\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.7356Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7356 - val_loss: 1.0875\n",
      "Epoch 00027: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.5142 - val_loss: 1.4021\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.3047 - val_loss: 1.2587\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2084 - val_loss: 1.2283\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1780 - val_loss: 1.2249\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1636 - val_loss: 1.2244\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1537 - val_loss: 1.2243\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1446 - val_loss: 1.2230\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1351 - val_loss: 1.2230\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1253 - val_loss: 1.2216\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1146 - val_loss: 1.2192\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1035 - val_loss: 1.2192\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0918 - val_loss: 1.2166\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0793 - val_loss: 1.2164\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0663 - val_loss: 1.2143\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0526 - val_loss: 1.2139\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0378 - val_loss: 1.2137\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0228 - val_loss: 1.2112\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0071 - val_loss: 1.2101\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9902 - val_loss: 1.2104\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9723 - val_loss: 1.2104\n",
      "Epoch 21/100\n",
      "70/86 [=======================>......] - ETA: 0s - loss: 0.9531Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9543 - val_loss: 1.2105\n",
      "Epoch 00021: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 1.5096 - val_loss: 1.4034\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2972 - val_loss: 1.2706\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2025 - val_loss: 1.2473\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1732 - val_loss: 1.2444\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1603 - val_loss: 1.2453\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1507 - val_loss: 1.2466\n",
      "Epoch 7/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.1417Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1421 - val_loss: 1.2454\n",
      "Epoch 00007: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.5192 - val_loss: 1.4090\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.3115 - val_loss: 1.2557\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2113 - val_loss: 1.2218\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1799 - val_loss: 1.2153\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1654 - val_loss: 1.2144\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1552 - val_loss: 1.2141\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1462 - val_loss: 1.2132\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1370 - val_loss: 1.2123\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1278 - val_loss: 1.2105\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1177 - val_loss: 1.2100\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1074 - val_loss: 1.2081\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0959 - val_loss: 1.2068\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0840 - val_loss: 1.2056\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0711 - val_loss: 1.2047\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0577 - val_loss: 1.2035\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0436 - val_loss: 1.2022\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0283 - val_loss: 1.2037\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0127 - val_loss: 1.2011\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9959 - val_loss: 1.2009\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9785 - val_loss: 1.2006\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.9603Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9603 - val_loss: 1.2008\n",
      "Epoch 00021: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.5120 - val_loss: 1.4068\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.3001 - val_loss: 1.2688\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2030 - val_loss: 1.2429\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1734 - val_loss: 1.2396\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1599 - val_loss: 1.2380\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1499 - val_loss: 1.2377\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1411 - val_loss: 1.2368\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1320 - val_loss: 1.2356\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1223 - val_loss: 1.2338\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1127 - val_loss: 1.2317\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1024 - val_loss: 1.2302\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0910 - val_loss: 1.2285\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0787 - val_loss: 1.2271\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0666 - val_loss: 1.2256\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0536 - val_loss: 1.2242\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0393 - val_loss: 1.2231\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0249 - val_loss: 1.2216\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0097 - val_loss: 1.2221\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9938 - val_loss: 1.2203\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9772 - val_loss: 1.2209\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9601 - val_loss: 1.2211\n",
      "Epoch 22/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.9411Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9415 - val_loss: 1.2202\n",
      "Epoch 00022: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.5188 - val_loss: 1.4116\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.3069 - val_loss: 1.2707\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2072 - val_loss: 1.2430\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1751 - val_loss: 1.2383\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1603 - val_loss: 1.2395\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1499 - val_loss: 1.2388\n",
      "Epoch 7/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.1401Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1406 - val_loss: 1.2387\n",
      "Epoch 00007: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 1.4563 - val_loss: 1.2783\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1509 - val_loss: 1.0934\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0318 - val_loss: 1.0489\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9921 - val_loss: 1.0384\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9730 - val_loss: 1.0338\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9601 - val_loss: 1.0296\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9491 - val_loss: 1.0268\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9395 - val_loss: 1.0236\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9289 - val_loss: 1.0203\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9185 - val_loss: 1.0172\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9086 - val_loss: 1.0140\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8976 - val_loss: 1.0087\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8862 - val_loss: 1.0058\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8747 - val_loss: 1.0032\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8633 - val_loss: 0.9999\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8514 - val_loss: 0.9980\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8393 - val_loss: 0.9947\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8266 - val_loss: 0.9912\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8137 - val_loss: 0.9908\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8006 - val_loss: 0.9899\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7866 - val_loss: 0.9845\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7728 - val_loss: 0.9825\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7585 - val_loss: 0.9803\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7437 - val_loss: 0.9792\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7286 - val_loss: 0.9783\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7132 - val_loss: 0.9757\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6978 - val_loss: 0.9740\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6815 - val_loss: 0.9752\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6654 - val_loss: 0.9732\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6488 - val_loss: 0.9718\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6316 - val_loss: 0.9724\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6149 - val_loss: 0.9732\n",
      "Epoch 33/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.5968Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5974 - val_loss: 0.9713\n",
      "Epoch 00033: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.4493 - val_loss: 1.2751\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1469 - val_loss: 1.1013\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0324 - val_loss: 1.0593\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9939 - val_loss: 1.0498\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9754 - val_loss: 1.0448\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9638 - val_loss: 1.0432\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9530 - val_loss: 1.0404\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9426 - val_loss: 1.0350\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9332 - val_loss: 1.0315\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9229 - val_loss: 1.0278\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9123 - val_loss: 1.0236\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9017 - val_loss: 1.0199\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8906 - val_loss: 1.0158\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8791 - val_loss: 1.0134\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8672 - val_loss: 1.0098\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8557 - val_loss: 1.0070\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8436 - val_loss: 1.0034\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8309 - val_loss: 0.9998\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8180 - val_loss: 0.9973\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8051 - val_loss: 0.9956\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7914 - val_loss: 0.9927\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7782 - val_loss: 0.9907\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7639 - val_loss: 0.9905\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7500 - val_loss: 0.9879\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7355 - val_loss: 0.9870\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7206 - val_loss: 0.9841\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7052 - val_loss: 0.9837\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6896 - val_loss: 0.9836\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6743 - val_loss: 0.9822\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6578 - val_loss: 0.9829\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6418 - val_loss: 0.9843\n",
      "Epoch 32/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.6248Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6248 - val_loss: 0.9841\n",
      "Epoch 00032: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4484 - val_loss: 1.2664\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1509 - val_loss: 1.0858\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0367 - val_loss: 1.0406\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9974 - val_loss: 1.0294\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9796 - val_loss: 1.0272\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9673 - val_loss: 1.0244\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9579 - val_loss: 1.0222\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9484 - val_loss: 1.0188\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9392 - val_loss: 1.0148\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9292 - val_loss: 1.0114\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9186 - val_loss: 1.0074\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9082 - val_loss: 1.0037\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8972 - val_loss: 1.0000\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8858 - val_loss: 0.9967\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8740 - val_loss: 0.9939\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8619 - val_loss: 0.9904\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8501 - val_loss: 0.9879\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8375 - val_loss: 0.9854\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8245 - val_loss: 0.9832\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8115 - val_loss: 0.9800\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7980 - val_loss: 0.9778\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7841 - val_loss: 0.9753\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7702 - val_loss: 0.9739\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7554 - val_loss: 0.9726\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7405 - val_loss: 0.9710\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7255 - val_loss: 0.9698\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7096 - val_loss: 0.9699\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6938 - val_loss: 0.9695\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6777 - val_loss: 0.9683\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6612 - val_loss: 0.9673\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6441 - val_loss: 0.9664\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6272 - val_loss: 0.9666\n",
      "Epoch 33/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.6099Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6101 - val_loss: 0.9673\n",
      "Epoch 00033: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 1.4593 - val_loss: 1.2833\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1554 - val_loss: 1.1015\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0373 - val_loss: 1.0532\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9955 - val_loss: 1.0382\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9758 - val_loss: 1.0320\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9623 - val_loss: 1.0265\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9502 - val_loss: 1.0238\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9392 - val_loss: 1.0190\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9278 - val_loss: 1.0135\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9159 - val_loss: 1.0114\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9043 - val_loss: 1.0057\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8916 - val_loss: 1.0014\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8793 - val_loss: 0.9959\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8661 - val_loss: 0.9935\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8527 - val_loss: 0.9885\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8390 - val_loss: 0.9846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8255 - val_loss: 0.9834\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8117 - val_loss: 0.9789\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7977 - val_loss: 0.9772\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7839 - val_loss: 0.9754\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7696 - val_loss: 0.9743\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7549 - val_loss: 0.9708\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7405 - val_loss: 0.9696\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7254 - val_loss: 0.9694\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7107 - val_loss: 0.9690\n",
      "Epoch 26/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.6957Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6956 - val_loss: 0.9688\n",
      "Epoch 00026: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4498 - val_loss: 1.2690\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1495 - val_loss: 1.0895\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0361 - val_loss: 1.0463\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9968 - val_loss: 1.0338\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9773 - val_loss: 1.0309\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9651 - val_loss: 1.0275\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9541 - val_loss: 1.0247\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9436 - val_loss: 1.0210\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9325 - val_loss: 1.0169\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9212 - val_loss: 1.0119\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9099 - val_loss: 1.0080\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8982 - val_loss: 1.0054\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8856 - val_loss: 1.0000\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8738 - val_loss: 0.9968\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8609 - val_loss: 0.9932\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8482 - val_loss: 0.9898\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8350 - val_loss: 0.9864\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8215 - val_loss: 0.9847\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8080 - val_loss: 0.9815\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7940 - val_loss: 0.9783\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7797 - val_loss: 0.9760\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7651 - val_loss: 0.9732\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7503 - val_loss: 0.9721\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7351 - val_loss: 0.9700\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7188 - val_loss: 0.9696\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7029 - val_loss: 0.9690\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6869 - val_loss: 0.9663\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6703 - val_loss: 0.9652\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6535 - val_loss: 0.9641\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6364 - val_loss: 0.9635\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6190 - val_loss: 0.9643\n",
      "Epoch 32/100\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.6012Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6015 - val_loss: 0.9648\n",
      "Epoch 00032: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4545 - val_loss: 1.2823\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1613 - val_loss: 1.1032\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0460 - val_loss: 1.0628\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0068 - val_loss: 1.0531\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9896 - val_loss: 1.0496\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9779 - val_loss: 1.0465\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9683 - val_loss: 1.0459\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9595 - val_loss: 1.0424\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9508 - val_loss: 1.0396\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9418 - val_loss: 1.0372\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9321 - val_loss: 1.0367\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9225 - val_loss: 1.0329\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9118 - val_loss: 1.0292\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9008 - val_loss: 1.0271\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8895 - val_loss: 1.0247\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8778 - val_loss: 1.0230\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8652 - val_loss: 1.0197\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8524 - val_loss: 1.0167\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8385 - val_loss: 1.0157\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8251 - val_loss: 1.0109\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8101 - val_loss: 1.0087\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7949 - val_loss: 1.0065\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7796 - val_loss: 1.0038\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7636 - val_loss: 1.0033\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7472 - val_loss: 1.0004\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7308 - val_loss: 0.9977\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7136 - val_loss: 0.9983\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6959 - val_loss: 0.9965\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6780 - val_loss: 0.9935\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6600 - val_loss: 0.9942\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6412 - val_loss: 0.9941\n",
      "Epoch 32/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.6224Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6230 - val_loss: 0.9933\n",
      "Epoch 00032: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4656 - val_loss: 1.2953\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1625 - val_loss: 1.1116\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0432 - val_loss: 1.0695\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0045 - val_loss: 1.0615\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9861 - val_loss: 1.0583\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9740 - val_loss: 1.0552\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9639 - val_loss: 1.0534\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9545 - val_loss: 1.0509\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9440 - val_loss: 1.0481\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9340 - val_loss: 1.0447\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9228 - val_loss: 1.0414\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9119 - val_loss: 1.0377\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9000 - val_loss: 1.0354\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8881 - val_loss: 1.0323\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8754 - val_loss: 1.0285\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8621 - val_loss: 1.0258\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8491 - val_loss: 1.0230\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8353 - val_loss: 1.0211\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8213 - val_loss: 1.0195\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8069 - val_loss: 1.0157\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7919 - val_loss: 1.0147\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7769 - val_loss: 1.0137\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7614 - val_loss: 1.0109\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7452 - val_loss: 1.0091\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7290 - val_loss: 1.0082\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7127 - val_loss: 1.0086\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6958 - val_loss: 1.0053\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6786 - val_loss: 1.0054\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6606 - val_loss: 1.0056\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6435 - val_loss: 1.0041\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6255 - val_loss: 1.0043\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6071 - val_loss: 1.0056\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.5893Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.5893 - val_loss: 1.0062\n",
      "Epoch 00033: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.4611 - val_loss: 1.2804\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1629 - val_loss: 1.0925\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0469 - val_loss: 1.0491\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0087 - val_loss: 1.0369\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9912 - val_loss: 1.0335\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9795 - val_loss: 1.0319\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9695 - val_loss: 1.0276\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9599 - val_loss: 1.0248\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9507 - val_loss: 1.0215\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9405 - val_loss: 1.0200\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9300 - val_loss: 1.0158\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9193 - val_loss: 1.0130\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9071 - val_loss: 1.0090\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8952 - val_loss: 1.0062\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8831 - val_loss: 1.0022\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8699 - val_loss: 0.9992\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8568 - val_loss: 0.9975\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8428 - val_loss: 0.9948\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8284 - val_loss: 0.9926\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8139 - val_loss: 0.9905\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7994 - val_loss: 0.9874\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7830 - val_loss: 0.9850\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7672 - val_loss: 0.9851\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7509 - val_loss: 0.9828\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7339 - val_loss: 0.9802\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7167 - val_loss: 0.9798\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6995 - val_loss: 0.9781\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6819 - val_loss: 0.9798\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6638 - val_loss: 0.9770\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6456 - val_loss: 0.9778\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6275 - val_loss: 0.9772\n",
      "Epoch 32/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 0.6080Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6090 - val_loss: 0.9767\n",
      "Epoch 00032: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.4614 - val_loss: 1.2949\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1655 - val_loss: 1.1151\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0474 - val_loss: 1.0723\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0087 - val_loss: 1.0605\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9911 - val_loss: 1.0590\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9799 - val_loss: 1.0561\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9709 - val_loss: 1.0530\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9620 - val_loss: 1.0511\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9535 - val_loss: 1.0486\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9452 - val_loss: 1.0460\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9350 - val_loss: 1.0452\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9248 - val_loss: 1.0406\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9140 - val_loss: 1.0372\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9032 - val_loss: 1.0365\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8918 - val_loss: 1.0326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8800 - val_loss: 1.0294\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8674 - val_loss: 1.0272\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8541 - val_loss: 1.0252\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8408 - val_loss: 1.0231\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8269 - val_loss: 1.0199\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8125 - val_loss: 1.0174\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7968 - val_loss: 1.0138\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7810 - val_loss: 1.0125\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7649 - val_loss: 1.0097\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7483 - val_loss: 1.0076\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7317 - val_loss: 1.0082\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7141 - val_loss: 1.0044\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6959 - val_loss: 1.0042\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6779 - val_loss: 1.0028\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6596 - val_loss: 1.0017\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6408 - val_loss: 1.0012\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6218 - val_loss: 1.0022\n",
      "Epoch 33/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.6037Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6036 - val_loss: 1.0032\n",
      "Epoch 00033: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 1.4578 - val_loss: 1.2828\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1564 - val_loss: 1.1042\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0423 - val_loss: 1.0633\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0054 - val_loss: 1.0550\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9883 - val_loss: 1.0518\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9773 - val_loss: 1.0505\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9679 - val_loss: 1.0478\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9592 - val_loss: 1.0476\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9505 - val_loss: 1.0433\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9411 - val_loss: 1.0410\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9317 - val_loss: 1.0378\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9216 - val_loss: 1.0362\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9107 - val_loss: 1.0329\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9000 - val_loss: 1.0321\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8886 - val_loss: 1.0276\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8767 - val_loss: 1.0244\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8641 - val_loss: 1.0232\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8515 - val_loss: 1.0197\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8384 - val_loss: 1.0164\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8240 - val_loss: 1.0146\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8094 - val_loss: 1.0119\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7946 - val_loss: 1.0083\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7791 - val_loss: 1.0068\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7631 - val_loss: 1.0045\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7468 - val_loss: 1.0044\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7301 - val_loss: 1.0005\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7126 - val_loss: 0.9994\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6952 - val_loss: 0.9990\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6779 - val_loss: 0.9971\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6595 - val_loss: 0.9966\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6412 - val_loss: 0.9973\n",
      "Epoch 32/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.6227Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6227 - val_loss: 0.9972\n",
      "Epoch 00032: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.4938 - val_loss: 1.3581\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2486 - val_loss: 1.1965\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1461 - val_loss: 1.1669\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1159 - val_loss: 1.1618\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1015 - val_loss: 1.1610\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0922 - val_loss: 1.1606\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0846 - val_loss: 1.1610\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0764 - val_loss: 1.1593\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0685 - val_loss: 1.1571\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0596 - val_loss: 1.1559\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0510 - val_loss: 1.1548\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0412 - val_loss: 1.1533\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0312 - val_loss: 1.1520\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0206 - val_loss: 1.1498\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0093 - val_loss: 1.1484\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9975 - val_loss: 1.1476\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9852 - val_loss: 1.1457\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9719 - val_loss: 1.1456\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9579 - val_loss: 1.1417\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9430 - val_loss: 1.1423\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9285 - val_loss: 1.1404\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9125 - val_loss: 1.1383\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8958 - val_loss: 1.1382\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8787 - val_loss: 1.1360\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8616 - val_loss: 1.1376\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8435 - val_loss: 1.1360\n",
      "Epoch 27/100\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.8238Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8242 - val_loss: 1.1363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00027: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4908 - val_loss: 1.3592\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2432 - val_loss: 1.2168\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1445 - val_loss: 1.1876\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1118 - val_loss: 1.1818\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0965 - val_loss: 1.1811\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0865 - val_loss: 1.1822\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0774 - val_loss: 1.1790\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0682 - val_loss: 1.1776\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0585 - val_loss: 1.1750\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0489 - val_loss: 1.1742\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0384 - val_loss: 1.1694\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0271 - val_loss: 1.1676\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0157 - val_loss: 1.1651\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0036 - val_loss: 1.1633\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9914 - val_loss: 1.1611\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9788 - val_loss: 1.1606\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9654 - val_loss: 1.1576\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9518 - val_loss: 1.1570\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9377 - val_loss: 1.1545\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9232 - val_loss: 1.1537\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9077 - val_loss: 1.1536\n",
      "Epoch 22/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 0.8914Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8916 - val_loss: 1.1536\n",
      "Epoch 00022: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 1.4972 - val_loss: 1.3608\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2507 - val_loss: 1.1983\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1477 - val_loss: 1.1653\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1161 - val_loss: 1.1623\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1017 - val_loss: 1.1616\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0917 - val_loss: 1.1599\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0830 - val_loss: 1.1582\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0746 - val_loss: 1.1561\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0660 - val_loss: 1.1549\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0565 - val_loss: 1.1524\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0464 - val_loss: 1.1499\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0361 - val_loss: 1.1475\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0248 - val_loss: 1.1442\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0128 - val_loss: 1.1430\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0014 - val_loss: 1.1407\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9878 - val_loss: 1.1397\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9745 - val_loss: 1.1368\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9607 - val_loss: 1.1344\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9463 - val_loss: 1.1330\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9314 - val_loss: 1.1320\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9154 - val_loss: 1.1305\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8992 - val_loss: 1.1297\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8821 - val_loss: 1.1293\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8651 - val_loss: 1.1269\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8475 - val_loss: 1.1267\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8283 - val_loss: 1.1265\n",
      "Epoch 27/100\n",
      "84/86 [============================>.] - ETA: 0s - loss: 0.8097Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8092 - val_loss: 1.1261\n",
      "Epoch 00027: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 1.4979 - val_loss: 1.3659\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2486 - val_loss: 1.2095\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1455 - val_loss: 1.1806\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1132 - val_loss: 1.1747\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0982 - val_loss: 1.1724\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0875 - val_loss: 1.1716\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0784 - val_loss: 1.1711\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0697 - val_loss: 1.1688\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0607 - val_loss: 1.1665\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0515 - val_loss: 1.1648\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0415 - val_loss: 1.1630\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0309 - val_loss: 1.1606\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0199 - val_loss: 1.1589\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0089 - val_loss: 1.1579\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9972 - val_loss: 1.1565\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9843 - val_loss: 1.1546\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9717 - val_loss: 1.1526\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9587 - val_loss: 1.1508\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9445 - val_loss: 1.1507\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9302 - val_loss: 1.1494\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9150 - val_loss: 1.1492\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8997 - val_loss: 1.1478\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8836 - val_loss: 1.1467\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8665 - val_loss: 1.1480\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8491 - val_loss: 1.1477\n",
      "Epoch 26/100\n",
      "71/86 [=======================>......] - ETA: 0s - loss: 0.8299Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8310 - val_loss: 1.1466\n",
      "Epoch 00026: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 1.5002 - val_loss: 1.3688\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 4ms/step - loss: 1.2558 - val_loss: 1.2007\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1513 - val_loss: 1.1641\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1185 - val_loss: 1.1564\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.1030 - val_loss: 1.1570\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0929 - val_loss: 1.1556\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0839 - val_loss: 1.1542\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0754 - val_loss: 1.1522\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0664 - val_loss: 1.1512\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0574 - val_loss: 1.1485\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0475 - val_loss: 1.1469\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0370 - val_loss: 1.1439\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0262 - val_loss: 1.1412\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0150 - val_loss: 1.1387\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 1.0029 - val_loss: 1.1370\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9909 - val_loss: 1.1340\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9779 - val_loss: 1.1329\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9643 - val_loss: 1.1310\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9506 - val_loss: 1.1290\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9364 - val_loss: 1.1273\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9218 - val_loss: 1.1276\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.9063 - val_loss: 1.1271\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8899 - val_loss: 1.1249\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8735 - val_loss: 1.1266\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8563 - val_loss: 1.1249\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8382 - val_loss: 1.1238\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8194 - val_loss: 1.1249\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.8002 - val_loss: 1.1248\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 0.7808Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7808 - val_loss: 1.1261\n",
      "Epoch 00029: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "lr ver1 Accuracy (CV):  59.7023%\n",
      "lr ver1 Log Loss (CV):   1.0570\n",
      "lr ver2 Accuracy (CV):  58.3210%\n",
      "lr ver2 Log Loss (CV):   1.0767\n",
      "lr ver3 Accuracy (CV):  51.5370%\n",
      "lr ver3 Log Loss (CV):   1.2228\n",
      "lr ver4 Accuracy (CV):  63.4195%\n",
      "lr ver4 Log Loss (CV):   0.9710\n",
      "lr ver5 Accuracy (CV):  62.6214%\n",
      "lr ver5 Log Loss (CV):   0.9947\n",
      "lr ver6 Accuracy (CV):  55.5294%\n",
      "lr ver6 Log Loss (CV):   1.1376\n"
     ]
    }
   ],
   "source": [
    "p_val_ver1 = np.zeros((X_1.shape[0], n_class))\n",
    "p_tst_ver1 = np.zeros((X_tst_1.shape[0], n_class))\n",
    "p_val_ver2 = np.zeros((X_2.shape[0], n_class))\n",
    "p_tst_ver2 = np.zeros((X_tst_2.shape[0], n_class))\n",
    "p_val_ver3 = np.zeros((X_3.shape[0], n_class))\n",
    "p_tst_ver3 = np.zeros((X_tst_3.shape[0], n_class))\n",
    "p_val_ver4 = np.zeros((X_4.shape[0], n_class))\n",
    "p_tst_ver4 = np.zeros((X_tst_4.shape[0], n_class))\n",
    "p_val_ver5 = np.zeros((X_5.shape[0], n_class))\n",
    "p_tst_ver5 = np.zeros((X_tst_5.shape[0], n_class))\n",
    "p_val_ver6 = np.zeros((X_6.shape[0], n_class))\n",
    "p_tst_ver6 = np.zeros((X_tst_6.shape[0], n_class))\n",
    "\n",
    "for number, (X, test) in enumerate([(X_1, X_tst_1), (X_2, X_tst_2), (X_3, X_tst_3),\n",
    "               (X_4, X_tst_4), (X_5, X_tst_5), (X_6, X_tst_6)],1):\n",
    "    for i_cv, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n",
    "        print(f'Training model for CV #{i_cv}')\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
    "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
    "        \n",
    "        clf = get_model(X.shape[1])\n",
    "        clf.fit(X[i_trn], \n",
    "            to_categorical(y[i_trn]),\n",
    "            validation_data=(X[i_val], to_categorical(y[i_val])),\n",
    "            epochs=100,\n",
    "            batch_size=512,\n",
    "            callbacks=[es])\n",
    "       \n",
    "        # Predict\n",
    "        if number==1:\n",
    "            p_val_ver1[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver1 += clf.predict(test) / n_class\n",
    "        elif number==2:\n",
    "            p_val_ver2[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver2 += clf.predict(test) / n_class\n",
    "        elif number==3:\n",
    "            p_val_ver3[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver3 += clf.predict(test) / n_class\n",
    "        elif number==4:\n",
    "            p_val_ver4[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver4 += clf.predict(test) / n_class\n",
    "        elif number==5:\n",
    "            p_val_ver5[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver5 += clf.predict(test) / n_class\n",
    "        else:\n",
    "            p_val_ver6[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver6 += clf.predict(test) / n_class\n",
    "            \n",
    "        del clf\n",
    "        clear_session()\n",
    "        gc.collect()\n",
    "            \n",
    "    print(\"Training has finished\")\n",
    "    print(\"*\"*100)\n",
    "\n",
    "            \n",
    "print(f'lr ver1 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver1, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver1 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver1):8.4f}')\n",
    "print(f'lr ver2 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver2, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver2 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver2):8.4f}')\n",
    "print(f'lr ver3 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver3, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver3 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver3):8.4f}')\n",
    "print(f'lr ver4 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver4, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver4 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver4):8.4f}')\n",
    "print(f'lr ver5 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver5, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver5 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver5):8.4f}')\n",
    "print(f'lr ver6 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver6, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver6 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver6):8.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제출 파일 생성 및 기타 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일 생성\n",
    "\n",
    "sub = pd.read_csv(sample_file, index_col=0)\n",
    "\n",
    "# Ver1\n",
    "sub[sub.columns] = p_tst_ver1\n",
    "sub.to_csv(sub_ver1_file)\n",
    "\n",
    "# Ver2\n",
    "sub[sub.columns] = p_tst_ver2\n",
    "sub.to_csv(sub_ver2_file)\n",
    "\n",
    "# Ver3\n",
    "sub[sub.columns] = p_tst_ver3\n",
    "sub.to_csv(sub_ver3_file)\n",
    "\n",
    "# Ver4\n",
    "sub[sub.columns] = p_tst_ver4\n",
    "sub.to_csv(sub_ver4_file)\n",
    "\n",
    "# Ver5\n",
    "sub[sub.columns] = p_tst_ver5\n",
    "sub.to_csv(sub_ver5_file)\n",
    "\n",
    "# Ver6\n",
    "sub[sub.columns] = p_tst_ver6\n",
    "sub.to_csv(sub_ver6_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:31.272596Z",
     "start_time": "2020-11-09T04:45:31.074976Z"
    }
   },
   "outputs": [],
   "source": [
    "# p_val 파일 생성 -> oof\n",
    "\n",
    "# Ver1\n",
    "np.savetxt(p_val_ver1_file, p_val_ver1, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(p_val_ver2_file, p_val_ver2, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(p_val_ver3_file, p_val_ver3, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(p_val_ver4_file, p_val_ver4, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver5\n",
    "np.savetxt(p_val_ver5_file, p_val_ver5, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver6\n",
    "np.savetxt(p_val_ver6_file, p_val_ver6, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_tst 파일 생성 -> test \n",
    "\n",
    "# Ver1\n",
    "np.savetxt(p_tst_ver1_file, p_tst_ver1, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(p_tst_ver2_file, p_tst_ver2, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(p_tst_ver3_file, p_tst_ver3, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(p_tst_ver4_file, p_tst_ver4, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver5\n",
    "np.savetxt(p_tst_ver5_file, p_tst_ver5, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver6\n",
    "np.savetxt(p_tst_ver6_file, p_tst_ver6, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
