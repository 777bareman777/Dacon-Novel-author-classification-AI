{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 import 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:22.350713Z",
     "start_time": "2020-11-09T04:32:22.049823Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.188282Z",
     "start_time": "2020-11-09T04:32:22.352714Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.216883Z",
     "start_time": "2020-11-09T04:32:23.190555Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.250120Z",
     "start_time": "2020-11-09T04:32:23.219024Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = Path('../data/dacon-novel-author-classification')\n",
    "feature_dir = Path('../build/feature')\n",
    "val_dir = Path('../build/val')\n",
    "tst_dir = Path('../build/tst')\n",
    "sub_dir = Path('../build/sub')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.282083Z",
     "start_time": "2020-11-09T04:32:23.252439Z"
    }
   },
   "outputs": [],
   "source": [
    "algo_name = 'cnn'\n",
    "feature_name = 'hashing'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "feature_file = feature_dir / f'{feature_name}.csv'\n",
    "\n",
    "p_val_ver1_file = val_dir / f'{model_name}_oof_pred_ver1.csv'\n",
    "p_tst_ver1_file = tst_dir / f'{model_name}_test_pred_ver1.csv'\n",
    "\n",
    "p_val_ver2_file = val_dir / f'{model_name}_oof_pred_ver2.csv'\n",
    "p_tst_ver2_file = tst_dir / f'{model_name}_test_pred_ver2.csv'\n",
    "\n",
    "p_val_ver3_file = val_dir / f'{model_name}_oof_pred_ver3.csv'\n",
    "p_tst_ver3_file = tst_dir / f'{model_name}_test_pred_ver3.csv'\n",
    "\n",
    "p_val_ver4_file = val_dir / f'{model_name}_oof_pred_ver4.csv'\n",
    "p_tst_ver4_file = tst_dir / f'{model_name}_test_pred_ver4.csv'\n",
    "\n",
    "p_val_ver5_file = val_dir / f'{model_name}_oof_pred_ver5.csv'\n",
    "p_tst_ver5_file = tst_dir / f'{model_name}_test_pred_ver5.csv'\n",
    "\n",
    "p_val_ver6_file = val_dir / f'{model_name}_oof_pred_ver6.csv'\n",
    "p_tst_ver6_file = tst_dir / f'{model_name}_test_pred_ver6.csv'\n",
    "\n",
    "sub_ver1_file = sub_dir / f'{model_name}_ver1.csv'\n",
    "sub_ver2_file = sub_dir / f'{model_name}_ver2.csv'\n",
    "sub_ver3_file = sub_dir / f'{model_name}_ver3.csv'\n",
    "sub_ver4_file = sub_dir / f'{model_name}_ver4.csv'\n",
    "sub_ver5_file = sub_dir / f'{model_name}_ver5.csv'\n",
    "sub_ver6_file = sub_dir / f'{model_name}_ver6.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.456017Z",
     "start_time": "2020-11-09T04:32:23.283900Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      He was almost choking. There was so much, so m...       3\n",
       "1                 “Your sister asked for it, I suppose?”       2\n",
       "2       She was engaged one day as she walked, in per...       1\n",
       "3      The captain was in the porch, keeping himself ...       4\n",
       "4      “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn = pd.read_csv(trn_file, index_col=0)\n",
    "print(trn.shape)\n",
    "trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.580127Z",
     "start_time": "2020-11-09T04:32:23.458379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19617, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Not at all. I think she is one of the most ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As the lady had stated her intention of scream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“And then suddenly in the silence I heard a so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>His conviction remained unchanged. So far as I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "index                                                   \n",
       "0      “Not at all. I think she is one of the most ch...\n",
       "1      \"No,\" replied he, with sudden consciousness, \"...\n",
       "2      As the lady had stated her intention of scream...\n",
       "3      “And then suddenly in the silence I heard a so...\n",
       "4      His conviction remained unchanged. So far as I..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = pd.read_csv(tst_file, index_col=0)\n",
    "print(tst.shape)\n",
    "tst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 토큰화 비교, 어간 추출과 표제어 추출 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.609289Z",
     "start_time": "2020-11-09T04:32:23.583484Z"
    }
   },
   "outputs": [],
   "source": [
    "# NLTK에 있는 단어 토큰화\n",
    "# -> Don't를 Do 와 n't로 분리, Jone's를 Jone 과 '로 분리.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK에 있는 단어 토큰화 \n",
    "# -> Don't를 Don 과 ' 와 t 로 분리, Jone's를 Jone 과 ' 와 s로 분리.\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# 케라스에 있는 단어 토큰화\n",
    "# -> 모든 알파벳을 소문자로 바꾸고, 온점이나, 컴마, 느낌표 등의 구두점을 제거.\n",
    "# -> 하지만 don't 나 jone's와 같은 경우 아포스트로피를 보존함.\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "# NLTK에 있는 어간 추출(Stemming) 표제어 추출(Lemmatization)\n",
    "\n",
    "# -> WordNetLemmatizer는 기본형을 추출, 속도가 오래 걸리고 복잡함.\n",
    "# -> Pos(Part of Speech)에 대한 설정이 없으면, 제대로된 어간을 추출하지 못할 수 있음.\n",
    "# -> 제대로된 어간을 추출하고 싶다면, 단어의 쓰임새를 알아야 됨.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# -> Poter Stemmer은 대표적인 문법 기준을 뽑아서, 추출하는 방식, 즉 어간 추출함. 어간은 단어의 의미를 담고 있는 핵심 부분임.\n",
    "# -> 영어의 접미사(suffix)를 제거해서, 단어의 의미를 담고있는 어간만 추출함\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# -> LancasterStemmer은 Poter Stemmer와 비슷하지만, 알고리즘이 다름.\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "# -> Porter Stemmer의 개선판, Porter Stemmer2라고 보면 됨.\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk의 word_tokenize를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.645042Z",
     "start_time": "2020-11-09T04:32:23.612849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_1 = trn.text[4]\n",
    "print(s_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'his', 'hands', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', '....', 'Oh', ',', 'my', 'God', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_1 = word_tokenize(s_1)\n",
    "print(tokenized_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentleman', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', '....', 'Oh', ',', 'my', 'God', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_1 = [lemmatizer.lemmatize(t) for t in tokenized_word_1]\n",
    "print(tokenized_lemmatizer_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'hi', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'I', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'Oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_1 = [porterStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_porter_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'hav', 'mercy', ',', 'gentlem', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'writ', 'that', ',', 'anyway', ';', 'hav', 'som', 'sham', '.', 'her', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'bef', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_1 = [lancasterStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_lancaster_word_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!', '”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!', '”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_1 = [snowballStemmer.stem(t) for t in tokenized_word_1]\n",
    "print(tokenized_snowball_word_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk의 WordPunctTokenizer를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_2 = trn.text[4]\n",
    "print(s_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'his', 'hands', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', '....', 'Oh', ',', 'my', 'God', '!”']\n"
     ]
    }
   ],
   "source": [
    "wordPunctTokenizer = WordPunctTokenizer()\n",
    "tokenized_word_2 = wordPunctTokenizer.tokenize(s_2)\n",
    "print(tokenized_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Have', 'mercy', ',', 'gentleman', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'Don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'Here', 'I', '’', 've', 'torn', 'my', 'heart', 'asunder', 'before', 'you', ',', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', '....', 'Oh', ',', 'my', 'God', '!”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_2 = [lemmatizer.lemmatize(t) for t in tokenized_word_2]\n",
    "print(tokenized_lemmatizer_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'hi', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'I', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'Oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_2 = [porterStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_porter_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'hav', 'mercy', ',', 'gentlem', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'writ', 'that', ',', 'anyway', ';', 'hav', 'som', 'sham', '.', 'her', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'bef', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_2 = [lancasterStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_lancaster_word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'have', 'merci', ',', 'gentlemen', '!”', 'odin', 'flung', 'up', 'his', 'hand', '.', '“', 'don', '’', 't', 'write', 'that', ',', 'anyway', ';', 'have', 'some', 'shame', '.', 'here', 'i', '’', 've', 'torn', 'my', 'heart', 'asund', 'befor', 'you', ',', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', '....', 'oh', ',', 'my', 'god', '!”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_2 = [snowballStemmer.stem(t) for t in tokenized_word_2]\n",
    "print(tokenized_snowball_word_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kerasd의 text_to_word_sequence를 통해서 토큰화를 진행 후, 어간 추출 및 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Have mercy, gentlemen!” odin flung up his hands. “Don’t write that, anyway; have some shame. Here I’ve torn my heart asunder before you, and you seize the opportunity and are fingering the wounds in both halves.... Oh, my God!”\n"
     ]
    }
   ],
   "source": [
    "s_3 = trn.text[4]\n",
    "print(s_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:23.682095Z",
     "start_time": "2020-11-09T04:32:23.646719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentlemen', '”', 'odin', 'flung', 'up', 'his', 'hands', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’ve', 'torn', 'my', 'heart', 'asunder', 'before', 'you', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wounds', 'in', 'both', 'halves', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_3 = text_to_word_sequence(s_3)\n",
    "print(tokenized_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.650811Z",
     "start_time": "2020-11-09T04:32:23.684004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentleman', '”', 'odin', 'flung', 'up', 'his', 'hand', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’ve', 'torn', 'my', 'heart', 'asunder', 'before', 'you', 'and', 'you', 'seize', 'the', 'opportunity', 'and', 'are', 'fingering', 'the', 'wound', 'in', 'both', 'half', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenized_lemmatizer_word_3 = [lemmatizer.lemmatize(t) for t in tokenized_word_3]\n",
    "print(tokenized_lemmatizer_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'merci', 'gentlemen', '”', 'odin', 'flung', 'up', 'hi', 'hand', '“don’t', 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', 'i’v', 'torn', 'my', 'heart', 'asund', 'befor', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "porterStemmer = PorterStemmer()\n",
    "tokenized_porter_word_3 = [porterStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_porter_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'mercy', 'gentlem', '”', 'odin', 'flung', 'up', 'his', 'hand', '“don’t', 'writ', 'that', 'anyway', 'hav', 'som', 'sham', 'her', 'i’ve', 'torn', 'my', 'heart', 'asund', 'bef', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'ar', 'fing', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "lancasterStemmer = LancasterStemmer()\n",
    "tokenized_lancaster_word_3 = [lancasterStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_lancaster_word_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:25.688544Z",
     "start_time": "2020-11-09T04:32:25.652709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“have', 'merci', 'gentlemen', '”', 'odin', 'flung', 'up', 'his', 'hand', \"“don't\", 'write', 'that', 'anyway', 'have', 'some', 'shame', 'here', \"i'v\", 'torn', 'my', 'heart', 'asund', 'befor', 'you', 'and', 'you', 'seiz', 'the', 'opportun', 'and', 'are', 'finger', 'the', 'wound', 'in', 'both', 'halv', 'oh', 'my', 'god', '”']\n"
     ]
    }
   ],
   "source": [
    "snowballStemmer = SnowballStemmer(\"english\")\n",
    "tokenized_snowball_word_3 = [snowballStemmer.stem(t) for t in tokenized_word_3]\n",
    "print(tokenized_snowball_word_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T14:54:47.082620Z",
     "start_time": "2020-11-04T14:54:47.055487Z"
    }
   },
   "source": [
    "## Hashing 피쳐 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 word_tokenize 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.310245Z",
     "start_time": "2020-11-09T04:44:31.758835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1024) (19617, 1024)\n"
     ]
    }
   ],
   "source": [
    "vec = HashingVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), n_features=2**10)\n",
    "X_1 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_1 = vec.transform(tst['text']).toarray()\n",
    "print(X_1.shape, X_tst_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.342347Z",
     "start_time": "2020-11-09T04:45:10.312078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.09950372,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.09950372,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.09950372,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 WordPunctTokenizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1024) (19617, 1024)\n"
     ]
    }
   ],
   "source": [
    "vec = HashingVectorizer(tokenizer=wordPunctTokenizer.tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), n_features=2**10)\n",
    "X_2 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_2 = vec.transform(tst['text']).toarray()\n",
    "print(X_2.shape, X_tst_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.09950372,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.09950372,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.09950372,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras의 text_to_word_sequence 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1024) (19617, 1024)\n"
     ]
    }
   ],
   "source": [
    "vec = HashingVectorizer(tokenizer=text_to_word_sequence, stop_words=stopwords.words('english'), ngram_range=(1, 3), n_features=2**10)\n",
    "X_3 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_3 = vec.transform(tst['text']).toarray()\n",
    "print(X_3.shape, X_tst_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.13245324,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.13245324,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.13245324,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 word_tokenize 사용, stopword 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.310245Z",
     "start_time": "2020-11-09T04:44:31.758835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1024) (19617, 1024)\n"
     ]
    }
   ],
   "source": [
    "vec = HashingVectorizer(tokenizer=word_tokenize, ngram_range=(1, 3), n_features=2**10)\n",
    "X_4 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_4 = vec.transform(tst['text']).toarray()\n",
    "print(X_4.shape, X_tst_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.342347Z",
     "start_time": "2020-11-09T04:45:10.312078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.06917145,  0.        ,\n",
       "        0.        ,  0.06917145,  0.        , -0.06917145, -0.06917145,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.06917145,\n",
       "        0.        , -0.06917145,  0.        ,  0.        ,  0.06917145,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.06917145,  0.        ,  0.06917145,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_4[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltk의 WordPunctTokenizer 사용, stopword 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1024) (19617, 1024)\n"
     ]
    }
   ],
   "source": [
    "vec = HashingVectorizer(tokenizer=wordPunctTokenizer.tokenize, ngram_range=(1, 3), n_features=2**10)\n",
    "X_5 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_5 = vec.transform(tst['text']).toarray()\n",
    "print(X_5.shape, X_tst_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.06917145,  0.        ,\n",
       "        0.        ,  0.06917145,  0.        , -0.06917145, -0.06917145,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.06917145,\n",
       "        0.        , -0.06917145,  0.        ,  0.        ,  0.06917145,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.06917145,  0.        ,  0.06917145,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_5[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- keras의 text_to_word_sequence 사용, stopword 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 1024) (19617, 1024)\n"
     ]
    }
   ],
   "source": [
    "vec = HashingVectorizer(tokenizer=text_to_word_sequence, ngram_range=(1, 3), n_features=2**10)\n",
    "X_6 = vec.fit_transform(trn['text']).toarray()\n",
    "X_tst_6 = vec.transform(tst['text']).toarray()\n",
    "print(X_6.shape, X_tst_6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.07881104,  0.        , -0.07881104,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -0.07881104,\n",
       "        0.        , -0.07881104,  0.        ,  0.        ,  0.07881104,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.07881104,  0.        ,  0.07881104,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_6[0, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T15:15:47.430701Z",
     "start_time": "2020-11-04T15:15:47.404265Z"
    }
   },
   "source": [
    "## cnn 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.370865Z",
     "start_time": "2020-11-09T04:45:10.344734Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(number):\n",
    "    inputs = Input(batch_shape=(None, number, 1))\n",
    "    x = Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(inputs)\n",
    "    x = Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(n_class, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:10.399912Z",
     "start_time": "2020-11-09T04:45:10.373016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = trn.author.values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:30.682036Z",
     "start_time": "2020-11-09T04:45:10.401772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5395 - val_loss: 1.4839\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.4548 - val_loss: 1.4471\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.4177 - val_loss: 1.4107\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.3939 - val_loss: 1.3912\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.3756 - val_loss: 1.3815\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.3605 - val_loss: 1.3754\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.3511 - val_loss: 1.3595\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.3360 - val_loss: 1.3498\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.3279 - val_loss: 1.3434\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 18ms/step - loss: 1.3193 - val_loss: 1.3393\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3114 - val_loss: 1.3412\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3068 - val_loss: 1.3300\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3012 - val_loss: 1.3334\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2949 - val_loss: 1.3454\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2909 - val_loss: 1.3220\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2826 - val_loss: 1.3229\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2786 - val_loss: 1.3324\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2730 - val_loss: 1.3180\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2665 - val_loss: 1.3233\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2677 - val_loss: 1.3235\n",
      "Epoch 21/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.2607Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2602 - val_loss: 1.3172\n",
      "Epoch 00021: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5395 - val_loss: 1.4822\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4523 - val_loss: 1.4336\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4126 - val_loss: 1.4018\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3867 - val_loss: 1.3790\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3688 - val_loss: 1.3718\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3487 - val_loss: 1.3560\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3382 - val_loss: 1.3491\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3268 - val_loss: 1.3404\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3180 - val_loss: 1.3475\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3101 - val_loss: 1.3284\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3056 - val_loss: 1.3263\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2944 - val_loss: 1.3219\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2907 - val_loss: 1.3331\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2872 - val_loss: 1.3220\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2816 - val_loss: 1.3136\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2761 - val_loss: 1.3128\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2745 - val_loss: 1.3137\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2672 - val_loss: 1.3099\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2620 - val_loss: 1.3124\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2627 - val_loss: 1.3060\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2558 - val_loss: 1.3061\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2514 - val_loss: 1.3062\n",
      "Epoch 23/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.2471Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2472 - val_loss: 1.3058\n",
      "Epoch 00023: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5382 - val_loss: 1.4815\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4520 - val_loss: 1.4226\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4098 - val_loss: 1.4028\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3872 - val_loss: 1.3782\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3693 - val_loss: 1.3619\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3592 - val_loss: 1.3572\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3455 - val_loss: 1.3420\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3348 - val_loss: 1.3368\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3262 - val_loss: 1.3301\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3202 - val_loss: 1.3225\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3092 - val_loss: 1.3217\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3038 - val_loss: 1.3167\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2975 - val_loss: 1.3170\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2906 - val_loss: 1.3119\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2850 - val_loss: 1.3163\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2833 - val_loss: 1.3242\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2746 - val_loss: 1.3045\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2732 - val_loss: 1.3101\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2659 - val_loss: 1.3006\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2621 - val_loss: 1.3161\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2592 - val_loss: 1.3010\n",
      "Epoch 22/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.2540Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2547 - val_loss: 1.3117\n",
      "Epoch 00022: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5385 - val_loss: 1.4860\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4589 - val_loss: 1.4393\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4212 - val_loss: 1.4102\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3953 - val_loss: 1.4003\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3770 - val_loss: 1.3772\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3630 - val_loss: 1.3694\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3461 - val_loss: 1.3591\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3371 - val_loss: 1.3660\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3297 - val_loss: 1.3500\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3216 - val_loss: 1.3530\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3096 - val_loss: 1.3420\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3042 - val_loss: 1.3430\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2957 - val_loss: 1.3348\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2883 - val_loss: 1.3432\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2867 - val_loss: 1.3302\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2782 - val_loss: 1.3307\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2727 - val_loss: 1.3248\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2674 - val_loss: 1.3331\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2647 - val_loss: 1.3268\n",
      "Epoch 20/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.2608Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2614 - val_loss: 1.3270\n",
      "Epoch 00020: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5436 - val_loss: 1.4981\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4654 - val_loss: 1.4498\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4310 - val_loss: 1.4341\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4007 - val_loss: 1.3961\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3794 - val_loss: 1.3828\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3681 - val_loss: 1.3728\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3553 - val_loss: 1.3631\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3458 - val_loss: 1.3631\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3375 - val_loss: 1.3604\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3292 - val_loss: 1.3492\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3204 - val_loss: 1.3479\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3139 - val_loss: 1.3399\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3052 - val_loss: 1.3439\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3032 - val_loss: 1.3348\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2922 - val_loss: 1.3320\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2854 - val_loss: 1.3492\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2880 - val_loss: 1.3212\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2741 - val_loss: 1.3175\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2707 - val_loss: 1.3148\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2678 - val_loss: 1.3170\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2589 - val_loss: 1.3231\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.2543Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2543 - val_loss: 1.3176\n",
      "Epoch 00022: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5513 - val_loss: 1.5121\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4941 - val_loss: 1.4815\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4601 - val_loss: 1.4549\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4412 - val_loss: 1.4449\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4268 - val_loss: 1.4283\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4129 - val_loss: 1.4240\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4022 - val_loss: 1.4090\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3931 - val_loss: 1.4038\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3824 - val_loss: 1.3993\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3754 - val_loss: 1.4094\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3693 - val_loss: 1.3958\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3635 - val_loss: 1.3889\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3559 - val_loss: 1.3914\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3527 - val_loss: 1.3934\n",
      "Epoch 15/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.3490Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3487 - val_loss: 1.3898\n",
      "Epoch 00015: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5428 - val_loss: 1.4933\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4717 - val_loss: 1.4719\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4530 - val_loss: 1.4491\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4347 - val_loss: 1.4366\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4225 - val_loss: 1.4204\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4094 - val_loss: 1.4142\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4011 - val_loss: 1.4051\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3892 - val_loss: 1.4025\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3813 - val_loss: 1.4001\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3742 - val_loss: 1.3904\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3691 - val_loss: 1.3889\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3609 - val_loss: 1.3876\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3535 - val_loss: 1.3868\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3485 - val_loss: 1.3875\n",
      "Epoch 15/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.3456Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3451 - val_loss: 1.4012\n",
      "Epoch 00015: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5539 - val_loss: 1.5133\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4844 - val_loss: 1.4580\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4547 - val_loss: 1.4418\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4354 - val_loss: 1.4388\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4224 - val_loss: 1.4241\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4083 - val_loss: 1.4049\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3984 - val_loss: 1.4002\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3870 - val_loss: 1.3997\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3814 - val_loss: 1.3885\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3728 - val_loss: 1.3820\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3666 - val_loss: 1.3764\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3593 - val_loss: 1.3815\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3519 - val_loss: 1.3793\n",
      "Epoch 14/100\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.3478Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3481 - val_loss: 1.3794\n",
      "Epoch 00014: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5475 - val_loss: 1.5007\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4818 - val_loss: 1.4641\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4544 - val_loss: 1.4470\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4356 - val_loss: 1.4303\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4164 - val_loss: 1.4164\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4065 - val_loss: 1.4102\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3910 - val_loss: 1.4069\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3836 - val_loss: 1.3935\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3746 - val_loss: 1.3981\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3662 - val_loss: 1.3815\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3620 - val_loss: 1.3875\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3568 - val_loss: 1.3871\n",
      "Epoch 13/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.3529Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3532 - val_loss: 1.3841\n",
      "Epoch 00013: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5525 - val_loss: 1.5164\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4864 - val_loss: 1.4772\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4554 - val_loss: 1.4498\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4331 - val_loss: 1.4325\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4169 - val_loss: 1.4202\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4080 - val_loss: 1.4143\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3923 - val_loss: 1.4068\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3828 - val_loss: 1.4074\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3791 - val_loss: 1.3956\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3676 - val_loss: 1.3959\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3619 - val_loss: 1.3894\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3536 - val_loss: 1.3855\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3508 - val_loss: 1.3939\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3446 - val_loss: 1.3918\n",
      "Epoch 15/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.3382Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3379 - val_loss: 1.3944\n",
      "Epoch 00015: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5602 - val_loss: 1.5339\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5207 - val_loss: 1.5139\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5059 - val_loss: 1.5069\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4968 - val_loss: 1.4981\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4883 - val_loss: 1.4971\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4834 - val_loss: 1.4897\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4770 - val_loss: 1.4873\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4713 - val_loss: 1.4868\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4658 - val_loss: 1.4823\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4603 - val_loss: 1.4823\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4566 - val_loss: 1.4823\n",
      "Epoch 12/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.4522Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4523 - val_loss: 1.4873\n",
      "Epoch 00012: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5568 - val_loss: 1.5274\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5184 - val_loss: 1.5130\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5040 - val_loss: 1.5044\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4954 - val_loss: 1.5033\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4920 - val_loss: 1.5015\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4894 - val_loss: 1.5016\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4805 - val_loss: 1.4939\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4772 - val_loss: 1.4932\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4725 - val_loss: 1.4899\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4676 - val_loss: 1.4891\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4621 - val_loss: 1.4852\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4563 - val_loss: 1.4856\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4516 - val_loss: 1.4847\n",
      "Epoch 14/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.4481Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4485 - val_loss: 1.4846\n",
      "Epoch 00014: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5562 - val_loss: 1.5258\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5174 - val_loss: 1.5086\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5064 - val_loss: 1.5000\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4986 - val_loss: 1.4966\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4927 - val_loss: 1.4896\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4841 - val_loss: 1.4905\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4810 - val_loss: 1.4840\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4727 - val_loss: 1.4848\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4670 - val_loss: 1.4798\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4631 - val_loss: 1.4773\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4563 - val_loss: 1.4826\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4506 - val_loss: 1.4797\n",
      "Epoch 13/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.4473Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4476 - val_loss: 1.4822\n",
      "Epoch 00013: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5607 - val_loss: 1.5311\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5203 - val_loss: 1.5128\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5051 - val_loss: 1.5031\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4959 - val_loss: 1.4954\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4896 - val_loss: 1.4941\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4835 - val_loss: 1.4886\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4800 - val_loss: 1.4881\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4736 - val_loss: 1.4853\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4659 - val_loss: 1.4886\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4627 - val_loss: 1.4832\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4583 - val_loss: 1.4838\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4537 - val_loss: 1.4806\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4461 - val_loss: 1.4874\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4437 - val_loss: 1.4842\n",
      "Epoch 15/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.4380Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4380 - val_loss: 1.4818\n",
      "Epoch 00015: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5602 - val_loss: 1.5333\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5187 - val_loss: 1.5120\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5036 - val_loss: 1.5058\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4983 - val_loss: 1.5001\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4896 - val_loss: 1.5008\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4864 - val_loss: 1.4983\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4801 - val_loss: 1.4926\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4732 - val_loss: 1.4894\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4680 - val_loss: 1.4859\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4626 - val_loss: 1.4886\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4573 - val_loss: 1.4841\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4537 - val_loss: 1.4844\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4499 - val_loss: 1.4815\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4447 - val_loss: 1.4845\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4408 - val_loss: 1.4869\n",
      "Epoch 16/100\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.4364Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4364 - val_loss: 1.4832\n",
      "Epoch 00016: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5536 - val_loss: 1.5136\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4800 - val_loss: 1.4479\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4266 - val_loss: 1.4111\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3998 - val_loss: 1.4019\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3785 - val_loss: 1.3753\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3616 - val_loss: 1.3679\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3478 - val_loss: 1.3506\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3369 - val_loss: 1.3498\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3254 - val_loss: 1.3380\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3137 - val_loss: 1.3592\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.3020 - val_loss: 1.3358\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2949 - val_loss: 1.3205\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2832 - val_loss: 1.3139\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2778 - val_loss: 1.3087\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2658 - val_loss: 1.3074\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2616 - val_loss: 1.3081\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2604 - val_loss: 1.3005\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2505 - val_loss: 1.3078\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.2460 - val_loss: 1.3033\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2380 - val_loss: 1.2948\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2352 - val_loss: 1.2905\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2272 - val_loss: 1.2941\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2255 - val_loss: 1.3050\n",
      "Epoch 24/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.2236Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2234 - val_loss: 1.2929\n",
      "Epoch 00024: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5483 - val_loss: 1.4949\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4625 - val_loss: 1.4368\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4188 - val_loss: 1.4118\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3912 - val_loss: 1.3918\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3679 - val_loss: 1.3680\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3469 - val_loss: 1.3662\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3328 - val_loss: 1.3397\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3180 - val_loss: 1.3371\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.3022 - val_loss: 1.3299\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.2915 - val_loss: 1.3235\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2858 - val_loss: 1.3283\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2777 - val_loss: 1.3230\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2678 - val_loss: 1.3171\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2630 - val_loss: 1.3114\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2568 - val_loss: 1.3086\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2529 - val_loss: 1.3165\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2431 - val_loss: 1.3151\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.2369Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2369 - val_loss: 1.3228\n",
      "Epoch 00018: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5525 - val_loss: 1.5027\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4748 - val_loss: 1.4438\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4333 - val_loss: 1.4080\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4029 - val_loss: 1.3821\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3740 - val_loss: 1.3602\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3515 - val_loss: 1.3428\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3443 - val_loss: 1.3318\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3264 - val_loss: 1.3330\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3179 - val_loss: 1.3157\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3056 - val_loss: 1.3333\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2979 - val_loss: 1.3079\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2893 - val_loss: 1.3225\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2835 - val_loss: 1.3009\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2793 - val_loss: 1.3033\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2723 - val_loss: 1.2931\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2639 - val_loss: 1.2939\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2608 - val_loss: 1.2998\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2562 - val_loss: 1.2902\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.2509 - val_loss: 1.2906\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2421 - val_loss: 1.2960\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2405 - val_loss: 1.2875\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.2321 - val_loss: 1.2874\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2285 - val_loss: 1.2771\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2241 - val_loss: 1.2824\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2238 - val_loss: 1.2854\n",
      "Epoch 26/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.2208Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2208 - val_loss: 1.2815\n",
      "Epoch 00026: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5466 - val_loss: 1.5010\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4665 - val_loss: 1.4469\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4238 - val_loss: 1.4174\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3990 - val_loss: 1.3979\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3767 - val_loss: 1.3883\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3607 - val_loss: 1.3831\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3463 - val_loss: 1.3757\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3319 - val_loss: 1.3487\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3202 - val_loss: 1.3526\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3065 - val_loss: 1.3463\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2988 - val_loss: 1.3280\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2863 - val_loss: 1.3386\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2830 - val_loss: 1.3260\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2717 - val_loss: 1.3163\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2675 - val_loss: 1.3097\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2633 - val_loss: 1.3031\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.2478 - val_loss: 1.3005\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2443 - val_loss: 1.3120\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2408 - val_loss: 1.2974\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2329 - val_loss: 1.2981\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2271 - val_loss: 1.3010\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.2292Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2292 - val_loss: 1.3025\n",
      "Epoch 00022: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5543 - val_loss: 1.5113\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4657 - val_loss: 1.4347\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4177 - val_loss: 1.4217\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3954 - val_loss: 1.3976\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3750 - val_loss: 1.3756\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3584 - val_loss: 1.3773\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3438 - val_loss: 1.3544\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3283 - val_loss: 1.3495\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3195 - val_loss: 1.3421\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3096 - val_loss: 1.3418\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2998 - val_loss: 1.3384\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2950 - val_loss: 1.3299\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2822 - val_loss: 1.3219\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2802 - val_loss: 1.3237\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2714 - val_loss: 1.3225\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2622 - val_loss: 1.3061\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2555 - val_loss: 1.3088\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2550 - val_loss: 1.3003\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2471 - val_loss: 1.3009\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2359 - val_loss: 1.3012\n",
      "Epoch 21/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.2328Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2320 - val_loss: 1.3047\n",
      "Epoch 00021: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5620 - val_loss: 1.5333\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5032 - val_loss: 1.4690\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4551 - val_loss: 1.4337\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4271 - val_loss: 1.4240\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3998 - val_loss: 1.3988\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3839 - val_loss: 1.3889\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3699 - val_loss: 1.3816\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3589 - val_loss: 1.3856\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3487 - val_loss: 1.3662\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.3389 - val_loss: 1.3669\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3340 - val_loss: 1.3627\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3233 - val_loss: 1.3591\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3194 - val_loss: 1.3512\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3116 - val_loss: 1.3506\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3072 - val_loss: 1.3470\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3013 - val_loss: 1.3558\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2958 - val_loss: 1.3435\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2914 - val_loss: 1.3445\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2877 - val_loss: 1.3383\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2857 - val_loss: 1.3497\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2787 - val_loss: 1.3424\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.2763Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2763 - val_loss: 1.3376\n",
      "Epoch 00022: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5553 - val_loss: 1.5253\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4976 - val_loss: 1.4826\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4584 - val_loss: 1.4503\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4312 - val_loss: 1.4326\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4011 - val_loss: 1.4148\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3867 - val_loss: 1.3981\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3687 - val_loss: 1.3882\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3567 - val_loss: 1.3809\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3502 - val_loss: 1.3801\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3447 - val_loss: 1.3940\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3343 - val_loss: 1.3697\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3270 - val_loss: 1.3663\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.3199 - val_loss: 1.3708\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3138 - val_loss: 1.3597\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3111 - val_loss: 1.3608\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3031 - val_loss: 1.3593\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2975 - val_loss: 1.3532\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2939 - val_loss: 1.3496\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2892 - val_loss: 1.3527\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2832 - val_loss: 1.3513\n",
      "Epoch 21/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.2786Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2791 - val_loss: 1.3531\n",
      "Epoch 00021: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5581 - val_loss: 1.5280\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4949 - val_loss: 1.4559\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4451 - val_loss: 1.4229\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4212 - val_loss: 1.4059\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3992 - val_loss: 1.3911\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3828 - val_loss: 1.3783\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3687 - val_loss: 1.3717\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3610 - val_loss: 1.3649\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3510 - val_loss: 1.3618\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3468 - val_loss: 1.3699\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3396 - val_loss: 1.3568\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3360 - val_loss: 1.3588\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3250 - val_loss: 1.3493\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3191 - val_loss: 1.3466\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3140 - val_loss: 1.3443\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3095 - val_loss: 1.3614\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3081 - val_loss: 1.3454\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2983 - val_loss: 1.3420\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2951 - val_loss: 1.3382\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2892 - val_loss: 1.3451\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2881 - val_loss: 1.3356\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2795 - val_loss: 1.3428\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2769 - val_loss: 1.3334\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.2741 - val_loss: 1.3342\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2701 - val_loss: 1.3357\n",
      "Epoch 26/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.2668Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2665 - val_loss: 1.3411\n",
      "Epoch 00026: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5528 - val_loss: 1.5186\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4948 - val_loss: 1.4752\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4570 - val_loss: 1.4532\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4264 - val_loss: 1.4366\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4021 - val_loss: 1.4005\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3824 - val_loss: 1.3828\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.3669 - val_loss: 1.3770\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3576 - val_loss: 1.3775\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3441 - val_loss: 1.3767\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3332 - val_loss: 1.3583\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3260 - val_loss: 1.3517\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3153 - val_loss: 1.3537\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3154 - val_loss: 1.3482\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3074 - val_loss: 1.3525\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3017 - val_loss: 1.3520\n",
      "Epoch 16/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.2954Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2952 - val_loss: 1.3481\n",
      "Epoch 00016: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5575 - val_loss: 1.5366\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4996 - val_loss: 1.4786\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4575 - val_loss: 1.4425\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4230 - val_loss: 1.4144\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3985 - val_loss: 1.3985\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3821 - val_loss: 1.3844\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3660 - val_loss: 1.3809\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3580 - val_loss: 1.3743\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3425 - val_loss: 1.3672\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3330 - val_loss: 1.3684\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3256 - val_loss: 1.3693\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3225 - val_loss: 1.3596\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.3156 - val_loss: 1.3494\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3047 - val_loss: 1.3546\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3048 - val_loss: 1.3523\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2959 - val_loss: 1.3484\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2915 - val_loss: 1.3487\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2868 - val_loss: 1.3454\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2811 - val_loss: 1.3424\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2786 - val_loss: 1.3493\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2723 - val_loss: 1.3360\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2701 - val_loss: 1.3329\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2638 - val_loss: 1.3362\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2626 - val_loss: 1.3379\n",
      "Epoch 25/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.2582Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.2579 - val_loss: 1.3462\n",
      "Epoch 00025: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "Training model for CV #1\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5616 - val_loss: 1.5346\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5076 - val_loss: 1.4875\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4795 - val_loss: 1.4739\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4669 - val_loss: 1.4720\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4572 - val_loss: 1.4658\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4475 - val_loss: 1.4552\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4404 - val_loss: 1.4754\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4350 - val_loss: 1.4496\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4254 - val_loss: 1.4510\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4205 - val_loss: 1.4469\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4179 - val_loss: 1.4429\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4139 - val_loss: 1.4376\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4031 - val_loss: 1.4384\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3963 - val_loss: 1.4377\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.3915Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3915 - val_loss: 1.4402\n",
      "Epoch 00015: early stopping\n",
      "Training model for CV #2\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5631 - val_loss: 1.5401\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5137 - val_loss: 1.4990\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4838 - val_loss: 1.4805\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4685 - val_loss: 1.4728\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4568 - val_loss: 1.4663\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4482 - val_loss: 1.4647\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4400 - val_loss: 1.4564\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4315 - val_loss: 1.4524\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4255 - val_loss: 1.4494\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4181 - val_loss: 1.4502\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4105 - val_loss: 1.4527\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4075 - val_loss: 1.4455\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4003 - val_loss: 1.4584\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3953 - val_loss: 1.4422\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3919 - val_loss: 1.4446\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3810 - val_loss: 1.4431\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3778 - val_loss: 1.4378\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3722 - val_loss: 1.4380\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3675 - val_loss: 1.4407\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - ETA: 0s - loss: 1.3661Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3661 - val_loss: 1.4418\n",
      "Epoch 00020: early stopping\n",
      "Training model for CV #3\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5629 - val_loss: 1.5337\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5116 - val_loss: 1.4904\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4857 - val_loss: 1.4720\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4728 - val_loss: 1.4648\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4624 - val_loss: 1.4624\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4520 - val_loss: 1.4479\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4449 - val_loss: 1.4462\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4360 - val_loss: 1.4433\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4299 - val_loss: 1.4396\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4200 - val_loss: 1.4329\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4159 - val_loss: 1.4347\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4075 - val_loss: 1.4267\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4006 - val_loss: 1.4305\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3949 - val_loss: 1.4327\n",
      "Epoch 15/100\n",
      "83/86 [===========================>..] - ETA: 0s - loss: 1.3906Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3902 - val_loss: 1.4321\n",
      "Epoch 00015: early stopping\n",
      "Training model for CV #4\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.5599 - val_loss: 1.5353\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5093 - val_loss: 1.4953\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4816 - val_loss: 1.4828\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4677 - val_loss: 1.4723\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4592 - val_loss: 1.4640\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4499 - val_loss: 1.4591\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4437 - val_loss: 1.4548\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4327 - val_loss: 1.4510\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4249 - val_loss: 1.4520\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4241 - val_loss: 1.4469\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4130 - val_loss: 1.4493\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.4090 - val_loss: 1.4522\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4029 - val_loss: 1.4435\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3966 - val_loss: 1.4533\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3914 - val_loss: 1.4391\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3872 - val_loss: 1.4487\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3843 - val_loss: 1.4354\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3747 - val_loss: 1.4416\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3708 - val_loss: 1.4381\n",
      "Epoch 20/100\n",
      "85/86 [============================>.] - ETA: 0s - loss: 1.3701Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.3699 - val_loss: 1.4449\n",
      "Epoch 00020: early stopping\n",
      "Training model for CV #5\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 22ms/step - loss: 1.5557 - val_loss: 1.5229\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.5033 - val_loss: 1.4984\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4804 - val_loss: 1.4836\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4649 - val_loss: 1.4716\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.4574 - val_loss: 1.4650\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4446 - val_loss: 1.4689\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4399 - val_loss: 1.4623\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4337 - val_loss: 1.4538\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4241 - val_loss: 1.4479\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4180 - val_loss: 1.4583\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4126 - val_loss: 1.4452\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.4054 - val_loss: 1.4579\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3984 - val_loss: 1.4425\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.3905 - val_loss: 1.4437\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 2s 21ms/step - loss: 1.3838 - val_loss: 1.4395\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3836 - val_loss: 1.4356\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3749 - val_loss: 1.4448\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3737 - val_loss: 1.4418\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3664 - val_loss: 1.4343\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3662 - val_loss: 1.4418\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 2s 19ms/step - loss: 1.3616 - val_loss: 1.4368\n",
      "Epoch 22/100\n",
      "84/86 [============================>.] - ETA: 0s - loss: 1.3557Restoring model weights from the end of the best epoch.\n",
      "86/86 [==============================] - 2s 20ms/step - loss: 1.3563 - val_loss: 1.4392\n",
      "Epoch 00022: early stopping\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "lr ver1 Accuracy (CV):  46.6444%\n",
      "lr ver1 Log Loss (CV):   1.3128\n",
      "lr ver2 Accuracy (CV):  42.4042%\n",
      "lr ver2 Log Loss (CV):   1.3840\n",
      "lr ver3 Accuracy (CV):  35.7678%\n",
      "lr ver3 Log Loss (CV):   1.4813\n",
      "lr ver4 Accuracy (CV):  47.1437%\n",
      "lr ver4 Log Loss (CV):   1.2948\n",
      "lr ver5 Accuracy (CV):  44.4779%\n",
      "lr ver5 Log Loss (CV):   1.3405\n",
      "lr ver6 Accuracy (CV):  39.1224%\n",
      "lr ver6 Log Loss (CV):   1.4344\n"
     ]
    }
   ],
   "source": [
    "p_val_ver1 = np.zeros((X_1.shape[0], n_class))\n",
    "p_tst_ver1 = np.zeros((X_tst_1.shape[0], n_class))\n",
    "p_val_ver2 = np.zeros((X_2.shape[0], n_class))\n",
    "p_tst_ver2 = np.zeros((X_tst_2.shape[0], n_class))\n",
    "p_val_ver3 = np.zeros((X_3.shape[0], n_class))\n",
    "p_tst_ver3 = np.zeros((X_tst_3.shape[0], n_class))\n",
    "p_val_ver4 = np.zeros((X_4.shape[0], n_class))\n",
    "p_tst_ver4 = np.zeros((X_tst_4.shape[0], n_class))\n",
    "p_val_ver5 = np.zeros((X_5.shape[0], n_class))\n",
    "p_tst_ver5 = np.zeros((X_tst_5.shape[0], n_class))\n",
    "p_val_ver6 = np.zeros((X_6.shape[0], n_class))\n",
    "p_tst_ver6 = np.zeros((X_tst_6.shape[0], n_class))\n",
    "\n",
    "for number, (X, test) in enumerate([(X_1, X_tst_1), (X_2, X_tst_2), (X_3, X_tst_3),\n",
    "               (X_4, X_tst_4), (X_5, X_tst_5), (X_6, X_tst_6)],1):\n",
    "    for i_cv, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n",
    "        print(f'Training model for CV #{i_cv}')\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
    "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
    "        \n",
    "        clf = get_model(X.shape[1])\n",
    "        clf.fit(X[i_trn], \n",
    "            to_categorical(y[i_trn]),\n",
    "            validation_data=(X[i_val], to_categorical(y[i_val])),\n",
    "            epochs=100,\n",
    "            batch_size=512,\n",
    "            callbacks=[es])\n",
    "       \n",
    "        # Predict\n",
    "        if number==1:\n",
    "            p_val_ver1[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver1 += clf.predict(test) / n_class\n",
    "        elif number==2:\n",
    "            p_val_ver2[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver2 += clf.predict(test) / n_class\n",
    "        elif number==3:\n",
    "            p_val_ver3[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver3 += clf.predict(test) / n_class\n",
    "        elif number==4:\n",
    "            p_val_ver4[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver4 += clf.predict(test) / n_class\n",
    "        elif number==5:\n",
    "            p_val_ver5[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver5 += clf.predict(test) / n_class\n",
    "        else:\n",
    "            p_val_ver6[i_val, :] = clf.predict(X[i_val])\n",
    "            p_tst_ver6 += clf.predict(test) / n_class\n",
    "            \n",
    "        del clf\n",
    "        clear_session()\n",
    "        gc.collect()\n",
    "            \n",
    "    print(\"Training has finished\")\n",
    "    print(\"*\"*100)\n",
    "\n",
    "            \n",
    "print(f'lr ver1 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver1, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver1 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver1):8.4f}')\n",
    "print(f'lr ver2 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver2, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver2 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver2):8.4f}')\n",
    "print(f'lr ver3 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver3, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver3 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver3):8.4f}')\n",
    "print(f'lr ver4 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver4, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver4 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver4):8.4f}')\n",
    "print(f'lr ver5 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver5, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver5 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver5):8.4f}')\n",
    "print(f'lr ver6 Accuracy (CV): {accuracy_score(y, np.argmax(p_val_ver6, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr ver6 Log Loss (CV): {log_loss(pd.get_dummies(y), p_val_ver6):8.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제출 파일 생성 및 기타 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 파일 생성\n",
    "\n",
    "sub = pd.read_csv(sample_file, index_col=0)\n",
    "\n",
    "# Ver1\n",
    "sub[sub.columns] = p_tst_ver1\n",
    "sub.to_csv(sub_ver1_file)\n",
    "\n",
    "# Ver2\n",
    "sub[sub.columns] = p_tst_ver2\n",
    "sub.to_csv(sub_ver2_file)\n",
    "\n",
    "# Ver3\n",
    "sub[sub.columns] = p_tst_ver3\n",
    "sub.to_csv(sub_ver3_file)\n",
    "\n",
    "# Ver4\n",
    "sub[sub.columns] = p_tst_ver4\n",
    "sub.to_csv(sub_ver4_file)\n",
    "\n",
    "# Ver5\n",
    "sub[sub.columns] = p_tst_ver5\n",
    "sub.to_csv(sub_ver5_file)\n",
    "\n",
    "# Ver6\n",
    "sub[sub.columns] = p_tst_ver6\n",
    "sub.to_csv(sub_ver6_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:45:31.272596Z",
     "start_time": "2020-11-09T04:45:31.074976Z"
    }
   },
   "outputs": [],
   "source": [
    "# p_val 파일 생성 -> oof\n",
    "\n",
    "# Ver1\n",
    "np.savetxt(p_val_ver1_file, p_val_ver1, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(p_val_ver2_file, p_val_ver2, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(p_val_ver3_file, p_val_ver3, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(p_val_ver4_file, p_val_ver4, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver5\n",
    "np.savetxt(p_val_ver5_file, p_val_ver5, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver6\n",
    "np.savetxt(p_val_ver6_file, p_val_ver6, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_tst 파일 생성 -> test \n",
    "\n",
    "# Ver1\n",
    "np.savetxt(p_tst_ver1_file, p_tst_ver1, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver2\n",
    "np.savetxt(p_tst_ver2_file, p_tst_ver2, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver3\n",
    "np.savetxt(p_tst_ver3_file, p_tst_ver3, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver4\n",
    "np.savetxt(p_tst_ver4_file, p_tst_ver4, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver5\n",
    "np.savetxt(p_tst_ver5_file, p_tst_ver5, fmt='%.18f', delimiter=',')\n",
    "\n",
    "# Ver6\n",
    "np.savetxt(p_tst_ver6_file, p_tst_ver6, fmt='%.18f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
