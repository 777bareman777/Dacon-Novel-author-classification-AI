{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 import 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T04:32:22.350713Z",
     "start_time": "2020-11-09T04:32:22.049823Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data/dacon-novel-author-classification')\n",
    "feature_dir = Path('../build/feature')\n",
    "val_dir = Path('../build/val')\n",
    "tst_dir = Path('../build/tst')\n",
    "sub_dir = Path('../build/sub')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_name = 'lr'\n",
    "feature_name = 'lemmatization-tfidf'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "feature_file = feature_dir / f'{feature_name}.csv'\n",
    "\n",
    "p_val_file = val_dir / f'{model_name}.val.csv'\n",
    "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
    "\n",
    "sub_file = sub_dir / f'{model_name}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      He was almost choking. There was so much, so m...       3\n",
       "1                 “Your sister asked for it, I suppose?”       2\n",
       "2       She was engaged one day as she walked, in per...       1\n",
       "3      The captain was in the porch, keeping himself ...       4\n",
       "4      “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn = pd.read_csv(trn_file, index_col=0)\n",
    "print(trn.shape)\n",
    "trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19617, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Not at all. I think she is one of the most ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As the lady had stated her intention of scream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“And then suddenly in the silence I heard a so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>His conviction remained unchanged. So far as I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "index                                                   \n",
       "0      “Not at all. I think she is one of the most ch...\n",
       "1      \"No,\" replied he, with sudden consciousness, \"...\n",
       "2      As the lady had stated her intention of scream...\n",
       "3      “And then suddenly in the silence I heard a so...\n",
       "4      His conviction remained unchanged. So far as I..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = pd.read_csv(tst_file, index_col=0)\n",
    "print(tst.shape)\n",
    "tst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPq0vxJVDS3R"
   },
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vf_TGrbKCaDK"
   },
   "outputs": [],
   "source": [
    "# 불용어\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \n",
    "             \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \n",
    "             \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \n",
    "             \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \n",
    "             \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \n",
    "             \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n",
    "             \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \n",
    "             \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \n",
    "             \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \n",
    "             \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \n",
    "             \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomTfidVectorier(TfidfVectorizer):\n",
    "    \n",
    "#     def build_analyzer(self):\n",
    "    \n",
    "#         stop_words = self.get_stop_words()\n",
    "       \n",
    "#         # 단어의 품사 정보 얻는 함수\n",
    "#         def get_wordnet_pos(word):\n",
    "#             tag = pos_tag([word])[0][1][0].upper()\n",
    "#             tag_dict = {\n",
    "#                 \"J\" : wordnet.ADJ,\n",
    "#                 \"N\" : wordnet.NOUN,\n",
    "#                 \"V\" : wordnet.VERB,\n",
    "#                 \"R\" : wordnet.ADV\n",
    "#             }\n",
    "            \n",
    "#             return tag_dict.get(tag, wordnet.NOUN)\n",
    "       \n",
    "#         def analyser(doc):\n",
    "#             # 부호를 제거\n",
    "#             doc_clean = re.sub(r'[^A-Za-z0-9 ]', '', doc)\n",
    "            \n",
    "#             # 토큰화\n",
    "#             doc_clean = word_tokenize(doc_clean)\n",
    "        \n",
    "#             # 품사 정보를 이용해서 표제어 추출\n",
    "#             lemmatizer = WordNetLemmatizer()\n",
    "#             tokenized_word = [lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in doc_clean] \n",
    "       \n",
    "#             # 내장된 _word_ngrams 사용\n",
    "#             # 불용어 까지 같이 처리\n",
    "#             return self._word_ngrams(tokenized_word, stop_words)\n",
    "#         return(analyser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTfidVectorier(TfidfVectorizer):\n",
    "    \n",
    "    def build_analyzer(self):\n",
    "       \n",
    "        # 단어의 품사 정보 얻는 함수\n",
    "        def get_wordnet_pos(word):\n",
    "            if word.startswith('J'):\n",
    "                return wordnet.ADJ\n",
    "            elif word.startswith('V'):\n",
    "                return wordnet.VERB\n",
    "            elif word.startswith('N'):\n",
    "                return wordnet.NOUN\n",
    "            elif word.startswith('R'):\n",
    "                return wordnet.ADV\n",
    "            else:\n",
    "                return None\n",
    "       \n",
    "        def analyser(doc):\n",
    "            # 부호를 제거\n",
    "            doc_clean = re.sub(r'[^A-Za-z0-9 ]', '', doc)\n",
    "            \n",
    "            # 토큰화\n",
    "            doc_clean = word_tokenize(doc_clean)\n",
    "            \n",
    "            # 태깅정보 획득 및 wordnetlemmatizer에 알맞는 태깅정보로 변환\n",
    "            pos_tagged = pos_tag(doc_clean)\n",
    "            wordnet_tagged = list(map(lambda x: (x[0], get_wordnet_pos(x[1])), pos_tagged))\n",
    "            \n",
    "            # 품사 정보를 이용해서 표제어 추출\n",
    "            tokenized_word = []\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            for word, tag in wordnet_tagged:\n",
    "                if tag is None:\n",
    "                    tokenized_word.append(word)\n",
    "                else:\n",
    "                    tokenized_word.append(lemmatizer.lemmatize(word,tag))\n",
    "       \n",
    "            # 내장된 _word_ngrams 사용\n",
    "            return self._word_ngrams(tokenized_word)\n",
    "        return(analyser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 9278) (19617, 9278)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 피쳐 생성 - using word_tokenize\n",
    "vec = CustomTfidVectorier(ngram_range=(1, 3), min_df=50)\n",
    "X_1 = vec.fit_transform(trn['text'])\n",
    "X_tst_1 = vec.transform(tst['text'])\n",
    "print(X_1.shape, X_tst_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱회귀 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54879,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = trn.author.values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for CV #1\n",
      "Training model for CV #2\n",
      "Training model for CV #3\n",
      "Training model for CV #4\n",
      "Training model for CV #5\n",
      "Training has finished\n",
      "****************************************************************************************************\n",
      "lr Accuracy (CV):  73.1974%\n",
      "lr Log Loss (CV):   0.7740\n"
     ]
    }
   ],
   "source": [
    "p_val = np.zeros((X_1.shape[0], n_class))\n",
    "p_tst = np.zeros((X_tst_1.shape[0], n_class))\n",
    "\n",
    "for i_cv, (i_trn, i_val) in enumerate(cv.split(X_1,y), 1):\n",
    "    print(f'Training model for CV #{i_cv}')\n",
    "    X_train, X_val = X_1[i_trn], X_1[i_val]\n",
    "    y_train, y_val = y[i_trn], y[i_val]\n",
    "    \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train,y_train)\n",
    "    \n",
    "    p_val[i_val, :] = lr_clf.predict_proba(X_1[i_val])\n",
    "    p_tst += lr_clf.predict_proba(X_tst_1) / n_class\n",
    "    \n",
    "print(\"Training has finished\")\n",
    "print(\"*\"*100)\n",
    "    \n",
    "print(f'lr Accuracy (CV): {accuracy_score(y, np.argmax(p_val, axis=1)) * 100:8.4f}%')\n",
    "print(f'lr Log Loss (CV): {log_loss(pd.get_dummies(y), p_val):8.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of [코드]_데이콘_기초_베이스라인_LSTM[keras tutorials].ipynb",
   "provenance": [
    {
     "file_id": "1jBr9-t5Gmda0w_DNEsgndfHkNbgaz6D7",
     "timestamp": 1604729733451
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
